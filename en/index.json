[{"authors":["admin"],"categories":null,"content":"Dr. Zhijian He is an associate Professor at School of Mathematics of South China University of Technology (SCUT). Before joining SCUT, he obtained a Ph.D. in Statistics from Department of Mathematical Science of Tsinghua University, advised by Prof. Xiaoqun Wang. His research interests are quasi-Monte Carlo methods and their applications in quantitative finance and statistics. He was a silver prize recipient of the New World Mathematics Awards (NWMA). He has published in top journals in the fields of statistics and computational mathematics, such as Journal of the Royal Statistical Society: Series B, SIAM Journal on Numerical Analysis, SIAM Journal on Scientific Computing, Mathematics of Computation. Part of his research is supported by National Science Foundation of China (NSFC).\n","date":1595203200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1595203200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/en/author/zhijian-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zhijian-he/","section":"authors","summary":"Dr. Zhijian He is an associate Professor at School of Mathematics of South China University of Technology (SCUT). Before joining SCUT, he obtained a Ph.D. in Statistics from Department of Mathematical Science of Tsinghua University, advised by Prof.","tags":null,"title":"Zhijian He","type":"authors"},{"authors":null,"categories":null,"content":"ç®€ä»‹ ğŸ“¢ ã€Šæ•°ç†ç»Ÿè®¡ã€‹æ˜¯ç»Ÿè®¡å­¦æ–¹å‘çš„å…¥é—¨è¯¾ç¨‹ï¼Œä¸»è¦å¤„ç†å¸¦æœ‰éšæœºæ€§çš„æ•°æ®ã€‚æœ¬è¯¾ç¨‹æ—¨åœ¨åŸ¹å…»å­¦ç”Ÿä½¿ç”¨æ•°ç†å·¥å…·è§£å†³å¸¸è§çš„ç»Ÿè®¡é—®é¢˜ï¼ŒæŒæ¡ç»Ÿè®¡å­¦æ ¸å¿ƒæ€æƒ³ï¼Œå»ºç«‹æ­£ç¡®çš„ç»Ÿè®¡è§‚ã€‚\næœ¬è¯¾ç¨‹ä¸€å…±64å­¦æ—¶ï¼Œå…·ä½“çš„è¯¾ç¨‹å®‰æ’è§ä¸‹æ–¹ã€‚ç”±äºåŸ¹å…»æ–¹æ¡ˆä¸­è§„å®šã€Šæ•°ç†ç»Ÿè®¡ã€‹æ˜¯åŒè¯­æ•™å­¦ï¼Œæ‰€ä»¥è¯¾ç¨‹æ•™å­¦æ¿ä¹¦å¤§éƒ¨åˆ†ä½¿ç”¨è‹±æ–‡ï¼Œä½œä¸šå’ŒæœŸæœ«è€ƒè¯•é‡‡ç”¨è‹±æ–‡å‡ºé¢˜ã€‚å…è®¸ï¼ˆä½†ä¸é¼“åŠ±ï¼‰å­¦ç”Ÿç”¨ä¸­æ–‡ä½œç­”ã€‚æ•™æä¸»è¦é‡‡ç”¨ä»¥ä¸‹ä¸¤æœ¬ã€‚\næˆ‘ä»¬ä¼šå¤„ç†ä¸€äº›å®é™…æ•°æ®ï¼Œéœ€è¦ä½¿ç”¨ç»Ÿè®¡è½¯ä»¶ã€‚æ¨èä½¿ç”¨ Rè½¯ä»¶ï¼Œå¹¶é…åˆ Rstudioç¼–è¾‘å™¨ã€‚å½“ç„¶ï¼Œå­¦ç”Ÿå¯ç”¨ä½¿ç”¨å…¶å®ƒè½¯ä»¶ï¼Œå¦‚Python, Matlabç­‰ã€‚\næ•™æ ğŸ“–   ä¸­æ–‡æ•™æ(TB1): æ•°ç†ç»Ÿè®¡å­¦è®²ä¹‰ï¼ˆç¬¬3ç‰ˆï¼‰ï¼Œé™ˆå®¶é¼ã€å­™å±±æ³½ã€æä¸œé£ã€åˆ˜åŠ›å¹³ç¼–è‘—ï¼Œé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾\n  è‹±æ–‡æ•™æ(TB2)ï¼šJohn Rice, Mathematical Statistics and Data Analysis, 3rd edition, 2007. Link\n  Rè½¯ä»¶å‚è€ƒèµ„æ–™ ğŸ“–    Rudy Angelesâ€™ R tutorial\n   R for Data Science by G. Grolemund and H. Wickham\n  Rè¯­è¨€å®ç”¨æ•™ç¨‹ï¼Œè–›æ¯…ã€é™ˆç«‹èç¼–è‘—ï¼Œæ¸…åå¤§å­¦å‡ºç‰ˆç¤¾\n  è€ƒæ ¸æ–¹å¼ ğŸ“ æœ€ç»ˆæˆç»© = å¹³æ—¶æˆç»©ï¼ˆ30%ï¼‰+æœŸæœ«æˆç»©ï¼ˆ70%ï¼‰\nè¯¾ç¨‹èµ„æº ğŸ”—    æˆ‘ä¹‹å‰å†™çš„è®²ä¹‰ï¼ˆç½‘é¡µç‰ˆï¼Œå«Rä»£ç ï¼‰\n   æˆ‘æ­£åœ¨å†™çš„è®²ä¹‰ï¼ˆPDFç‰ˆï¼Œä¸å«Rä»£ç ï¼‰\n   æ–¯å¦ç¦å¤§å­¦Art Owenæ•™æˆçš„è®²ä¹‰\n  è¯¾ç¨‹å®‰æ’ ğŸ“†    No Readings ğŸ“š Topics ğŸ”ˆ Notes     1 TB1: Ch 1 TB2: Ch 1-7 Outline of course, Review of Probability, Fundamentals of Statistics  intro lec1   2 TB1: Ch 2.1 TB2: Ch 8.1-8.5 Method of moments and maximum likelihood estimation  mle   3 TB1: Ch 2.2 TB2: Ch 8.7-8.8 Unbiasedness, MSE, UMVUE, Apymptotic properties    4 TB1: Ch 2.3 Confidence interval  CI   5 TB1: Ch 2.4 TB2: Ch 10 Graphical summaries: empirical CDF, density estimates  DistEstimation   6 TB1: Ch 3 TB2: Ch 9, 11.1-11.3.1 Hypothesis test: concepts, LR tests, NP lemma  hypothesisTest   7 TB1: Ch 4 TB2: Ch 14 Linear Least squares: estimation, tests, prediction and control  LSM1 LSM2   8 TB1: Ch 5.1-5.2 TB2: Ch 12 Analysis of Variance     ä¸Šè¯¾è®²åˆ°çš„Rä»£ç     Rstudioäº‘è®¡ç®—\n  å¤§æ•°å®šå¾‹ä¸ä¸­å¿ƒæé™å®šç†ï¼š chap1-LLN_CLT.R\n  æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¡ˆä¾‹ï¼š mle-examples.Rmd\n  ç½®ä¿¡åŒºé—´ï¼š CI.R\n  ç»éªŒCDFï¼š empiricalCDF.R\n  å¯†åº¦ä¼°è®¡ï¼š density.R\n  çº¿æ€§å›å½’çš„åº”ç”¨: case1.R, case2.R, case3.R, case4.R, case5.R\n  ","date":1596931200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596931200,"objectID":"899b6a6afa5d68e0a5e2b31b85e37795","permalink":"/en/courses/stat/","publishdate":"2020-08-09T00:00:00Z","relpermalink":"/en/courses/stat/","section":"courses","summary":"è¿™æ˜¯æ•°ç†ç»Ÿè®¡è¯¾ç¨‹","tags":null,"title":"æ•°ç†ç»Ÿè®¡","type":"docs"},{"authors":null,"categories":null,"content":"æ•™æ   ä¸­æ–‡æ•™æ\n  è‹±æ–‡æ•™æ\n  è¯¾ç¨‹èµ„æº ","date":1596931200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596931200,"objectID":"38d94ee05c8252999a5107be669c3de6","permalink":"/en/courses/bayes/","publishdate":"2020-08-09T00:00:00Z","relpermalink":"/en/courses/bayes/","section":"courses","summary":"è¿™æ˜¯è´å¶æ–¯ç»Ÿè®¡è¯¾ç¨‹","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"ä½œä¸šè¯´æ˜ ğŸ’¬ é€šè¿‡é•¿æ±Ÿé›¨è¯¾å ‚æäº¤ä½œä¸šï¼Œä¸€èˆ¬ä¸ºæ¯å‘¨ä¸€æ¬¡ä½œä¸šï¼Œæœ‰ç›¸åº”çš„deadlineã€‚å­¦ç”Ÿé€šè¿‡é›¨è¯¾å ‚ï¼ˆå…¬ä¼—å·/å°ç¨‹åº/ç”µè„‘ç«¯ï¼‰æäº¤ä½œä¸šã€‚\n  è¯·å‡†å¤‡ä¸€ä¸ªä½œä¸šæœ¬å†™ä¸‹æ¯æ¬¡çš„ä½œä¸šï¼Œå¹¶é€šè¿‡æ‹ç…§çš„æ–¹å¼åœ¨é›¨è¯¾å ‚ä¸Šæäº¤ã€‚æ³¨æ„ç…§ç‰‡éœ€è¦æ¸…æ™°å¯é˜…è¯»ï¼Œæäº¤çš„ç­”æ¡ˆè¦å¯¹åº”ç›¸åº”çš„é¢˜å·ã€‚\n  æœ¬è¯¾ç¨‹è¿˜æœ‰äº›ä¸Šæœºç¼–ç¨‹çš„ä½œä¸šå’Œprojectï¼Œå¯é€šè¿‡é›¨è¯¾å ‚çš„ç”µè„‘ç«¯ä»¥PDFçš„å½¢å¼æäº¤ç”µå­ç‰ˆä½œä¸šã€‚\n  ä½œä¸šè¿˜æä¾› Rmarkdownå’Œ Latexç‰ˆæœ¬ï¼Œä¾›ç†Ÿæ‚‰çš„åŒå­¦ä½¿ç”¨ã€‚\n   æ³¨æ„ï¼šä½œä¸šåªæ¥å—å›¾ç‰‡å’ŒPDFä¸¤ç§æ ¼å¼ï¼Œåˆ‡å‹¿ä¸Šä¼ å‹ç¼©æ–‡ä»¶ï¼Œå¦åˆ™å½±å“åŠ©æ•™æ‰¹æ”¹æ•ˆç‡ã€‚åŸåˆ™ä¸Šä¸å…è®¸è¿Ÿäº¤ä½œä¸šï¼Œé™¤éæœ‰æ­£å½“ç†ç”±ã€‚   åŠ©æ•™ ğŸ‘   éƒ‘é“®ï¼ŒğŸ“§ zhengzhengyv@126.com (è´Ÿè´£å‰å…«æ¬¡ä½œä¸š)\n  æå‡Œæ¥ ï¼ŒğŸ“§ 1144747170@qq.com (è´Ÿè´£åå…«æ¬¡ä½œä¸š)\n  æœ‰å…³ä½œä¸šé—®é¢˜è¯·å’¨è¯¢åŠ©æ•™ï¼\nä½œä¸šé¢˜ç›® ğŸ’¯  æ³¨æ„ï¼šå¦‚æœä¸‹é¢é“¾æ¥æ— æ³•æ‰“å¼€ï¼Œæœ‰å¯èƒ½æ˜¯å¾…å¸ƒç½®çš„ä½œä¸šè¿˜æ²¡ä¸Šä¼ åˆ°æœåŠ¡å™¨æ‰€è‡´ã€‚å¦‚æœé›¨è¯¾å ‚å‘å¸ƒäº†ä½œä¸šï¼Œä½†å¯¹åº”ä½œä¸šçš„é“¾æ¥æ— æ•ˆï¼Œè¯·é‚®ä»¶å‘ŠçŸ¥æˆ‘ï¼      åºå· PDFç‰ˆ Rmarkdownç‰ˆ Latexç‰ˆ     1 hw1.pdf hw1.Rmd hw1.tex   2 hw2.pdf hw2.Rmd hw2.tex   3 hw3.pdf hw3.Rmd hw3.tex   4 hw4.pdf hw4.Rmd hw4.tex   5 hw5.pdf hw5.Rmd hw5.tex   6 hw6.pdf hw6.Rmd hw6.tex   7 hw7.pdf hw7.Rmd hw7.tex   8 hw8.pdf hw8.Rmd hw8.tex   9 hw9.pdf hw9.Rmd hw9.tex   10 hw10.pdf hw10.Rmd hw10.tex   11 hw11.pdf hw11.Rmd hw11.tex   12 hw12.pdf hw12.Rmd hw12.tex   13 hw13.pdf hw13.Rmd hw13.tex   14 hw14.pdf hw14.Rmd hw14.tex   15 hw15.pdf hw15.Rmd hw15.tex   16 hw16.pdf hw16.Rmd hw16.tex    ","date":1598223600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598227200,"objectID":"98658567b74b9e707ae10744486d3187","permalink":"/en/courses/stat/hw/","publishdate":"2020-08-24T00:00:00+01:00","relpermalink":"/en/courses/stat/hw/","section":"courses","summary":"ä½œä¸šè¯´æ˜ ğŸ’¬ é€šè¿‡é•¿æ±Ÿé›¨è¯¾å ‚æäº¤ä½œä¸šï¼Œä¸€èˆ¬ä¸ºæ¯å‘¨ä¸€æ¬¡ä½œä¸šï¼Œæœ‰ç›¸åº”çš„deadlineã€‚å­¦ç”Ÿé€šè¿‡é›¨è¯¾å ‚ï¼ˆå…¬ä¼—å·/å°ç¨‹åº/ç”µè„‘ç«¯ï¼‰æäº¤ä½œä¸šã€‚\n  è¯·å‡†å¤‡ä¸€ä¸ªä½œä¸šæœ¬å†™ä¸‹æ¯æ¬¡çš„ä½œä¸šï¼Œå¹¶é€šè¿‡æ‹ç…§çš„æ–¹å¼åœ¨é›¨è¯¾å ‚ä¸Šæäº¤ã€‚æ³¨æ„ç…§ç‰‡éœ€è¦æ¸…æ™°å¯é˜…è¯»ï¼Œæäº¤çš„ç­”æ¡ˆè¦å¯¹åº”ç›¸åº”çš„é¢˜å·ã€‚\n  æœ¬è¯¾ç¨‹è¿˜æœ‰äº›ä¸Šæœºç¼–ç¨‹çš„ä½œä¸šå’Œprojectï¼Œå¯é€šè¿‡é›¨è¯¾å ‚çš„ç”µè„‘ç«¯ä»¥PDFçš„å½¢å¼æäº¤ç”µå­ç‰ˆä½œä¸šã€‚\n  ä½œä¸šè¿˜æä¾› Rmarkdownå’Œ Latexç‰ˆæœ¬ï¼Œä¾›ç†Ÿæ‚‰çš„åŒå­¦ä½¿ç”¨ã€‚\n   æ³¨æ„ï¼šä½œä¸šåªæ¥å—å›¾ç‰‡å’ŒPDFä¸¤ç§æ ¼å¼ï¼Œåˆ‡å‹¿ä¸Šä¼ å‹ç¼©æ–‡ä»¶ï¼Œå¦åˆ™å½±å“åŠ©æ•™æ‰¹æ”¹æ•ˆç‡ã€‚åŸåˆ™ä¸Šä¸å…è®¸è¿Ÿäº¤ä½œä¸šï¼Œé™¤éæœ‰æ­£å½“ç†ç”±ã€‚   åŠ©æ•™ ğŸ‘   éƒ‘é“®ï¼ŒğŸ“§ zhengzhengyv@126.com (è´Ÿè´£å‰å…«æ¬¡ä½œä¸š)\n  æå‡Œæ¥ ï¼ŒğŸ“§ 1144747170@qq.","tags":null,"title":"æ•°ç†ç»Ÿè®¡","type":"docs"},{"authors":null,"categories":null,"content":"é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯å­¦æ´¾ é¢‘ç‡å­¦æ´¾ï¼ˆä¼ ç»Ÿå­¦æ´¾ï¼‰   é¢‘ç‡å­¦æ´¾è®¤ä¸ºæ ·æœ¬ä¿¡æ¯æ¥è‡ªæ€»ä½“ï¼Œä»…é€šè¿‡ç ”ç©¶æ ·æœ¬ä¿¡æ¯å¯ä»¥å¯¹æ€»ä½“ä¿¡æ¯åšå‡ºåˆç†çš„æ¨æ–­å’Œä¼°è®¡ï¼Œå¹¶ä¸”æ ·æœ¬è¶Šå¤šï¼Œå°±è¶Šå‡†ç¡®ã€‚\n  ä»£è¡¨æ€§äººç‰©ï¼šè´¹å¸Œå°” (R. A. Fisher, 1890-1962)\n  è´å¶æ–¯å­¦æ´¾  èµ·æºäºè‹±å›½å­¦è€…è´å¶æ–¯(T. Bayes, 1702-1761)åœ¨1763å¹´å‘è¡¨çš„è‘—åè®ºæ–‡ã€Šè®ºæœ‰å…³æœºé‡é—®é¢˜çš„æ±‚è§£ã€‹ æœ€åŸºæœ¬è§‚ç‚¹ï¼šä»»ä½•ä¸€ä¸ªæœªçŸ¥é‡éƒ½å¯ä»¥çœ‹ä½œæ˜¯éšæœºçš„ï¼Œåº”è¯¥ç”¨ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå»æè¿°æœªçŸ¥å‚æ•°ï¼Œè€Œä¸æ˜¯é¢‘ç‡æ´¾è®¤ä¸ºçš„å›ºå®šå€¼ã€‚è¿™ç§ä¿¡æ¯ç§°ä¸ºå…ˆéªŒä¿¡æ¯ï¼Œæ˜¯ä¸»è§‚ä¿¡æ¯ã€‚  è´å¶æ–¯å…¬å¼ æ¡ˆä¾‹ä¸€ï¼šå¤©èå·æ ¸æ½œè‰‡æœæ•‘ 1968å¹´5æœˆï¼Œç¾å›½æµ·å†›çš„å¤©èå·æ ¸æ½œè‰‡åœ¨å¤§è¥¿æ´‹äºšé€Ÿæµ·æµ·åŸŸçªç„¶å¤±è¸ªï¼Œæ½œè‰‡å’Œè‰‡ä¸Šçš„99åæµ·å†›å®˜å…µå…¨éƒ¨æ³æ— éŸ³ä¿¡ã€‚æŒ‰ç…§äº‹åè°ƒæŸ¥æŠ¥å‘Šçš„è¯´æ³•ï¼Œç½ªé­ç¥¸é¦–æ˜¯è¿™è‰˜æ½œè‰‡ä¸Šçš„ä¸€æšå¥‡æ€ªçš„é±¼é›·ï¼Œå‘å°„å‡ºå»åç«Ÿç„¶æ•Œæˆ‘ä¸åˆ†ï¼Œæ‰­å¤´å°„å‘è‡ªå·±ï¼Œè®©æ½œè‰‡ä¸­å¼¹çˆ†ç‚¸ã€‚\nä¸ºäº†å¯»æ‰¾å¤©èå·çš„ä½ç½®ï¼Œç¾å›½æ”¿åºœä»å›½å†…è°ƒé›†äº†åŒ…æ‹¬å¤šä½ä¸“å®¶çš„æœç´¢éƒ¨é˜Ÿå‰å¾€ç°åœºï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä½åå«John Cravençš„æ•°å­¦å®¶ï¼Œä»–çš„å¤´è¡”æ˜¯â€œç¾å›½æµ·å†›ç‰¹åˆ«è®¡åˆ’éƒ¨é¦–å¸­ç§‘å­¦å®¶â€ã€‚åœ¨æœå¯»æ½œè‰‡çš„é—®é¢˜ä¸Šï¼ŒCravenæå‡ºçš„æ–¹æ¡ˆä½¿ç”¨äº†è´å¶æ–¯å…¬å¼ã€‚\nè¿™ç§æ–¹æ³•å·²ç»ç”¨äº2009å¹´æ³•èˆª447å’Œ2014å¹´é©¬èˆª370çš„æœæ•‘ã€‚ å…·ä½“åŸç†å‚è€ƒï¼š Bayesian search theory\n20è‹±é‡Œæµ·åŸŸçš„æ¦‚ç‡å›¾ æ¡ˆä¾‹äºŒï¼šè”é‚¦å…šäººæ–‡é›†ä½œè€…å…¬æ¡ˆ  1787å¹´5æœˆï¼Œç¾å›½å„å·çš„ä»£è¡¨åœ¨è´¹åŸå¬å¼€åˆ¶å®ªä¼šè®®ã€‚ 1787å¹´9æœˆï¼Œç¾å›½çš„å®ªæ³•è‰æ¡ˆè¢«åˆ†å‘åˆ°å„å·è¿›è¡Œè®¨è®ºã€‚ä¸€æ‰¹åå¯¹æ´¾ä»¥â€œåè”é‚¦ä¸»ä¹‰è€…â€ä¸ºç¬”åï¼Œå‘è¡¨äº†å¤§é‡æ–‡ç« å¯¹è¯¥è‰æ¡ˆæå‡ºæ‰¹è¯„ã€‚å®ªæ³•èµ·è‰äººä¹‹ä¸€äºšå†å±±å¤§Â·æ±‰å¯†å°”é¡¿ç€æ€¥äº†ï¼Œä»–æ‰¾åˆ°æ›¾ä»»å¤–äº¤å›½åŠ¡ç§˜ä¹¦ï¼ˆå³åæ¥çš„å›½åŠ¡å¿ï¼‰çš„çº¦ç¿°Â·æ°ä¼Šï¼Œä»¥åŠçº½çº¦å¸‚å›½ä¼šè®®å‘˜éº¦è¿ªé€Šï¼Œä¸€åŒä»¥Publiusçš„ç¬”åå‘è¡¨æ–‡ç« ï¼Œå‘å…¬ä¼—è§£é‡Šä¸ºä»€ä¹ˆç¾å›½éœ€è¦ä¸€éƒ¨å®ªæ³•ã€‚ä»–ä»¬èµ°ç¬”å¦‚é£ï¼Œé€šå¸¸åœ¨ä¸€å‘¨ä¹‹å†…å°±ä¼šå‘è¡¨3-4ç¯‡æ–°çš„è¯„è®ºã€‚ 1788å¹´ï¼Œä»–ä»¬æ‰€å†™çš„85ç¯‡æ–‡ç« ç»“é›†å‡ºç‰ˆï¼Œè¿™å°±æ˜¯ç¾å›½å†å²ä¸Šè‘—åçš„ã€Šè”é‚¦å…šäººæ–‡é›†ã€‹ã€‚  æ¡ˆä¾‹äºŒï¼šè”é‚¦å…šäººæ–‡é›†ä½œè€…å…¬æ¡ˆ   1810å¹´ï¼Œæ±‰å¯†å°”é¡¿æ¥å—äº†ä¸€ä¸ªæ”¿æ•Œçš„å†³æ–—æŒ‘æˆ˜ã€‚åœ¨å†³æ–—ä¹‹å‰æ•°æ—¥ï¼Œæ±‰å¯†å°”é¡¿è‡ªçŸ¥æ—¶æ—¥ä¸å¤šï¼Œä»–åˆ—å‡ºäº†ä¸€ä»½ã€Šè”é‚¦å…šäººæ–‡é›†ã€‹çš„ä½œè€…åå•ã€‚\n  1818å¹´ï¼Œéº¦è¿ªé€Šåˆæå‡ºäº†å¦ä¸€ä»½ä½œè€…åå•ã€‚è¿™ä¸¤ä»½åå•å¹¶ä¸ä¸€è‡´ã€‚åœ¨85ç¯‡æ–‡ç« ä¸­ï¼Œæœ‰73ç¯‡æ–‡ç« çš„ä½œè€…èº«ä»½è¾ƒä¸ºæ˜ç¡®ï¼Œå…¶ä½™12ç¯‡å­˜åœ¨äº‰è®®ã€‚\n  1955å¹´ï¼Œå“ˆä½›å¤§å­¦ç»Ÿè®¡å­¦æ•™æˆFredrick Mostelleræ‰¾åˆ°èŠåŠ å“¥å¤§å­¦çš„å¹´è½»ç»Ÿè®¡å­¦å®¶David Wallanceï¼Œå»ºè®®ä»–è·Ÿè‡ªå·±ä¸€èµ·åšä¸€ä¸ªå°è¯¾é¢˜ï¼Œä»–æƒ³ç”¨ç»Ÿè®¡å­¦çš„æ–¹æ³•ï¼Œé‰´å®šå‡ºã€Šè”é‚¦å…šäººæ–‡é›†ã€‹çš„ä½œè€…èº«ä»½ã€‚\n  é‡‡ç”¨çš„æ–¹æ³•æ˜¯ä»¥è´å¶æ–¯å…¬å¼ä¸ºæ ¸å¿ƒçš„åˆ†ç±»ç®—æ³•ã€‚å…ˆæŒ‘é€‰ä¸€äº›èƒ½å¤Ÿåæ˜ ä½œè€…å†™ä½œé£æ ¼çš„è¯æ±‡ï¼Œåœ¨å·²ç»ç¡®å®šäº†ä½œè€…çš„æ–‡æœ¬ä¸­ï¼Œå¯¹è¿™äº›ç‰¹å¾è¯æ±‡çš„å‡ºç°é¢‘ç‡è¿›è¡Œç»Ÿè®¡ï¼Œç„¶åå†ç»Ÿè®¡è¿™äº›è¯æ±‡åœ¨é‚£äº›ä¸ç¡®å®šä½œè€…çš„æ–‡æœ¬ä¸­çš„å‡ºç°é¢‘ç‡ï¼Œä»è€Œæ ¹æ®è¯é¢‘çš„å·®åˆ«æ¨æ–­ä½œè€…å½’å±ã€‚\n  è´å¶æ–¯çš„å‘å±• ç»å…¸ç»Ÿè®¡å­¦çš„å›°éš¾  ç»å…¸ç»Ÿè®¡å­¦æ¯”è¾ƒé€‚åˆäºè§£å†³å°å‹çš„é—®é¢˜ï¼ŒåŒæ—¶éœ€è¦è¶³å¤Ÿå¤šçš„æ ·æœ¬æ•°æ® éƒ½å¤§æ•°æ®æ—¶ä»£äº†ï¼Œè¿˜å­˜åœ¨æ•°æ®ç¨€ç–æ€§é—®é¢˜å—ï¼Ÿè‡´ç—…åŸºå›   è´å¶æ–¯ç½‘ç»œå¸¦æ¥å·¥å…·é©å‘½  é¾™å·é£çš„å½¢æˆã€æ˜Ÿç³»çš„èµ·æºã€è‡´ç—…åŸºå› ã€å¤§è„‘çš„è¿ä½œæœºåˆ¶ç­‰ï¼Œè¦æ­ç¤ºéšè—åœ¨è¿™äº›é—®é¢˜èƒŒåçš„è§„å¾‹ï¼Œå°±å¿…é¡»ç†è§£å®ƒä»¬çš„æˆå› ç½‘ç»œ è´å¶æ–¯å…¬å¼+å›¾è®º  è´å¶æ–¯ç»Ÿè®¡çš„å‘å±• åº”ç”¨é¢†åŸŸ  è‡ªç„¶è¯­è¨€å¤„ç†ï¼šè®¡ç®—æœºç¿»è¯‘è¯­è¨€ã€è¯†åˆ«è¯­éŸ³ã€è®¤è¯†æ–‡å­—å’Œæµ·é‡æ–‡çŒ®çš„æ£€ç´¢   å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥æ¬¢è¿æ‚¨!\n  äººå·¥æ™ºèƒ½ã€æ— äººé©¾é©¶ åƒåœ¾çŸ­ä¿¡ã€åƒåœ¾é‚®ä»¶è¯†åˆ«  è´å¶æ–¯å†³ç­–  å¦‚ä½•åœ¨ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹æ‰¾é¤é¦†åƒé¥­ï¼Ÿ  ä¸€äº›è¯„ä»· **Berger (1985)**è¯´ï¼š\n â€œé˜²æ­¢è¯¯ç”¨çš„æœ€å¥½æ–¹æ³•æ˜¯ç»™äººä»¬åœ¨å…ˆéªŒä¿¡æ¯æ–¹é¢ä»¥é€‚å½“çš„æ•™è‚²ï¼Œå¦å¤–åœ¨è´å¶æ–¯åˆ†æçš„æœ€åæŠ¥å‘Šä¸­ï¼Œåº”å°†å…ˆéªŒåˆ†å¼€æ¥å†™ï¼Œä»¥ä¾¿ä½¿å…¶ä»–äººå¯¹ä¸»è§‚è¾“å…¥çš„åˆç†æ€§åšå‡ºè¯„ä»·ã€‚â€\n **Good (1973)**æ›´æ˜¯ç›´æˆªäº†å½“çš„è¯´ï¼š\n â€œä¸»è§‚ä¸»ä¹‰è€…ç›´æŠ’ä»–ä»¬çš„åˆ¤æ–­ï¼Œè€Œå®¢è§‚ä¸»ä¹‰è€…ä»¥å‡è®¾æ¥æ©ç›–å…¶åˆ¤æ–­ï¼Œå¹¶ä»¥æ­¤äº«å—ç§‘å­¦å®¢è§‚æ€§çš„è£è€€ã€‚â€\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5618aff72171059f504dafc78155192d","permalink":"/en/courses/bayes/chap1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap1/","section":"courses","summary":"é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯å­¦æ´¾ é¢‘ç‡å­¦æ´¾ï¼ˆä¼ ç»Ÿå­¦æ´¾ï¼‰   é¢‘ç‡å­¦æ´¾è®¤ä¸ºæ ·æœ¬ä¿¡æ¯æ¥è‡ªæ€»ä½“ï¼Œä»…é€šè¿‡ç ”ç©¶æ ·æœ¬ä¿¡æ¯å¯ä»¥å¯¹æ€»ä½“ä¿¡æ¯åšå‡ºåˆç†çš„æ¨æ–­å’Œä¼°è®¡ï¼Œå¹¶ä¸”æ ·æœ¬è¶Šå¤šï¼Œå°±è¶Šå‡†ç¡®ã€‚\n  ä»£è¡¨æ€§äººç‰©ï¼šè´¹å¸Œå°” (R. A. Fisher, 1890-1962)\n  è´å¶æ–¯å­¦æ´¾  èµ·æºäºè‹±å›½å­¦è€…è´å¶æ–¯(T. Bayes, 1702-1761)åœ¨1763å¹´å‘è¡¨çš„è‘—åè®ºæ–‡ã€Šè®ºæœ‰å…³æœºé‡é—®é¢˜çš„æ±‚è§£ã€‹ æœ€åŸºæœ¬è§‚ç‚¹ï¼šä»»ä½•ä¸€ä¸ªæœªçŸ¥é‡éƒ½å¯ä»¥çœ‹ä½œæ˜¯éšæœºçš„ï¼Œåº”è¯¥ç”¨ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå»æè¿°æœªçŸ¥å‚æ•°ï¼Œè€Œä¸æ˜¯é¢‘ç‡æ´¾è®¤ä¸ºçš„å›ºå®šå€¼ã€‚è¿™ç§ä¿¡æ¯ç§°ä¸ºå…ˆéªŒä¿¡æ¯ï¼Œæ˜¯ä¸»è§‚ä¿¡æ¯ã€‚  è´å¶æ–¯å…¬å¼ æ¡ˆä¾‹ä¸€ï¼šå¤©èå·æ ¸æ½œè‰‡æœæ•‘ 1968å¹´5æœˆï¼Œç¾å›½æµ·å†›çš„å¤©èå·æ ¸æ½œè‰‡åœ¨å¤§è¥¿æ´‹äºšé€Ÿæµ·æµ·åŸŸçªç„¶å¤±è¸ªï¼Œæ½œè‰‡å’Œè‰‡ä¸Šçš„99åæµ·å†›å®˜å…µå…¨éƒ¨æ³æ— éŸ³ä¿¡ã€‚æŒ‰ç…§äº‹åè°ƒæŸ¥æŠ¥å‘Šçš„è¯´æ³•ï¼Œç½ªé­ç¥¸é¦–æ˜¯è¿™è‰˜æ½œè‰‡ä¸Šçš„ä¸€æšå¥‡æ€ªçš„é±¼é›·ï¼Œå‘å°„å‡ºå»åç«Ÿç„¶æ•Œæˆ‘ä¸åˆ†ï¼Œæ‰­å¤´å°„å‘è‡ªå·±ï¼Œè®©æ½œè‰‡ä¸­å¼¹çˆ†ç‚¸ã€‚\nä¸ºäº†å¯»æ‰¾å¤©èå·çš„ä½ç½®ï¼Œç¾å›½æ”¿åºœä»å›½å†…è°ƒé›†äº†åŒ…æ‹¬å¤šä½ä¸“å®¶çš„æœç´¢éƒ¨é˜Ÿå‰å¾€ç°åœºï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä½åå«John Cravençš„æ•°å­¦å®¶ï¼Œä»–çš„å¤´è¡”æ˜¯â€œç¾å›½æµ·å†›ç‰¹åˆ«è®¡åˆ’éƒ¨é¦–å¸­ç§‘å­¦å®¶â€ã€‚åœ¨æœå¯»æ½œè‰‡çš„é—®é¢˜ä¸Šï¼ŒCravenæå‡ºçš„æ–¹æ¡ˆä½¿ç”¨äº†è´å¶æ–¯å…¬å¼ã€‚\nè¿™ç§æ–¹æ³•å·²ç»ç”¨äº2009å¹´æ³•èˆª447å’Œ2014å¹´é©¬èˆª370çš„æœæ•‘ã€‚ å…·ä½“åŸç†å‚è€ƒï¼š Bayesian search theory","tags":null,"title":"ç¬¬1ç« ","type":"docs"},{"authors":null,"categories":null,"content":"\rThis note is adapted to the paper entitled â€œVariation Inference: A Review for Statisticansâ€ by Blei et al.Â (2017).\nBasic ideas of VI\rLet \\(z=z_{1:m}\\) be the latent variables that govern the distribution of the data (observations) \\(x=x_{1:n}\\).\rThe prior is denoted by \\(p(z)\\). The likelihood is \\(p(x|z)\\). The posterior thus is given by\r\\[p(z|x)=\\frac{p(z)p(x|z)}{p(x)}\\propto p(z)p(x|z).\\]\rThe denominator \\(p(x)\\) contains the marginal density of the obsevations, also called the evidence.\n\rABC algorithms provide a kind of approximations of the posterior in the context of simulation.\n\rVI provides another kind of approximations of the posterior by minizing the Kullback-Leibler (KL) divergence to the exact posterior over a family of approximate densities \\(\\mathcal Q\\). That is\r\\[q^*(z) = \\arg \\min_{q(z)\\in\\mathcal Q} KL (q(z)||p(z|x)).\\]\rThe KL divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. The formal definition of KL divergence is \\[KL(p_1||p_2)=E_{p_1}[\\log (p_1(x)/p_2(x))]=E_{p_1}[\\log p_1(x)]-E_{p_1}[\\log p_2(x)].\\]\n\r\\(KL(p_1||p_2)\\ge 0\\), where the equality holds iff \\(p_1(z)=p_2(z)\\) w.p.1. This can be proved via Jensen inequality. Noting that\r\\[KL(p_1||p_2)=-E_{p_1}[\\log (p_2(x)/p_1(x))]\\ge -\\log E_{p_1}[p_2(x)/p_1(x)]=-\\log \\int \\frac{p_2(x)}{p_1(x)} p_1(x) dx =0.\\]\rThe equality holds iff \\(p_2(x)/p_1(x)\\) is constant w.p.1.\n\rIt is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread, i.e, \\(KL(p_1||p_2)\\neq KL(p_2||p_1)\\).\n\rIt also does not satisfy the triangle inequality \\(KL(p_1||p_2)+KL(p_2||p_3)\\ge KL(p_1||p_3)\\).\n\r\rNote that the objective is not computable because it requires computing \\(\\log p(x)\\), which is typically infeasible. To see why,\r\\[KL (q(z)||p(z|x))=E_{q}[\\log q(z)]-E_{q}[\\log p(z|x)]=E_{q}[\\log q(z)]-E_{q}[\\log p(z)]-E_{q}[\\log p(x|z)]+\\log p(x).\\]\rAs a result, we optimize an alternative objective that is equivalent to the KL up to an added constant,\r\\[\\begin{equation}\rELBO(q)=E_{q}[\\log(p(x,z)/q(z))]=E_{q}[\\log p(z)]+E_{q}[\\log p(x|z)]-E_{q}[\\log q(z)].\\label{eq:elbo}\r\\end{equation}\\]\rThis function is called the evidence lower bound (ELBO). It is easy to see that\r\\[ELBO(q)=-KL (q(z)||p(z))+\\log p(x).\\]\rSince the log-evidence is constant,\r\\[\\begin{equation}\rq^*(z) = \\arg \\min_{q(z)\\in\\mathcal Q} KL (q(z)||p(z|x))=\\arg\\max_{q(z)\\in\\mathcal Q} ELBO(q).\r\\label{eq:elboopt}\r\\end{equation}\\]\n\rIt follows from \\(KL(p_1||p_2)\\ge 0\\) that \\(ELBO(q)\\le \\log p(x)\\) for any \\(q\\). This means the ELBO is a lower-bound of the log-evidence, explaining its name.\n\rFrom the second equality in , maximazing the ELBO mirrors the usual balance between likelihood and prior.\n\r\r\rThe Mean-Field Variational Family\rThe complexity of the family determines the complexity of the optimization; it is more difficulty to optimize over a complex family than a simple family. We next focus on the mean-field variational family,\r\\[\\begin{equation}\rq(z) = \\prod_{j=1}^m q_j(z_j).\\label{eq:mfvf}\r\\end{equation}\\]\rEach latent variable \\(z_j\\) is governed by its own variational factor, the density \\(q_j\\). That is \\(z_j\\stackrel{ind}\\sim q_j\\).\nOne may specify the parametric form of the individual variational factors. In principle, each can take on any parametric form appropriate to the corresponding random variable.\n\rA continous variable might have a Gaussian factor.\n\rA categorical variable will typically have a categorical factor.\n\r\r\rCoordinate Ascent Mean-Field VI\rThis section describe one of the most commonly used algorithms for solving the optimizatin problem subject to the mean-field variational family . The coordinate ascent VI (CAVI) iteratively optimizes each factor of the mean-field variation density, while holding the others fixed. It climbs the ELBO to a local optimum.\nLet \\(z_{-j}\\) be the vector of \\(z\\) by removing the \\(j\\)th component \\(z_j\\), and let \\(p(z_j|z_{-j},x)\\) be the complete conditional of \\(z_j\\) given all of the other latent variables in the model and the observations.\rFixing the other variational factors, \\(q_\\ell(z_\\ell)\\), \\(\\ell\\neq j\\), the optimal \\(q_j(z_j)\\) is then propotional to the exponentiated expected log of the complete conditional,\r\\[\\begin{equation}\rq_j^*(z_j) \\propto \\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\}\\propto \\exp\\{E_{-j}[\\log p(x,z)]\\},\\label{eq:cavi}\r\\end{equation}\\]\rwhere the expectation is with respective to the currently fixed variational density over \\(z_{-j}\\), i.e, \\(\\prod_{\\ell\\neq j} q_\\ell (z_\\ell)\\). To see why, when fixing the other variational factors, \\(q_\\ell(z_\\ell)\\), \\(\\ell\\neq j\\), it follows from that\r\\[\\begin{align*}\rELBO(q) \u0026amp;= ELBO(q_j) = E_{q}[\\log(p(x,z)/q(z))] \\\\\u0026amp;= E_{q}[\\log(p(z_{j}|z_{-j},x)p(z_{-j},x)/q_{j}(z_j)/q_{-j}(z_{-j}))]\\\\\r\u0026amp; = E_{q}[\\log(p(z_{j}|z_{-j},x)/q_j(z_j)] + \\text{const}\\\\\r\u0026amp;=E_{q_j}[E_{-j}[\\log(p(z_{j}|z_{-j},x)]-\\log q_j(z_j)] + \\text{const}\\\\\r\u0026amp;=E_{q_j}[\\log (\\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\}/q_j(z_j))]+ \\text{const}\\\\\r\u0026amp;= - KL(q_j(z_j)||c\\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\})+ \\text{const},\r\\end{align*}\\]\rwhere \\(c\\) is a normalized constant such that \\(c\\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\) is a PDF.\rSince \\(KL\\ge 0\\), the maximization of ELBO attains at \\(q_j(z_j)=c\\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\}\\) w.p.1. Therefore, the optimal \\(q_j(z_j)\\) is propotional to \\(\\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\}\\).\nCAVI goes as follows: Initizlize the variational factors \\(q_j(z_j)\\); Update each factor of the mean-field variation density by , while holding the others fixed, until the ELBO converges. To check the convergence, we may compute the ELBO after a (few) loop of all the factors.\nThe ELBO is (generally) a nonconvex objective function. CAVI only guarantees to a local optimum, which can be sensitive to iniitialization. Also, the updated variational factor should have a closed form.\n\rApplication I: Bayesian mixture of Gaussians\rAs a concrete example, we consider a Bayesian mixture of unit-variance univariate Gaussians. There are \\(K\\) mixture components, corresponding to \\(K\\) Gaussian distributions with means \\(\\mu=(\\mu_1,\\dots,\\mu_K)\\). Given the means, the data is generated via\r\\[x_i|\\mu,\\alpha\\stackrel{iid}{\\sim} \\sum_{k=1}^K \\alpha_{k} N(\\mu_k,1),\\]\rwhere \\(\\alpha_{k}\u0026gt;0\\) is the probablity drawn from the \\(k\\)th Guassian with \\(\\sum_{k=1}^K \\alpha_{k} =1\\).\nWe now add some latent variables to reformulate the model. This is actually a technique of data augment. Let the latent variable \\(c_i\\) be an indicator \\(K\\)-vector, all zeros expect for a one in the position corresponding to \\(x_i\\)â€™s cluster. There are \\(K\\) possible values for \\(c_i\\). As a result, \\(x_i|\\mu,c_i\\sim N(c_i^\\top \\mu,1)\\), \\(c_i\\sim \\text{categorical}(\\alpha)=:CG(\\alpha)\\), where \\(\\alpha=(\\alpha_{1},\\dots,\\alpha_{K})\\). Assume that the mean parameters are drawn independently from a common prior \\(p(\\mu_k)\\sim N(0,\\sigma^2)\\); the prior variance \\(\\sigma^2\\) ia a hyperparameter; and the prior for the latent indicators is \\(c_i\\sim CG(1/K,1/K,\\dots,1/K)\\).\nThe full hierarchical model is\r\\[\\begin{align}\r\\mu_k\u0026amp;\\stackrel{iid}{\\sim} N(0,\\sigma^2), \u0026amp; k=1,\\dots,K,\\\\\rc_i\u0026amp;\\stackrel{iid}{\\sim} \\text{categorical}(1/K,1/K,\\dots,1/K), \u0026amp; i=1,\\dots, n,\\\\\rx_i|\\mu,c_i\u0026amp;\\stackrel{ind}{\\sim} N(c_i^\\top \\mu,1), \u0026amp;i=1,\\dots, n.\r\\end{align}\\]\rThe latent variables are \\(z=(\\mu, c)\\). The joint density of latent and observed variables is\r\\[p(\\mu,c,x) = p(\\mu) \\prod_{i=1}^n p(c_i)p(x_i|c_i,\\mu).\\]\rThe evidence is\r\\[\\begin{align}\rp(x)= \\int p(\\mu) \\prod_{i=1}^n \\sum_{c_i} p(c_i)p(x_i|c_i,\\mu) d\\mu=\\sum_{c_1,\\dots,c_n}\\prod_{i=1}^n p(c_i) \\int p(\\mu) \\prod_{i=1}^n p(x_i|c_i,\\mu) d\\mu.\\label{eq:gmmevi}\r\\end{align}\\]\rThanks to conjugacy between the Gaussian prior on the components and the Gaussian likelihood, each individual integral \\(I(c_1,\\dots,c_n):=\\int p(\\mu) \\prod_{i=1}^n p(x_i|c_i,\\mu) d\\mu\\) is computable. However, the total cases of the configuration \\((c_1,\\dots,c_n)\\) is \\(K^n\\). As a result, the complexity of computing is \\(O(K^n)\\), which is infeasible for moderate sample size \\(n\\) and \\(K\\). For example, when \\(K=3\\) and \\(n=100\\), \\(K^n = 3^{100}\\approx 5.2\\times 10^{47}\\). In this sense, we can say that the evidence is intractable.\nIn VI, we choose the mean-field variational family as the form\r\\[q(\\mu,c) = \\prod_{k=1}^K q(\\mu_k;m_k,s_k^2)\\prod_{i=1}^nq(c_i;\\psi_i),\\]\rwhere the variational factor \\(q(\\mu_k;m_k,s_k^2)\\) for the mean \\(\\mu_i\\) is a Guassian \\(N(m_k,s_k^2)\\), and the variational factor \\(q(c_i;\\psi_i)\\) for the indicator is \\(CG(\\psi_i)\\).\rBy , we have\r\\[\\begin{align}\rELBO(m,s^2,\\psi)\u0026amp;=E_{q}[\\log p(z)]+E_{q}[\\log p(x|z)]-E_{q}[\\log q(z)]\\notag\\\\\r\u0026amp;=\\sum_{k=1}^K E_{\\mu_k\\sim N(m_k,s_k^2)}[\\log p(\\mu_k)]\\notag\\\\\r\u0026amp;\\quad+\\sum_{i=1}^n (E_{c_i\\sim CG(\\psi_i)}[\\log p(c_i)]+E_{c_i\\sim CG(\\psi_i),\\mu\\sim N(m,\\text{diag}(s^2))}[\\log p(x_i|c_i,\\mu)])\\notag\\\\\r\u0026amp;\\quad-\\sum_{i=1}^n E_{c_i\\sim CG(\\psi_i)}[\\log q(c_i;\\psi_i)]-\\sum_{k=1}^K E_{\\mu_k\\sim N(m_k,s_k^2)}[\\log q(\\mu_k;m_k,s_k^2)]\\notag\\\\\r\u0026amp;=\\frac K 2-K\\log\\sigma-n\\log K-\\frac 12 n\\log(2\\pi)+\\frac 1 2\\sum_{i=1}^n x_i^2+\\sum_{k=1}^K\\left[\\log(s_k)-\\frac{m_k^2+s_k^2}{2\\sigma^2}\\right] \\notag\\\\\r\u0026amp;\\quad-\\sum_{i=1}^n\\sum_{k=1}^K\\psi_{ik}\\left[\\frac{m_k^2+s_k^2}2-x_im_k+\\log(\\psi_{ik}) \\right]\\notag\\\\\r\u0026amp;=\\sum_{k=1}^K\\left[\\log(s_k)-\\frac{m_k^2+s_k^2}{2\\sigma^2}\\right] -\\sum_{i=1}^n\\sum_{k=1}^K\\psi_{ik}\\left[\\frac{m_k^2+s_k^2}2-x_im_k+\\log(\\psi_{ik}) \\right]+\\text{const}.\\label{eq:BMGelbo}\r\\end{align}\\]\nNote that all the expectation in the ELBO can be computed in closed form. There are many methods to find a local optimum of .\n\rNewton-Raphson algorithm. It suffices to find the root of \\(\\nabla ELBO(m,s^2,\\psi) = 0\\). Let \\(\\lambda = (m,s^2,\\psi)\\in \\mathbb{R}^{2K+n(K-1)}\\) be a vector of parameters. The Newton-Raphson method uses the iteration\r\\[\\lambda^{(t+1)}=\\lambda^{(t)}-(D^2 ELBO(\\lambda^{(t)}))^{-1} \\nabla ELBO(\\lambda^{(t)}),\\]\rwhere \\(D^2 ELBO(\\lambda^{(t)})\\) is the Hessian matrix.\n\rGradient ascent algorithm. It is a first-order iterative optimization algorithm for finding a local maximum. The iteration is\r\\[\\lambda^{(t+1)}=\\lambda^{(t)} + \\eta_t \\nabla ELBO(\\lambda^{(t)}),\\]\rwhere \\(\\eta_t\u0026gt;0\\) is the learning rate.\n\rCAVI. The iteration is\r\\[\\begin{align}\r\\psi_{t+1,ik} \u0026amp;\\propto \\exp\\{E[\\mu_k]x_i-E[\\mu_k^2]/2\\}\\propto \\exp\\{m_{t,k}x_i-(m_{t,k}^2+s_{t,k}^2)/2\\},\\\\\rm_{t+1,k}\u0026amp;=\\frac{\\sum_{i=1}^n \\psi_{t+1,ik}x_i}{1/\\sigma^2+\\sum_{i=1}^n\\psi_{t+1,ik}}, \\label{eq:miter} \\\\\rs_{t+1,k}^2\u0026amp;=\\frac{1}{1/\\sigma^2+\\sum_{i=1}^n\\psi_{t+1,ik}}, k=1,\\dots,K, i=1,\\dots,n, \\label{eq:siter}\r\\end{align}\\]\rwhere \\(\\psi_{t,\\cdot}, m_{t,\\cdot},s^2_{t,\\cdot}\\) denote the parameters at the step \\(t\\). Note that the algorithm does not need the initial varitional factors for \\(\\psi_i\\).\n\r\rNext, we implement the CAVI algorithm for \\(K=5\\) and \\(n=10^3\\). After a few steps, we can see that the ELBO converges.\nset.seed(1)\r## data generation\rK = 5 # the number of clusters\rn = 1000 # the number of data x_i\rmu = matrix(rnorm(K,mean=0.2,sd=2),ncol = 1) # the means of the K clusters\rc = sample(1:K,n,replace = T) # the indicator\rx = matrix(mu[c]+rnorm(n),ncol = 1)\rplot(density(x),xlab = \u0026#39;x\u0026#39;,main=\u0026#39;kernel density of the data\u0026#39;)\rsig = 1 # hyperparameter for the variance of mu_i\r## ELBO minus a constant\relbo \u0026lt;- function(m,s,psi){\rre = sum(log(s)-(m^2+s^2)/(2*sig^2))- sum(psi%*%(m^2+s^2)/2)+\rt(x)%*%psi%*%m-sum(log(psi)*psi)\rreturn(re)\r}\r## iteration for CAVI\rcavi \u0026lt;- function(m,s){\rpsi = matrix(0,n,K)\rfor(i in 1:n){\rtmp = x[i]*m-(m^2+s^2)/2\rmtmp = max(tmp)\rlogsum = mtmp+log(sum(exp(tmp-mtmp)))\rpsi[i,] = exp(tmp-logsum)\r}\rde = 1/sig^2+colSums(psi)\rm = t(x)%*%psi/de\rs = sqrt(1/de)\rreturn(list(m_next=matrix(m,ncol = 1),s_next=matrix(s,ncol = 1),psi_next=psi))\r}\r## initialization\rnstep = 1e4 # maximal steps\rtolerance = 1e-6 # tolerance for the relative change\rm = matrix(rnorm(K,0,1),K,1) s = matrix(5,K,1)\rELBO = matrix(0,nstep,1)\rstep = 1\rrelative_change = tolerance+1\rwhile(TRUE){\rpara = cavi(m,s)\rm = para$m_next\rs = para$s_next\rpsi = para$psi_next\rELBO[step] = elbo(m,s,psi)\rif(step\u0026gt;1)\rrelative_change = (ELBO[step]-ELBO[step-1])/ELBO[step]\rif(step==nstep | abs(relative_change)\u0026lt;tolerance){ # stopping rule\rbreak\r}\relse{\rstep = step+1\r}\r}\rhatc = apply(psi, 1,which.max)\rcenter = cbind(sort(mu),sort(m))\rcolnames(center) = c(\u0026#39;True Centers\u0026#39;,\u0026#39;VI Means\u0026#39;)\rknitr::kable(center)\r\r\rTrue Centers\rVI Means\r\r\r\r-1.4712572\r-1.5346393\r\r-1.0529076\r-0.9663787\r\r0.5672866\r0.3772178\r\r0.8590155\r0.9019340\r\r3.3905616\r3.4156061\r\r\r\rplot(1:step,ELBO[1:step],type = \u0026#39;b\u0026#39;,xlab = \u0026#39;Step\u0026#39;,ylab=\u0026#39;ELBO\u0026#39;,pch = 16)\rplot(x,col=hatc)\rabline(h=m,col=1:5,lty=2,lwd=2)\r\rExponential Family Conditionals\rAre there specific forms for the local variational\rapproximations in which we can easily compute closed-form conditional ascent updates? Yes, the answer is exponential family conditionals.\nConsider the generic model \\(p(z,x)\\) and suppose each complete conditional is in the exponential family:\r\\[\\begin{equation}\rp(z_j|z_{-j},x) = h(z_j)\\exp\\{\\eta_j(z_{-j},x)^\\top t(z_j)-a(\\eta_j(z_{-j},x))\\},\r\\end{equation}\\]\rwhere \\(t(z_j)\\) is the sufficient statistic, and \\(\\eta_j(z_{-j},x)\\) are the natural parameters.\nConsider mean-field VI for this class of models, where \\(q(z)\\) is given by . The update becomes\r\\[\\begin{align}\rq_j^*(z_j) \u0026amp;\\propto \\exp\\{E_{-j}[\\log p(z_j|z_{-j},x)]\\}\\\\\r\u0026amp;=\\exp\\{\\log h(z_j)+ E_{-j}[\\eta_j(z_{-j},x)^\\top t(z_j)]-E_{-j}[a(\\eta_j(z_{-j},x))]\\}\\\\\r\u0026amp;\\propto h(z_j)\\exp\\{E_{-j}[\\eta_j(z_{-j},x)]^\\top t(z_j)\\}.\r\\end{align}\\]\rThis updata reveals the parametric form of the optimal VI factors. Each one is in the same exponential family as its corresponding complete conditional. Let \\(\\nu_j\\) denote the variational parameter for the \\(j\\)th variational factor. When we update each factor, we set its parameter equal to the expected parameter of the complete conditional,\r\\[\\nu_j = E_{-j}[\\eta_j(z_{-j},x)].\\]\nThere are many popular models fall into this category, including:\n\rBayesian mixtures of exponential family models with conjugate priors.\rHierarchical hidden Markov models.\rKalman filter models and switching Kalman filters.\rMixed-membership models of exponential families.\rFactorial mixtures / hidden Markov models of exponential families.\rBayesian linear regression.\rAny model containing only conjugate pairs and multinomials.\r\rSome popular models do not fall into this category, including:\n\rBayesian logistic regression and other nonconjugate Bayesian generalized linear models.\rCorrelated topic model, dynamic topic model.\rDiscrete choice models.\rNonlinear matrix factorization models.\r\r\rStochastic Gradient Variational Inference\rCAVI may require interating thought the entire dataset at each iteration. As the dataset size grows, each\riteration becomes more computationally expensive (see and ). In more realistic models, the gradient of the ELBO is rarely available in closed form. Stochastic gradient methods (Robbins\rand Monro, 1951) are useful for optimizing an objective function whose gradient can be unbiasedly estimated. Stochastic gradient variational inference (SGVI) becomes an alternative to coordinate ascent. SGVI combines gradients and stochastic optimazation.\nNow we rewrite the ELBO as a function of variational parameters \\(\\lambda\\) (a vector), denoted by \\(\\mathcal L(\\lambda)\\). Let \\(\\nabla_\\lambda\\mathcal L(\\lambda)\\) be the gradient vector of \\(\\mathcal L(\\lambda)\\) w.r.t. \\(\\lambda\\). Gradient ascent algorithm iterates\r\\[\\lambda^{(t+1)}=\\lambda^{(t)} + \\eta_t \\nabla_\\lambda\\mathcal L(\\lambda^{(t)}),\\quad t=0,\\dots,T.\\]\rLet \\(\\widehat{\\nabla_\\lambda\\mathcal L(\\lambda)}\\) be an unibased estimate of \\(\\nabla_\\lambda\\mathcal L(\\lambda)\\). SGVI iterates as follow,\r\\[\\begin{equation}\r\\lambda^{(t+1)}=\\lambda^{(t)} + \\eta_t \\widehat{\\nabla_\\lambda\\mathcal L(\\lambda^{(t)})},\\quad t=1,\\dots,T.\\label{eq:sgdite}\r\\end{equation}\\]\rUnder certain regularity conditions, and the learning rates satisfy the Robbins-Monro conditions\r\\[\\sum_{t=0}^\\infty \\eta_t=\\infty,\\ \\sum_{t=0}^\\infty \\eta_t^2\u0026lt;\\infty,\\]\rthe iterations converge to a local optimum (Robbins and Monro, 1951). Many sequences will satisfy these conditions, for example, \\(\\eta_t=t^{-\\kappa}\\) for \\(\\kappa\\in(0.5,1]\\). Adaptive learning rates are currently\rpopular (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2015; Kingma and Ba, 2015). Adam is a promising method, which is pulished in\n\rKingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the International Conference on Learning Representations. (Cited by 43201 on 2020/5/27)\n\rThe name Adam\ris derived from adaptive moment estimation.\nOne important thing is to obtain an unbiased estimate of the gradient \\(\\nabla_\\lambda\\mathcal L(\\lambda)\\).\rThe variational density is now written as \\(q(z;\\lambda)\\). Then,\r\\[\\nabla_\\lambda\\mathcal L(\\lambda) = \\nabla_\\lambda E_q[\\log p(z,x)]-\\nabla_\\lambda E_q[\\log q(z;\\lambda)].\\]\rNote that in some cases (such as mean-field variational family with Gaussian or categorical factors) of variational densities, \\(E_q[\\log q(z;\\lambda)]\\) is analytically solvable, and so does its gradient \\(\\nabla_\\lambda E_q[\\log q(z;\\lambda)]\\). We thus focus on estimating \\(\\nabla_\\lambda E_q[\\log p(z,x)]\\). Suppose that\r\\(\\nabla_\\lambda E_q[\\log p(z,x)]\\) can be written as an expectation, i.e.,\r\\[\\begin{equation}\r\\nabla_\\lambda E_q[\\log p(z,x)]=E[h(z)].\\label{eq:expform}\r\\end{equation}\\]\rThen one can easily find an unbiased estimate of \\(\\nabla_\\lambda\\mathcal L(\\lambda)\\),\r\\[\\begin{equation}\r\\widehat{\\nabla_\\lambda\\mathcal L(\\lambda^{(t)})}=\\frac 1 N \\sum_{i=1}^N h(z_i) -\\nabla_\\lambda E_q[\\log q(z;\\lambda)],\\label{eq:sgd}\r\\end{equation}\\]\rwhere \\(z_i\\) are Monte Carlo samples or quasi-Monte Carlo samples.\nThere are two tricks to obtain the expectation form . Allowing the interchange of integration and differentiation, the score function gradient method\rmakes use of\r\\[\\begin{align}\r\\nabla_\\lambda E_q[\\log p(z,x)]\u0026amp;=\\nabla_\\lambda\\int \\log p(z,x) q(z;\\lambda)d z\\\\\r\u0026amp;= \\int \\log p(z,x) \\nabla_\\lambda q(z;\\lambda)d z\\\\\r\u0026amp;= \\int \\log p(z,x) (\\nabla_\\lambda \\log q(z;\\lambda)) q(z;\\lambda)d z\\\\\r\u0026amp;=E_q[\\log p(z,x) \\nabla_\\lambda \\log q(z;\\lambda)],\r\\end{align}\\]\rachieving the form with \\(h(z)=\\log p(z,x) \\nabla_\\lambda \\log q(z;\\lambda)\\) and \\(z\\sim q(z;\\lambda)\\). On the other hand, the reparameterization method rewrites the expectation \\(E_q[\\log p(z,x)]\\)\ras an expectation w.r.t. a density independently of the parameter \\(\\lambda\\), say, \\(E_{p_0}[\\log p(h(z;\\lambda),x)]\\), where \\(h(z;\\lambda)\\sim q(z;\\lambda)\\) and \\(z\\sim p_0(z)\\) independent of \\(\\lambda\\). As a result,\rby allowing the interchange of integration and differentiation,\r\\[\\begin{align}\r\\nabla_\\lambda E_q[\\log p(z,x)]\u0026amp;=\\nabla_\\lambda E_{p_0}[\\log p(h(z;\\lambda),x)]\\\\\r\u0026amp;= \\nabla_\\lambda \\int \\log (p(h(z;\\lambda),x)) p_0(z) d z\\\\\r\u0026amp;= \\int \\nabla_\\lambda \\log (p(h(z;\\lambda),x)) p_0(z) d z\\\\\r\u0026amp;=E_{p_0}[\\nabla_\\lambda \\log (p(h(z;\\lambda),x))],\r\\end{align}\\]\rachieving the form with \\(h(z)=\\nabla_\\lambda \\log (p(h(z;\\lambda),x))\\) and \\(z\\sim p_0(z)\\).\nThe reparameterization gradient typically exhibits lower variance than the score function gradient but\ris restricted to models where the variational family can be reparametrized via a differentiable mapping. We refer to the article\n\rXu, M., Quiroz, M., Kohn, R., \u0026amp; Sisson, S. A. (2018). Variance reduction properties of the reparameterization trick. arXiv preprint arXiv:1809.10330.\n\rThe convergence of the gradient ascent scheme in tends\rto be slow when gradient estimators have a high variance.\rTherefore, various approaches for reducing the variance of\rboth gradient estimators exist; e.g.Â control variates,\rRao-Blackwellization, importance sampling as well as quasi-Monte Carlo. For the use of qausi-Monte Carlo in VI, we refer to\n\rBuchholz, A., Wenzel, F., \u0026amp; Mandt, S. (2018). Quasi-monte carlo variational inference. arXiv preprint arXiv:1807.01604.\n\r\rBayesian multinomial logistic regression\rThe famous (Fisherâ€™s or Andersonâ€™s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\nLet \\(y_i\\in \\{0,\\dots,K\\}\\) be the categorial data, which relates to \\(x_i = (x_{i1},\\dots,x_{ip})^\\top\\). The multinomial logistic regression is given by\n\\[\\begin{equation}\rP(y_i=k|\\beta) = \\frac{\\exp\\{x_i^\\top \\beta_k\\}}{\\sum_{j=0}^K \\exp\\{x_i^\\top \\beta_j\\}},\\quad k=0,\\dots,K,\r\\end{equation}\\]\nwhere \\(\\beta_k\\in \\mathbb{R}^{p\\times 1}\\) and the parameters are \\(\\beta=(\\beta_1,\\dots,\\beta_K)\\), and we set \\(\\beta_0=0\\) for indentifying the model.\rThe prior we used is \\(\\beta_k\\stackrel{iid}\\sim N(0,\\sigma_0^2I_p),k=1,\\dots,K\\).\rWe treated the designed matrix \\(X\\) as a constant matrix.\nThe variational density we used is Gaussian, i.e., \\(q(\\beta_{ij})\\sim N(\\mu_{ij},\\sigma_{ij}^2)\\). Let \\(\\psi_{ij}=\\log (\\sigma_{ij})\\) so that \\(q(\\beta_{ij})\\sim N(\\mu_{ij},\\exp(2\\psi_{ij})).\\) Now the variational parameters are \\(\\mu_{ij}\\) and \\(\\psi_{ij}\\). We encapsulate them in a vector \\(\\lambda\\). Denote the ELBO by \\(L(\\lambda)\\). We thus have\r\\[\\begin{align}\rL(\\lambda) \u0026amp;= E_q[\\log p(\\beta)] + E_q[\\log p(y|\\theta)] - E_q[\\log q(\\beta)]\\\\\r\u0026amp;=\\sum_{ik}\\left(\\psi_{ik}-\\frac {\\mu_{ik}^2+\\exp(2\\psi_{ik})}{2\\sigma^2_0}\\right) + \\sum_{i=1}^n E_q\\left[\\log \\left(\\frac{\\sum_{k=0}^K\\exp\\{x_i^\\top \\beta_k\\}1\\{y_i=k\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top \\beta_k\\}}\\right)\\right]+\\text{const}\\\\\r\u0026amp;=L_1(\\lambda)+L_2(\\lambda)+\\text{const}.\r\\end{align}\\]\nIt is easy to see that\r\\[\\frac{\\partial L_1(\\lambda)}{\\partial \\mu_{ik}}=-\\frac{\\mu_{ik}}{\\sigma_0^2},\\quad \\frac{\\partial L_1(\\lambda)}{\\partial \\psi_{ik}}=1-\\frac{\\exp(2\\psi_{ik})}{\\sigma_0^2}.\\]\rThis implies \\(\\nabla_\\lambda L_1(\\lambda)\\) has a close form.\nThe score function gradient for \\(L_2(\\lambda)\\) is given by\r\\[\\nabla_\\lambda L_2(\\lambda)=\\sum_{i=1}^n E_q\\left[\\log \\left(\\frac{\\sum_{k=0}^K\\exp\\{x_i^\\top \\beta_k\\}1\\{y_i=k\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top \\beta_k\\}}\\right)\\nabla_\\lambda \\log q(\\beta;\\lambda)\\right],\\]\rwhere\r\\[\\frac{\\partial \\log q(\\beta;\\lambda)}{\\partial \\mu_{ik}}=\\frac{\\beta_{ik}-\\mu_{ik}}{\\exp(2\\psi_{ik})},\\ \\frac{\\partial \\log q(\\beta;\\lambda)}{\\partial \\psi_{ik}}=\\frac{(\\beta_{ik}-\\mu_{ik})^2}{\\exp(2\\psi_{ik})}-1.\\]\nWe now rewrites the expectation \\(L_2(\\lambda)\\) as\r\\[\\begin{align}\rL_2(\\lambda)\u0026amp;=\\sum_{i=1}^n E_q\\left[\\log \\left(\\frac{\\sum_{k=0}^K\\exp\\{x_i^\\top \\beta_k\\}1\\{y_i=k\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top \\beta_k\\}}\\right)\\right]\\\\\r\u0026amp;=\\sum_{i=1}^n E\\left[\\log \\left(\\frac{\\sum_{k=0}^K\\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}1\\{y_i=k\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}}\\right)\\right],\\\\\r\\end{align}\\]\rwhere \\(z_k\\stackrel{iid}\\sim N(0,I_p)\\).\rThe\rreparameterization gradient is given by\r\\[\\begin{align}\r\\nabla_\\lambda L_2(\\lambda)\u0026amp;= \\sum_{i=1}^n E\\left[\\nabla_\\lambda\\log \\left(\\frac{\\sum_{k=0}^K\\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}1\\{y_i=k\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}}\\right)\\right]\\\\\r\u0026amp;=:\\sum_{i=1}^n E[\\nabla_\\lambda h_{i}(z;\\lambda)].\r\\end{align}\\]\nwhere\r\\[\\begin{align}\r\\frac{\\partial h_{i}(z;\\lambda)}{\\mu_{jk}}\u0026amp;= x_{ij}1\\{y_i=k\\} -\\frac{x_{ij}\\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}},\r\\end{align}\\]\r\\[\\begin{align}\r\\frac{\\partial h_{i}(z;\\lambda)}{\\psi_{jk}}\u0026amp;=x_{ij}z_{jk}\\exp(\\psi_{jk})1\\{y_i=k\\}-\\frac{x_{ij}z_{jk}\\exp(\\psi_{jk})\\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}}{\\sum_{k=0}^K \\exp\\{x_i^\\top (\\mu_{\\cdot k}+\\text{diag}(\\exp(\\psi_{\\cdot k}))z_k)\\}}.\r\\end{align}\\]\rAs a result,\n\rFigure 1: The species are Iris setosa, versicolor, and virginica.\r\r\r\r\rSepal.Length\rSepal.Width\rPetal.Length\rPetal.Width\rSpecies\r\r\r\r1\r5.1\r3.5\r1.4\r0.2\rsetosa\r\r2\r4.9\r3.0\r1.4\r0.2\rsetosa\r\r3\r4.7\r3.2\r1.3\r0.2\rsetosa\r\r51\r7.0\r3.2\r4.7\r1.4\rversicolor\r\r52\r6.4\r3.2\r4.5\r1.5\rversicolor\r\r53\r6.9\r3.1\r4.9\r1.5\rversicolor\r\r101\r6.3\r3.3\r6.0\r2.5\rvirginica\r\r102\r5.8\r2.7\r5.1\r1.9\rvirginica\r\r103\r7.1\r3.0\r5.9\r2.1\rvirginica\r\r\r\rWe next show the results for the iris data with the score function gradient based Adam algorithm.\n\r## data generation\rset.seed(0)\rdata(iris)\rn \u0026lt;- nrow(iris)\rp \u0026lt;- ncol(iris)\rK \u0026lt;- nlevels(iris$Species)\rX \u0026lt;- model.matrix(Species ~ ., data=iris) # design matrix\rY \u0026lt;- model.matrix(~ Species - 1, data=iris)\rsigma0 = 10\relbo_hat\u0026lt;- function(mu,psi,N){\rL1 = sum(psi-(mu^2+exp(2*psi))/(2*sigma0^2))\rbeta = matrix(0,p,K) # the first column is zero est = matrix(0,N,1)\rfor (i in 1:N){\rbeta[,-1] = mu + exp(psi) * matrix(rnorm(p*(K-1)),p,K-1)\rtmp = exp(X%*%beta)\rden = rowSums(tmp)\rnum = rowSums(tmp*Y)\rest[i] = sum(log(num/den))\r}\rreturn(mean(est)+L1)\r}\rscore_fun_gradient \u0026lt;- function(mu,psi,N){\rdmu = -mu/sigma0^2\rdpsi = 1-exp(2*psi)/sigma0^2\rbeta = matrix(0,p,K) # the first column is zero for (i in 1:N){\rbeta[,-1] = mu + exp(psi) * matrix(rnorm(p*(K-1)),p,K-1)\rtmp = exp(X%*%beta)\rden = rowSums(tmp)\rnum = rowSums(tmp*Y)\rsumlog = sum(log(num/den))\rdmu = dmu + sumlog*(beta[,-1]-mu)/exp(2*psi)/N\rdpsi = dpsi + sumlog*((beta[,-1]-mu)^2/exp(2*psi)-1)/N\r}\rreturn(list(dmu=-dmu,dpsi=-dpsi)) # return negative gradient for adapting the Adam\r}\r#Adam: See the paper \u0026#39;ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\u0026#39;\ralpha = 0.1\rbeta1 = 0.9\rbeta2 = 0.999\reps = 1e-8\r#initial parameters\rmut = matrix(0,p,K-1)\rpsit = matrix(0,p,K-1)\rmu_mt = matrix(0,p,K-1) #first moment\rpsi_mt = matrix(0,p,K-1)\rmu_vt = matrix(0,p,K-1) #second moment\rpsi_vt = matrix(0,p,K-1)\rT = 500\rNg = 100 #sample size for gradient\rNelbo = 10000 #sample size for estimating elbo\relbo = matrix(0,T,1)\rfor(t in 1:T){\rgt = score_fun_gradient(mut,psit,Ng)\rmu_mt = beta1*mu_mt + (1-beta1)*gt$dmu\rmu_vt = beta2*mu_vt + (1-beta2)*gt$dmu^2\rhat_mu_mt = mu_mt/(1-beta1^t)\rhat_mu_vt = mu_vt/(1-beta2^t)\rmut = mut - alpha*hat_mu_mt/(sqrt(hat_mu_vt)+eps)\rpsi_mt = beta1*psi_mt + (1-beta1)*gt$dpsi\rpsi_vt = beta2*psi_vt + (1-beta2)*gt$dpsi^2\rhat_psi_mt = psi_mt/(1-beta1^t)\rhat_psi_vt = psi_vt/(1-beta2^t)\rpsit = psit - alpha*hat_psi_mt/(sqrt(hat_psi_vt)+eps) #update the state\relbo[t] = elbo_hat(mut,psit,Nelbo)\r}\rplot(elbo,type=\u0026#39;b\u0026#39;,xlab = \u0026#39;Iteration\u0026#39;,ylab=\u0026quot;ELBO\u0026quot;,pch=16)\r\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7d333683bccc71a3910f7eeb09d00491","permalink":"/en/courses/bayes/vi/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/vi/","section":"courses","summary":"This note is adapted to the paper entitled â€œVariation Inference: A Review for Statisticansâ€ by Blei et al.Â (2017).\nBasic ideas of VI\rLet \\(z=z_{1:m}\\) be the latent variables that govern the distribution of the data (observations) \\(x=x_{1:n}\\).","tags":null,"title":"å˜åˆ†æ¨æ–­","type":"docs"},{"authors":null,"categories":null,"content":"\rIntroduction to Bayesian computation\rThe goals are to estimate\n\rthe posterior distribution \\(p(\\theta|y)\\propto p(\\theta)p(y|\\theta)\\)\n\rthe posterior predictive distribution\r\\[p(\\tilde y|y) = \\int p(\\tilde y|\\theta)p(\\theta|y)d \\theta =E[p(\\tilde y|\\theta)|y]\\]\n\r\rWe are therefore insterested in estimating the posterior expectation\r\\[\\mu=E[h(\\theta)|y]=\\int h(\\theta)p(\\theta|y)d \\theta\\]\n\rmoments: \\(h(\\theta)=\\theta^k\\)\rprobability: \\(h(\\theta)=1_A(\\theta)\\), \\(A\\subseteq \\Theta\\)\rpredictive density: \\(h(\\theta)=p(\\tilde y|\\theta)\\) for fixed \\(\\tilde y\\)\r\r\rMonte Carlo methods\rSuppose we can simulate \\(\\theta^{(1)},\\dots,\\theta^{(N)}\\sim p(\\theta|y)\\) independently. Monte Carlo (MC) esimate is then the sample average:\r\\[\\hat{\\mu}_N = \\frac1N\\sum_{i=1}^N h(\\theta^{(i)})\\]\n\rLLN: \\(\\hat\\mu_N\\to \\mu\\) w.p.1 as \\(N\\to \\infty\\)\rCLT: \\(\\hat\\mu_N-\\mu=O_p(N^{-1/2})\\)\r\rTraditional quadrature rulesâ€™ have error rate \\(O(N^{-r/d})\\), where \\(r\\ge 1\\) depends on the smoothness of the functions, and \\(d\\) is the dimension of \\(\\theta\\). This suffers the curse of dimensionality.\nMC has an error rate \\(O(N^{-1/2})\\) independently of the smoothness and the dimension of the functions. The task is to simulate iid samples \\(\\theta^{(i)}\\sim p(\\theta|y)\\).\n\rRandom number generators\rWe start with a pseudo-random number generator:\r\\[u_1,\\dots,u_n,\\dots\\stackrel{iid}\\sim U(0,1)\\]\n\rMersenne Twister by Matsumoto \u0026amp; Nishimura (1998), whose period is \\(2^{19937}-1\u0026gt;10^{6000}\\)\rRngStreams by Lâ€™Ecuyer, Simard, Chen, Kelton (2002)\r\rThey arenâ€™t really uniform random, but good ones are close enough.\n\rNon-uniform random variables\rSome common distributions (such as Normal, exponential, binomial, Poisson etc.) are already in some scientific softwares (R, Python, Matlab, Julia, Mathematica, etc.)\nWe are now concerned with a general distribution. Principled approaches are\n\rinversion\racceptance-rejection\r\r\rInversion\rLet \\(F(x)\\) be the CDF of the distribution of interest. We can simulate the distribution via iid uniforms \\(U_1,\\dots,U_N\\stackrel{iid}\\sim U(0,1)\\):\n\\[X_i=F^{-1}(U_i),i=1,\\dots,N\\]\n\r\\(F^{-1}\\) is the inverse of the CDF \\(F\\), definited by\r\\[F^{-1}(u)=\\inf\\{x\\in\\mathbb{R}|F(x)\\ge u\\}\\]\n\rit is easy to see that \\(X_i\\stackrel{iid}\\sim F\\)\n\r\r\rInversion: examples\rGaussian\r\\[Z=\\Phi^{-1}(U)\\sim N(0,1)\\]\n\\[X=\\mu+\\sigma\\Phi^{-1}(U) \\sim N(\\mu,\\sigma^2)\\]\n\rExponential\r\\[X= -\\frac 1\\lambda \\log (1-U)\\sim Exp(\\lambda)\\]\n\rBernoulli\r\\[X=1\\{U\\le p\\}\\sim Bin(1,p)\\]\n\r\rMultivariate inverse transformation\r\rlet \\(F(x_1,\\dots,x_d)\\) be the PDF of \\(X_1,\\dots,X_d\\)\rlet \\(F_i(x_i)\\) be the marginal distribution of \\(X_i\\)\rfor \\(i=2,\\dots,d\\), let \\(F_i(x_i|x_1,\\dots,x_{i-1})\\) be the conditional CDF\r\rThe multivariate inverse transformation is proposed by Rosenblatt (1952), which simulates the components \\(X_i\\) recursively, i.e.,\n\\[X_1=F_1^{-1}(U_1)\\]\r\\[X_i=F_i^{-1}(U_i|X_1,\\dots,X_{i-1}),\\ i=2,\\dots,d\\]\n\rthe output has the destribution \\(F\\)\rthe order of simulating the components can be arbitrary\rthe critical issue is to know the conditional CDFs in advance\r\r\rAcceptance-rejection\r\rsuppose the target distrubtion is \\(f(x)\\) with the support \\(\\mathcal{X}\\)\rwe can sample \\(Y\\sim g\\), where \\(g\\) is another density satisfying: there exists \\(M\u0026gt;0\\) such that\r\\[\\frac{f(x)}{g(x)}\\le M\\ \\forall x\\in \\mathcal{X}\\]\rwe can compute \\(f(x)/g(x)\\)\r\rThe algorithm goes below\n\rStep 1: simulate \\(Y\\sim g\\)\n\rStep 2: accept \\(Y\\) as a draw from \\(f\\) with probability \\(f(Y)/(Mg(Y))\\). If the draw is rejected, return to Step 1.\n\rStep 2â€™: simulate \\(U\\sim U(0,1)\\)\r\\[\r\\begin{cases}\r\\text{accept } Y \u0026amp; U\\le f(Y)/(Mg(Y))\\\\\r\\text{go to Step 1 }\u0026amp; else\r\\end{cases}\r\\]\n\r\r\rAcceptance-rejection\r\rthe acceptance probability: \\[E[f(Y)/(Mg(Y))]=\\frac 1 M\\]\rwe may choose the smallest \\(M\\) such that \\(f(x)\\le Mg(x)\\) for all \\(x\\in\\mathcal{X}\\)\r\r\rAcceptance-rejection for Bayesian computation\r\rthe target density\r\\[p(\\theta|y)=\\frac{p(\\theta)p(y|\\theta)}{p(y)}\\]\n\rthe constant \\(p(y)\\) is unknown\n\rthe AR algorithm works well if taking \\(f(\\theta)=p(\\theta)p(y|\\theta)\\)\rand using proposal density \\(\\propto g(\\theta)\\) with\r\\[\\frac{p(\\theta)p(y|\\theta)}{g(\\theta)}\\le M\\]\n\r\r\rExample: Gamma distribution\r\r\\(Gamma(\\alpha,\\lambda)\\), \\(\\alpha\u0026gt;0\\) is the shape, \\(\\lambda\u0026gt;0\\) is the rate\n\rdensity\n\r\r\\[f(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x}1\\{x\u0026gt; 0\\}\\]\n\\[Gamma(\\alpha,\\lambda)\\stackrel{d}{=}\\frac 1 \\lambda Gamma(\\alpha,1)\\]\n\\[Gamma(\\alpha,1)\\stackrel{d}{=}U(0,1)^{1/\\alpha}Gamma(\\alpha+1,1)\\]\n\rso our target is \\(Gamma(\\alpha,1)\\) with \\(\\alpha\u0026gt;1\\). For this case, the density is bounded.\n\rthe proposal\r\\[g(x)=?\\]\n\r\r\rGamma density\r\rAhrens and Dieter (1974) took proposals from a density that combines a\rGaussian density in the center and an exponential density in the right tail.\n\rMarsaglia and Tsang (2000) present an AR algorithm from a truncated\r\\(N(0,1)\\)\n\r\r\rExample: Beta distribution\r\r\\(Beta(\\alpha,\\beta)\\) density\r\\[f(x)=\\frac{\\Gamma(\\alpha,\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1\\{0\u0026lt;x\u0026lt;1\\}\\]\n\rgenerate a Beta from two independent Gammas\r\\[Beta(\\alpha,\\beta)\\stackrel{d}{=}\\frac{Gamma(\\alpha,\\lambda)}{Gamma(\\alpha,\\lambda)+Gamma(\\beta,\\lambda)}\\]\n\rfor \\(\\alpha\u0026gt;1\\) and \\(\\beta\u0026gt;1\\), the beta density is unimodal and achieves its maximum at \\(x^*=(\\alpha-1)/(\\alpha+\\beta-2)\\)\n\r\r\rBeta density\r\rProposal distribution: \\(U(0,1)\\)\r\\(M=f(x^*)\\)\raccept \\(U\\sim U(0,1)\\) with probability \\(f(U)/M\\)\r\r\rBeta generator: R code\rmyBeta \u0026lt;- function(n,alpha,beta){\rif(alpha\u0026lt;=1 | beta\u0026lt;=1)\rstop(\u0026quot;alpha, beta cannot be \u0026lt;= 1\u0026quot;)\rM = dbeta((alpha-1)/(alpha+beta-2),alpha,beta)\rx = rep(0,n)\rfor(i in 1:n){\rwhile (TRUE){\rU = runif(1)\rif(dbeta(U,alpha,beta)\u0026gt;= M*runif(1)){\rx[i] = U\rbreak\r}\r}\r}\rreturn(x)\r}\r\rSimulation results\r\rTable 1: \r\r\rmean\rsd\r\r\r\rmyBeta\r0.4001780\r0.1968478\r\rdbeta\r0.3996059\r0.2000705\r\rtrue values\r0.4000000\r0.2000000\r\r\r\r\rImportance Sampling\r\rthe target is to estimate \\(\\mu=E_f[h(X)]\\) w.r.t. the density \\(f(x)\\)\n\rthe proposal density \\(g(x)\\): \\(g(x)\u0026gt;0\\) whenever \\(h(x)f(x)\u0026gt;0\\)\r\\[\\mu=\\int h(x)f(x)dx=\\int h(x)\\frac{f(x)}{g(x)}g(x)dx=E_g[h(X)f(X)/g(X)]\\]\n\r\\(f(x)/g(x)\\) called the likelihood ratio (LR)\n\r\rThe IS algorithm goes below\n\rStep 1: simulate \\(n\\) samples \\(X_1,\\dots,X_n\\) from \\(g(x)\\)\rStep 2: compute the sample average:\r\\[\\hat{\\mu}_{IS}=\\frac 1N\\sum_{i=1}^N \\frac{h(X_i)f(X_i)}{g(X_i)}\\]\r\r\rChoosing the proposal\r\\[Var[\\hat{\\mu}_{IS}] = \\frac{\\sigma^2_g}{N}\\]\n\\[\\sigma^2_g = \\int \\left(\\frac{h(x)f(x)}{g(x)}-\\mu\\right)^2g(x)d x=\\int\\frac{(h(x)f(x)-\\mu g(x))^2}{g(x)}dx\\]\n\rif \\(g(x)=h(x)f(x)/\\mu\\) and \\(h\\ge 0\\), then we have the optimal case \\(\\sigma^2_g=0\\)\rbut unattainable: \\(\\mu\\) is unknown constant\rwe may find \\(g(x)\\approx h(x)f(x)/\\mu\\)\r\r\rThe weight function\r\rlet \\(w(x)=f(x)/g(x)\\) be the LR\r\\[\\sigma^2_g =\\int \\frac{(hf)^2}{g}dx -\\mu^2\\]\r\r\\[\\int \\frac{(hf)^2}{g}dx=E_f[w(X)h(X)^2]=E_g[w(X)^2h(X)^2]\\]\n\rif \\(w(x)\\) is bounded, then \\(\\sigma^2_g\\) is bounded\rif \\(w(x)\\) is unbounded, then \\(\\sigma^2_g\\) may be unbounded (the worst case!)\r\r\rSelf-normalized IS (SNIS)\rWhat if we cannot compute \\(f/g\\)? Suppose that\r\\[f(x)=c_f\\tilde{f}(x),\\ g(x)=c_g\\tilde{g}(x)\\]\nand we can compute \\(\\tilde f,\\tilde g\\) but not the constants \\(c_f,c_g\\). Then we use\n\\[\\hat{\\mu}_{SNIS}= \\frac{\\frac 1 N\\sum_{i=1}^nh(X_i)\\tilde{f}(X_i)/\\tilde{g}(X_i)}{\\frac 1 N\\sum_{i=1}^n\\tilde{f}(X_i)/\\tilde{g}(X_i)}\\]\nor, equivalently,\n\\[\\hat{\\mu}_{SNIS}= \\frac{\\frac 1 N\\sum_{i=1}^Nh(X_i){f}(X_i)/{g}(X_i)}{\\frac 1 N\\sum_{i=1}^N{f}(X_i)/{g}(X_i)}=\\frac{\\frac 1 N\\sum_{i=1}^Nh(X_i)w(X_i)}{\\frac 1 N\\sum_{i=1}^Nw(X_i)}\\]\n\rVariance of SNIS\r\rTaylor expansions\r\\[f(\\bar X,\\bar Y)\\approx f(\\mu_1,\\mu_2)+f_x(\\mu_1,\\mu_2)(\\bar X-\\mu_1)+f_y(\\mu_1,\\mu_2)(\\bar Y-\\mu_2)\\]\r\r\\[E[f(\\bar X,\\bar Y)]\\approx f(\\mu_1,\\mu_2)\\]\n\\[Var[f(\\bar X,\\bar Y)]\\approx f_x^2Var[\\bar X]+f_y^2Var[\\bar Y]+2f_xf_yCov(\\bar X,\\bar Y)\\]\n\rfor \\(f(x,y)=x/y\\), \\(f_x=1/y,f_y=-x/y^2\\)\r\\[Var[f(\\bar X,\\bar Y)]\\approx \\frac{\\sigma_X^2}{N\\mu_2^2}+\\frac{\\mu_1^2\\sigma_Y^2}{N\\mu_2^4}-\\frac{2\\mu_1}{N\\mu_2^3}Cov(X,Y)\\]\n\r\\(Var[\\hat{\\mu}_{SNIS}]\\approx \\frac{1}{N}E_g[w(X)^2(h(X)-\\mu)^2]\\)\n\r\\(Var[\\hat{\\mu}_{IS}]= \\frac{1}{N}E_g[(h(X)w(X)-\\mu)^2]\\)\n\r\r\rOptimal SNIS\r\rSNIS: \\(g_{opt}(x)\\propto f(x)|h(x)-\\mu|\\)\n\rIS: \\(g_{opt}(x)\\propto f(x)|h(x)|\\)\n\r\r\rEffective sample size\r\rUnequal weighting raises variance, see, Kong (1992), Evans and Swartz (1995)\n\rfor iid \\(Y_i\\) with variance \\(\\sigma^2\\) and fixed \\(w_i\\ge 0\\),\r\\[Var\\left(\\frac{\\sum_{i}w_iY_i}{\\sum_iw_i}\\right)=\\frac{\\sum_iw_i^2\\sigma^2}{(\\sum_iw_i)^2}=\\frac{\\sigma^2}{N_{eff}}\\]\n\r\rwhere the effective sample size \\(N_{eff}\\) is defined as\n\\[N_{eff} = \\frac{(\\sum_{i=1}^Nw_i)^2}{\\sum_{i=1}^Nw_i^2}\\in [1,N]\\]\n\r\\(N_{eff}\\) is small if there are few extremely high weights which would unduly influence the distribution\n\rfor equal weights, we have \\(N_{eff}=N\\)\n\r\r\rExample 1\rSuppose the posterior distribution is \\(N(\\mu,\\sigma^2)\\), the proposal distribution is \\(t_3(\\mu,\\sigma^2)\\). Consider \\(\\mu=\\sigma=2\\).\n\rExample 1\r## [1] \u0026quot;Effective sample size is 9178 / 10000\u0026quot;\r\rTable 2: \r\r\rn=100\rn=1000\rn=10000\rexact_value\r\r\r\rMean\r2.214328\r2.011347\r1.945335\r2\r\rVariance\r3.069694\r3.688183\r4.052519\r4\r\r\r\r\rExample 2\rSuppose the posterior distribution is \\(t_3(\\mu,\\sigma^2)\\), the proposal distribution is \\(N(\\mu,\\sigma^2)\\).\n## [1] \u0026quot;Effective sample size is 6180 / 10000\u0026quot;\r\rTable 3: \r\r\rn=100\rn=1000\rn=10000\rexact_value\r\r\r\rMean\r1.802681\r1.784954\r2.019707\r2\r\rVariance\r4.630305\r6.931088\r6.875331\r12\r\r\r\r\rIS vs acceptance rejection\r\rAcceptance-rejection requires bounded LR \\(f/g\\)\n\rWe also have to know a bound\n\rIS and SNIS require us to keep track of weights\n\rPlain IS requires normalized \\(f/g\\)\n\rAcceptance-rejection samples cost more (due to rejections)\n\r\r\rIS for rare events\r\rrare events:\r\\[h(x)=1_A(x), \\mu = E_f[h(x)]=\\int_A f(x) dx=\\epsilon\\approx 0\\]\n\rcoefficient of variation of \\(\\hat{\\mu}\\)\r\\[cv:=\\frac{\\sigma/\\sqrt{N}}{\\mu}=\\frac{\\sqrt{\\epsilon(1-\\epsilon)}}{\\sqrt{n}\\epsilon}\\approx \\frac{1}{\\sqrt{n\\epsilon}}\\]\n\rto get \\(cv=0.1\\) takes \\(N\\ge 100/\\epsilon\\), e.g., \\(\\epsilon = 10^{-5}\\), then \\(N\\ge 10^7\\)\n\rTaking \\(X\\sim f\\) does not get enough data from the important region \\(A\\).\n\rGet more data from \\(A\\) (from a proper proposal \\(g(x)\\)), and then correct the bias (the LR function)\n\r\r\rChanging a parameter\r\rnorminal distribution \\(p(x;\\theta_0)\\), \\(\\theta_0\\in\\Theta\\)\n\rproposal distribution \\(p(x;\\theta)\\), \\(\\theta\\in\\Theta\\)\n\restimator\r\\[\\hat\\mu_\\theta=\\frac{1}{N}\\sum_{i=1}^N h(X_i) \\frac{p(X_i;\\theta_0)}{p(X_i;\\theta)}\\]\n\r\rThe importance ratio often simplifies, e.g., in exponential families.\n\rExponential tilting\rMany important distributions can be written in the form\r\\[p(x;\\theta) = a(\\theta)\\exp[\\eta(\\theta)^\\top T(x)]b(x), \\theta\\in \\Theta\\]\n\\[\\hat\\mu_\\theta=\\frac{a(\\theta_0)}{a(\\theta)}\\frac{1}{N}\\sum_{i=1}^N h(X_i) \\exp[(\\eta(\\theta_0)-\\eta(\\theta))^\\top T(X_i)]\\]\n\r\\(\\eta(\\theta)\\) is the natrual parameter\n\rThis is called the â€˜exponential twistingâ€™.\n\rThe goal is to choose \\(\\theta\\in\\Theta\\) such that \\(Var[\\hat\\mu_\\theta]\\) is minimized.\n\r\r\rA simple example\r\rnorminal distribution \\(p(x;\\theta_0)=N(x;0,1)\\)\n\rproposal distribution \\(p(x;\\theta)=N(x;\\theta,1)\\), \\(\\theta\\in\\mathbb{R}\\)\n\rtarget function \\(h(x) = 1\\{x\u0026gt;c\\}\\), for large \\(c\u0026gt;0\\), \\(\\mu=E[h(X)]=1-\\Phi(c)\\approx 0\\)\n\rIS estimator\r\\[\\hat\\mu_\\theta=\\frac{1}{N}\\sum_{i=1}^N h(X_i) \\frac{N(X_i;0,1)}{N(X_i;\\theta,1)}=\\frac{1}{N}\\sum_{i=1}^N h(X_i) e^{-\\frac{2\\theta X_i-\\theta^2}{2}}\\]\n\rIS variance \\(Var[\\hat\\mu_\\theta]=\\sigma^2_\\theta/N\\)\r\\[\\sigma^2_\\theta=\\frac{e^{\\theta^2}}{\\sqrt{2\\pi}}\\int_c^\\infty e^{-\\frac{(x+\\theta)^2}{2}}dx-\\mu^2=e^{\\theta^2}[1-\\Phi(c+\\theta)]-\\mu^2\\]\n\rthe optimal parameter \\(\\theta^*=\\arg \\min_{\\theta\\in \\mathbb{R}} e^{\\theta^2}[1-\\Phi(c+\\theta)]\\)\n\r\r\rThe effect of different parameters\r## [1] \u0026quot;the threshold c = 3\u0026quot;\r## [1] \u0026quot;the true value is 0.00135\u0026quot;\r## [1] \u0026quot;the optimal theta is 3.155\u0026quot;\r## [1] \u0026quot;variance reduction factor is 222\u0026quot;\r\rApplications in Computational Finance\r\rP. Glasserman, P. Heidelberger, and P. Shahabuddin. Asymptotically optimal importance\rsampling and stratification for pricing path-dependent options. Mathematical Finance, 9\r(2):117â€“152, 1999.\n\rP. Glasserman, P. Heidelberger, and P. Shahabuddin. Variance reduction techniques for\restimating value-at-risk. Management Science, 46(10):1349â€“1364, 2000.\n\rP. Glasserman, J. Li. Importance Sampling for Portfolio Credit Risk. Management Science, 51(11):1643â€“1656, 2005.\n\rXie, Fei, Zhijian He, and Xiaoqun Wang. An Importance Sampling-Based Smoothing Approach for Quasi-Monte Carlo Simulation of Discrete Barrier Options. European Journal of Operational Research, October 17, 2018.\rhttps://doi.org/10.1016/j.ejor.2018.10.030\n\r\r\rImportance Sampling for Portfolio Credit Risk\rOur interest centers on the distribution of losses\rfrom default over a fixed horizon.\n\r\\(m\\): number of obligors\n\r\\(Y_k\\): default indicator for \\(k\\)th obligor, \\(Y_k=1\\) denotes the default; \\(Y_k=0\\) otherwise\n\r\\(p_k\\): marginal probability that \\(k\\)th obligor defaults\n\r\\(c_k\\): loss resulting from default of \\(k\\)th obligor\n\r\\(L=c_1Y_1+\\dots+c_mY_m\\): total loss from defaults\n\r\rOur goal is to estimate tail probabilities \\(P(L\u0026gt;x)\\), especially at large values of \\(x\\)\n\rNormal copula model\rIn the normal copula model, dependence\ris introduced through a multivariate normal vector \\(X_1,\\dots,X_m\\) of latent variables. Each default indicator is represented as\r\\[Y_k = 1\\{X_k\u0026gt; x_k\\},\\ k=1,\\dots,m.\\]\r\\[X_k = a_{k1}Z_1+\\dots+a_{kd}Z_d+b_k\\epsilon_k\\]\n\r\\(x_k\\) are chosen to match \\(P(X_k\u0026gt;x_k)=p_k\\)\n\r\\(Z_1,\\dots,Z_d\\stackrel{iid}{\\sim} N(0,1)\\) are systematic risk factors\n\r\\(\\epsilon_k\\stackrel{iid}{\\sim} N(0,1)\\) is an idiosyncratic risk\n\r\\(a_{k1},\\dots,a_{kd}\\) are the loading factors satisfying \\(\\sum_{j=1}^d a_{kj}^2\\le 1\\)\n\r\\(b_k=\\sqrt{1-\\sum_{j=1}^d a_{kj}^2}\\) so that \\(X_k\\sim N(0,1)\\)\n\r\r\rIS for independent obligors\rConsider the simple case of independent obligors: \\(a_{ij}=0,\\ b_k=1\\), i.e., \\(Y_k\\sim Bin(1,p_k)\\) independently. The idea is to replace each default probability \\(p_k\\) by some other default probability \\(q_k\\), the basic IS identity is\r\\[P(L\u0026gt;x)= \\tilde{E}\\left[1\\{L\u0026gt;x\\}\\prod_{k=1}^m\\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}\\right]\\]\nExponential Twisting: Glasserman and Li (2005) chooses\r\\[q_{k,\\theta} = \\frac{p_ke^{\\theta c_k}}{1+p_k(e^{\\theta c_k}-1)}\\]\n\rThe original probabilities correspond to \\(\\theta=0\\)\n\rif \\(\\theta\u0026gt;0\\), this does indeed increase the default\rprobabilities; a larger exposure \\(c_k\\) results in a greater\rincrease in the default probability.\n\r\r\rChoosing the optimal parameter\rThe LR is reduced to\r\\[\\prod_{k=1}^m\\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}=\\exp(-\\theta L+\\psi(\\theta))\\]\nwhere\r\\[\\psi(\\theta)=\\log E[e^{\\theta L}]=\\sum_{k=1}^m \\log(1+p_k(e^{\\theta c_k}-1))\\]\ris the cumulant generating function (CGF) of L.\nThe optimal parameter is\r\\[\\theta^* = \\arg \\min_{\\theta\\ge 0} \\{M_2(\\theta)=E_\\theta[1\\{L\u0026gt;x\\}e^{-2\\theta L+2\\psi(\\theta)}]\\}\\]\n\rChoosing the sub-optimal parameter\rObserve that for \\(\\theta\\ge 0\\),\r\\[M_2(\\theta)\\le e^{-2\\theta x+2\\psi(\\theta)}\\]\nMinimizing \\(M_2(\\theta)\\) is difficult, but minimizing\rthe upper bound is easy:\r\\[\\theta_x = \\arg \\min_{\\theta\\ge 0}e^{-2\\theta x+2\\psi(\\theta)}=\\arg \\max_{\\theta\\ge 0} \\{\\theta x-\\psi(\\theta)\\}\\]\nThe function \\(\\psi(\\theta)\\) is strictly convex and passes through the origin, so the maximum\ris attained at\r\\[\\theta_x = \\begin{cases}\r\\text{unique solution to }\\psi\u0026#39;(\\theta)=x,\\ \u0026amp;x\u0026gt;\\psi\u0026#39;(0)\\\\\r0,\\ \u0026amp;x\\le \\psi\u0026#39;(0).\r\\end{cases}\r\\]\n\rfor the first case, \\(E_{\\theta_x}[L]=\\psi\u0026#39;(\\theta_x)=x\\), thus, we have shifted the distribution of L so that x is now its mean.\n\rfor the second case, the event \\(\\{L\u0026gt;x\\}\\) is not rare, so we do not change the probabilities.\n\r\r\rDependent Obligors: Conditional Importance Sampling\rFor general factor models, \\(Y_k\\) are dependent; but they are independent conditinal on the systematic risk factors \\(Z_1,\\dots,Z_d\\). So we can apply the so-called conditional IS.\n\rStep 1: simulate \\(Z_1,\\dots,Z_d\\sim N(0,1)\\) and compute the default probability\r\\(p_k=p_k(Z_1,\\dots,Z_d)\\)\n\rStep 2: for simulated \\(p_k\\), obtain the twisting parameter \\(\\theta_x=\\theta_x(Z_1,\\dots,Z_d)\\)\n\rStep 3: compute the LR for the \\(\\theta_x\\)\n\rStep 4: repeat Steps 1â€“4 \\(N\\) times and then obtain the final IS estimate\n\r\r\rNumerical results\rThe numerical results were reported in Glasserman and Li (2005).\n\r\\(21\\)-factor model with \\(m=1000\\) obligors\r\\(p_k = 0.01(1+\\sin(16\\pi k/m))\\)\r\\(c_k=(\\lceil5k/m\\rceil)^2\\)\rVRF = â€œVariance Reduction Factorâ€\r\r\r\r\\(x\\)\r\\(P(L\u0026gt;x)\\)\rVRF\r\r\r\r10,000\r0.0114\r33\r\r14,000\r0.0065\r53\r\r18,000\r0.0037\r83\r\r22,000\r0.0021\r125\r\r30,000\r0.0006\r278\r\r40,000\r0.0001\r977\r\r\r\r\rExtensions\rThe defual indicators\n\\[Y_k=1\\{X_k\u0026gt;x_k\\}\\]\n\r\\(X_k\\) follow t copula model\r\rJoshua C.C. Chan, Dirk P. Kroese. Efficient estimation of large portfolio loss probabilities in t-copula models. European Journal of Operational Research, 205:361â€“367, 2010.\n\r\\(X_k\\) follow another advanced models, e.g., self-exciting model, Giesecke et al.Â (2010)\r\rRandom default exposures: \\(c_k=e_k\\ell_k\\), where \\(\\ell_k\\in[0,1]\\) denotes a random\rpercentage loss, and \\(e_k\u0026gt;0\\) are constants.\r\\[L = \\sum_{k=1}^m e_k\\ell_k1\\{X_k\u0026gt;x_k\\}\\]\n\r\\(\\ell_k\\) are iid truncated normals or betas\r\r\rCross-entropy\rThe optimal proposal\rdensity is obtained by locating the member \\(p(x;\\theta),\\theta\\in\\Theta\\) that minimizes\rits cross-entropy distance to the zero-variance proposal\rdensity \\(q^*(x)\\propto h(x)p(x;\\theta_0)\\).\nThe minimization of the cross-entropy is equivalent to solving\rthe following maximization problem\r\\[\\max_{\\theta\\in\\Theta} \\int h(x)p(x;\\theta_0)\\log p(x;\\theta)d x=\\max_{\\theta\\in\\Theta} E_{\\theta_0}[h(X)\\log p(X;\\theta)]\\]\nSince most often an analytical solution to the above maximization\rproblem is not available, we consider instead its stochastic\rcounterpart\r\\[\\theta^*=\\arg \\max_{\\theta\\in\\Theta}\\frac 1{N_0}\\sum_{i=1}^{N_0}h(X_i)\\log p(X_i;\\theta),\\ X_i\\stackrel{iid}{\\sim} p(x;\\theta_0)\\]\nMore detials see Rubinstein (1997), Rubinstein \u0026amp; Kroese (2004).\n\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5883ea17d6b0f16b6854de6822b32c40","permalink":"/en/courses/bayes/chap10/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap10/","section":"courses","summary":"Introduction to Bayesian computation\rThe goals are to estimate\n\rthe posterior distribution \\(p(\\theta|y)\\propto p(\\theta)p(y|\\theta)\\)\n\rthe posterior predictive distribution\r\\[p(\\tilde y|y) = \\int p(\\tilde y|\\theta)p(\\theta|y)d \\theta =E[p(\\tilde y|\\theta)|y]\\]","tags":null,"title":"ç¬¬10ç« ","type":"docs"},{"authors":null,"categories":null,"content":"\rMarkov chains\rConsider the discrete chain:\r\\[P(X_i\\in A|X_0=x_0,\\dots,X_{i-1}=x_{i-1})=P(X_i\\in A|X_{i-1}=x_{i-1})\\]\n\r\\(X_i\\in\\Omega=\\{\\omega_1,\\dots,\\omega_M\\}\\)\n\rTransition distribution:\r\\[P(X_i=y|X_{i-1}=x)=T_i(y|x)\\]\n\rDistribution of this chain is now determined by \\(p_0(x)=P(X_0=x)\\) and \\(T_i(y|x)\\)\n\rHomogeneous chain:\r\\[P(X_i=y|X_{i-1}=x)=P(X_1=y|X_0=x)=T(y|x)\\]\n\r\r\rWhere does this chain go?\r\\[p_{i}(\\omega_k) = P(X_i=\\omega_k) = \\sum_{j=1}^Mp_{i-1}(\\omega_j)T(\\omega_k|\\omega_j)\\]\n\r\\(\\boldsymbol{p}_i=(p_i(\\omega_1),\\dots,p_i(\\omega_M))\\)\n\r\\(P\\) is the transition matrix with entries \\(P_{ij}=P(\\omega_j|\\omega_i)\\)\n\r\\(\\boldsymbol{p}_i = \\boldsymbol{p}_{i-1} P\\)\n\r\\(\\boldsymbol{p}_i = \\boldsymbol{p}_{0} P^n\\)\n\r\r\rExample: A portion of the MontrÃ©al mÃ©tro\r\rTransition matrix\r\rAfter 100 steps\r\rNo matter where you start \\(p_{100}(\\text{Berri})\\doteq 0.31\\)\n\rThese are almost IID from the stationary distribution.\n\r\r\rStationary distribution\r\\[\\pi = \\pi P \\text{ so } \\pi^\\top = P^\\top\\pi^\\top\\]\n\r\\(\\pi^{\\top}\\) is an eigenvector of \\(P^\\top\\) with eigenvalue 1\r\r\rLaw of large numbers\rLet \\(X_i\\) be a time-homogenous Markov chain on a finite set \\(\\Omega\\)\nTheorem: If \\(P\\) is irreducible, then\n\\[P_{\\omega_0}\\left(\\lim_{n\\to\\infty}\\frac 1n\\sum_{i=1}f(X_i)=\\sum_{\\omega\\in\\Omega}\\pi(\\omega)f(\\omega)\\right)=1\\]\n\rIrreducibility: \\(P_x(\\tau_y\u0026lt;\\infty)\u0026gt;0\\) for any \\(x,y\\in \\Omega\\), where \\(\\tau_y\\) is the first time \\(y\\) is visited, i.e., \\[\\tau_y:=\\inf\\{i\\ge 1:X_i=y,X_0=x\\}\\]\r\r\rWhat we will do\r\rGiven \\(\\pi\\) we will find a transition matrix \\(P\\) with \\(\\pi P=\\pi\\)\n\rThen sample \\(X_1,\\dots,X_n\\) via \\(P\\)\n\r\rHere is what could go wrong:\n\rIt might take a long time before \\(\\boldsymbol{p}_n\\approx \\pi\\) (slow convergence)\n\r\\(X_n\\) might get stuck for a long time (slow mixing)\n\r\rThe goal is to find the good one for \\(P\\).\n\rDetailed balance\r\rStationarity balances flow into \\(y\\) with flow out of \\(y\\)\r\\[\\sum_{x\\in\\Omega} \\pi(x)P(y|x)=\\pi(y) = \\sum_{x\\in\\Omega}\\pi(y)P(x|y)\\]\rNB: \\(\\pi = \\pi P\\)\n\rDetailed balance is stronger:\r\\[\\pi(x)P(y|x)=\\pi(y)P(x|y),\\forall x,y\\in\\Omega\\]\n\rdetailed balance \\(\\rightarrow\\) balance\n\r\r\rThe road map\rThe goal is to build a Markov chain with a unique stationary distribution which equals the targe distribution.\nbuild a Markov chain with a unique stationary distribution. This holds if the Markov chian is irreducible, aperiodic, and not transient. E.g., random walk has a positive probablility of eventually reaching any state from any other state.\n\rstationary distribution = the targe distribution (detailed balance transition)\n\r\r\rThe Metropolis algorithm\rThe Metropolis algorithm (Metropolis et al.Â 1953) is an adaptation of a random walk with an acceptance/rejection rule to converge to the specified target distribution.\n\rtarget distribution: \\(p(x)\\)\n\rsymmetric proposal distribution at time \\(t\\): \\(q(y|x)=q(x|y)\\)\n\r\rThe algorithm goes below:\nfor \\(t=0\\), draw a starting poing \\(x_0\\sim p_0(x)\\)\rfor \\(t=1,2,\\dots\\), sample \\(y_t\\sim q(y_t|x_{t-1})\\), accept \\(y_t\\) as an output of \\(x_t\\) with probability\r\\[r(x_{t-1},y_t)=\\min\\left(\\frac{p(y_t)}{p(x_{t-1})},1\\right)\\]\rOtherwise, taking \\(x_{t}=x_{t-1}\\)\r\r\rThe transition for the Metropolis algorithm\rThe transition is a mixture of a point and a proposal distribution.\n\\[T(y|x) = r(x,y)q(y|x)+\\left[1-\\int r(x,y) q(y|x)dy\\right]1\\{y=x\\}\\]\n\\[r(x,y)=\\min\\left(\\frac{p(y)}{p(x)},1\\right)\\]\nDetailed balance: \\(p(x)T(y|x)=p(y)T(x|y)\\)\nIf \\(x\\neq y\\),\n\\[\\frac{p(x)T(y|x)}{p(y)T(x|y)}=\\frac{p(x)r(x,y)}{p(y)r(y,x)}\\frac{q(y|x)}{q(x|y)}=\\frac{p(x)\\min(p(y)/p(x),1)}{p(y)\\min(p(x)/p(y),1)}=1\\]\n\rRandom walk Metropolis\r\rthe proposal density:\r\r\\[y_{t+1}=x_t+ N(0,\\sigma^2I_d)\\]\n\\[y_{t+1}=x_t+ U[-\\sigma,\\sigma]^d\\]\nHow large a step \\(\\sigma\\)?\n\rTiny step: large \\(p(y_{t+1})/p(x_t)\\), high acceptance\n\rLarge step: small \\(p(y_{t+1})/p(x_t)\\), low acceptance\n\r\rWe might have wanted high acceptance and large moves. But thereâ€™s a tradeoff.\nThe rule of thumb: \\(\\sigma=2.38/\\sqrt{d}\\), see Gelman, Roberts, Gilks (1996)\n\rImprovements on RWM\r\rfor the target distribution \\(p\\approx N(\\mu,\\Sigma)\\)\n\rthe proposal: \\(y_{t+1}\\sim N(x_{t},\\lambda\\hat{\\Sigma})\\)\n\ruse sample \\(x_i\\) to estimate \\(\\Sigma\\)\n\ra tune parameter \\(\\lambda\\)\n\r\r\rExample: Bivariate unit normal with normal proposal\r\rtarget distribution: bivariate unit normal \\(N(0,I_2)\\)\n\rsymmetric proposal distribution: \\(q(y|x)=N(y|x,0.2^2I_2)\\)\n\r\r\rThe Metropolis-Hastings algorithm\rThe Metropolis-Hastings (MH) algorithm (Hastings, 1970) generalizes the basic Metropolis algorithm. The proposal needs no longer be symmetric.\nThe detailed balance:\n\\[p(x)r(x,y)q(y|x)=p(y)r(y,x)q(x|y)\\]\nHow to choose \\(r(x,y)\\) subject to \\(0\\le r(x,y)\\le 1\\)?\nThe result:\n\\[r(x,y)=\\min\\left(\\frac{p(y)q(x|y)}{p(x)q(y|x)},1\\right)\\]\n\rThe independent MH algorithm\r\rthe proposal: \\(q(y|x)=q(y)\\)\r\r\\[r(x,y)=\\min\\left(\\frac{p(y)q(x)}{p(x)q(y)},1\\right)\\]\n\rAlthough the proposal variates are iid, the states are not independent.\n\rWhat-if \\(q(x)=p(x)\\)?\n\r\r\rUnnormalized target density\rThe target density \\(p(x)=C\\tilde{p}(x)\\), where \\(C\u0026gt;0\\) is unknown constant.\n\\[\\frac{p(y)q(x|y)}{p(x)q(y|x)} = \\frac{C\\tilde{p}(y)q(x|y)}{C\\tilde{p}(x)q(y|x)}=\\frac{\\tilde{p}(y)q(x|y)}{\\tilde{p}(x)q(y|x)}\\]\nThe MH algorithm also works for unnormalized proposal density.\n\rEstimating the expectation\r\rThe law of large numbers supports:\r\\[\\hat\\mu = \\frac 1n\\sum_{i=1}^n f(X_i)\\]\n\rBurn-in (skipping the first \\(b\\) observations, e.g., \\(b=n/2\\))\r\\[\\hat\\mu_b = \\frac 1{n-b}\\sum_{i=b+1}^n f(X_i)\\]\n\rThinning (just use every \\(k\\)th observation, \\(k\u0026gt;1\\))\r\\[\\hat\\mu_k = \\frac 1{n/k}\\sum_{i=1}^{n/k} f(X_{ki})\\]\n\r\rSee Owen (2017)\n\rThe Gibbs sampler\rSuppose the targe distribution \\(X=(x_1,\\dots,x_d)\\sim p(X)\\). Maybe we can sample one \\(x_j\\) at a time, with others fixed, i.e., \\(x_j|x_{-j}\\), where \\(x_{-j}=(x_1,\\dots,x_{j-1},x_{j+1},\\dots,x_d)\\).\nRandom scan Gibbs: for \\(t=1,\\dots,n\\)\n\\(j\\sim U\\{1,\\dots,d\\}\\)\r\\(x_{-j}^{(t)}=x_{-j}^{(t-1)}\\)\r\\(x_{j}^{(t)}\\sim p(x_j|x_{-j}^{(t)})\\)\r\rDeterministic scan: \\(j\\) cycles through \\(1,\\dots,d\\) repeatedly, i.e., \\[j=1+(t-1) \\mod d\\]\n\rOne step of Gibbs\rIs a Metropolis-Hastings that always accepts, i.e., \\(r(x,y)\\equiv 1\\).\nProposal \\(q\\) just changes component \\(x_j\\): \\(p(x^{(t)}_j|x_{-j}^{(t-1)})\\)\nDetailed balance:\n\\[\\frac{p(y)q(x|y)}{p(x)q(y|x)}=\\frac{p(y_{-j})p(y_j|y_{-j})p(x_j|y_{-j})}{p(x_{-j})p(x_j|x_{-j})p(y_j|x_{-j})}=1\\]\nNB: \\(x_{-j}=y_{-j}\\)\n\rExample: Bivariate normal distribution\r\rtarget distribution: \\((x_1,x_2)^\\top\\sim N(\\mu,\\Sigma)\\)\n\r\\(\\mu=(\\mu_1,\\mu_2)^\\top,\\sigma_{11}=\\sigma_{22}=1,\\sigma_{12}=\\sigma_{21}=\\rho\\in (-1,1)\\)\n\rconditional distribution:\r\\[x_1|x_2\\sim N(\\mu_1+\\rho(x_2-\\mu_2),1-\\rho^2)\\]\n\r\r\\[x_2|x_1\\sim N(\\mu_2+\\rho(x_1-\\mu_1),1-\\rho^2)\\]\n\rThe simulation\r\rChanllenges of monitoring convergence: mixing and stationarity\r\rDid the chain mix well?\rWe can use the ACF or a trace.\nThe autocorrelation function (ACF) is a measure of the correlation between observations of a time series that are separated by \\(k\\) time units.\nRecent promising work by Gorham \u0026amp; Mackey using Stein discrepancy can\rprovide a â€œYesâ€ (but itâ€™s expensive).\n\rExample: hierarchical normal model\rThe data \\(y_{ij},i=1,\\dots,n_j,j=1,\\dots,J\\):\nUnder the hierarchical normal model:\n\\[y_{ij}\\sim N(\\theta_j,\\sigma^2)\\]\n\\[\\theta_j\\sim N(\\mu,\\tau^2)\\]\nUniform prior distribution: \\((\\mu,\\log \\sigma,\\tau)\\propto 1\\)\r\\[(\\mu,\\log \\sigma,\\log \\tau)\\propto \\tau\\]\n\rPosterior distribution\rThe joint posterior density of all the parameters is\r\\[p(\\theta,\\mu,\\log \\sigma,\\tau|y) \\propto \\tau\\prod_{j=1}^J N(\\theta_j|\\mu,\\tau^2)\\prod_{j=1}^J\\prod_{i=1}^{n_j}N(y_{ij}|\\theta_j,\\sigma^2)\\]\n\rConditional posterior distribution\r\rfor \\(\\theta_j\\)\r\\[\\theta_j|\\mu,\\sigma,\\tau,y\\sim N(\\hat{\\theta}_j,V_{\\theta_j})\\]\n\rfor \\(\\mu\\)\r\\[\\mu|\\theta,\\sigma,\\tau,y\\sim N(\\hat{\\mu},\\tau^2/J),\\ \\hat{\\mu}=\\frac 1J\\sum_{j=1}^J\\theta_j\\]\n\rfor \\(\\sigma^2\\)\r\\[\\sigma^2|\\theta,\\mu,\\tau,y=\\sigma^2|\\theta,y\\sim \\mathrm{Inv-}\\chi^2(n,\\hat{\\sigma}^2)\\]\n\rfor \\(\\tau^2\\)\r\\[\\tau^2|\\theta,\\mu,\\sigma,y=\\tau^2|\\theta,\\mu,y\\sim \\mathrm{Inv-}\\chi^2(J-1,\\hat{\\tau}^2)\\]\n\r\r\\[\\hat{\\sigma}^2 = \\frac 1 n\\sum_{j=1}^J\\sum_{i=1}^{n_j}(y_{ij}-\\theta_j)^2,\\ \\hat{\\tau}^2=\\frac{1}{J-1}\\sum_{j=1}^J(\\theta_j-\\mu)^2\\]\n\rThe results\r\rStarting pionts: \\(\\theta_j^{(0)}\\sim U\\{y_{ij},i=1,\\dots,n_j\\},\\mu^{(0)}=\\frac 1 J\\sum_{j=1}^T \\theta_j^{(0)},(\\tau^2)^{(0)}\\sim \\mathrm{Inv-}\\chi^2(J-1,\\hat{\\tau}^2), (\\sigma^2)^{(0)}\\sim \\mathrm{Inv-}\\chi^2(n,\\hat{\\sigma}^2)\\)\n\rThe potential scale reduction \\(\\hat{R}\\), which declines to \\(1\\) as \\(n\\to\\infty\\)\n\r\r\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7c528d1fe2a1b043374bda5d442a4d24","permalink":"/en/courses/bayes/chap11/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap11/","section":"courses","summary":"Markov chains\rConsider the discrete chain:\r\\[P(X_i\\in A|X_0=x_0,\\dots,X_{i-1}=x_{i-1})=P(X_i\\in A|X_{i-1}=x_{i-1})\\]\n\r\\(X_i\\in\\Omega=\\{\\omega_1,\\dots,\\omega_M\\}\\)\n\rTransition distribution:\r\\[P(X_i=y|X_{i-1}=x)=T_i(y|x)\\]\n\rDistribution of this chain is now determined by \\(p_0(x)=P(X_0=x)\\) and \\(T_i(y|x)\\)\n\rHomogeneous chain:\r\\[P(X_i=y|X_{i-1}=x)=P(X_1=y|X_0=x)=T(y|x)\\]","tags":null,"title":"ç¬¬11ç« ","type":"docs"},{"authors":null,"categories":null,"content":"3 steps in BDA   set up the statistical model\n  compute the posterior distribution\n  model checking and model improvement\n  Statistical inference Goal: draw conclusions about unobserved quantities from the data (observed)\n  potentially observable quantities, e.g., future observations of a process\n  not directly observable quantities, e.g., unobservable population parameters\n  Notations and assumptions   unobservable population parameters of interest: $\\theta=(\\theta_1,\\dots,\\theta_m)$\n  the observed data: $y=(y_1,\\dots,y_n)$\n  potentially observable quantities: $\\tilde y$, e.g., $\\tilde y=y_{n+1}$\n  Assumption 1\n exchangeability: the $n$ values $y_i$ are exchangeable, e.g., iid samples conditonal on the population parameters.  Assumption 2\n conditional independence of $y$ and $\\tilde y$ given $\\theta$  Bayesian inference To make inferences about the posterior distributions, such as $p(\\theta|y)$ and $p(\\tilde y|y)$\nBayes\u0026rsquo; rule\n$$p(\\theta|y)=\\frac{p(\\theta,y)}{p(y)}=\\frac{p(y|\\theta)p(\\theta)}{p(y)}$$\n$$p(\\theta|y)\\propto p(y|\\theta)p(\\theta)$$\nThe imiplied constant is $$p(y)=\\int p(y|\\theta)p(\\theta) d \\theta.$$\nPrediction To make inferences about an unknown observable quantity\n  prior predictive distribution: $p(y)$\n  posterior predictive dsitribution: $p(\\tilde y|y)$\n  $$ p(\\tilde y|y) = \\int p(\\tilde y,\\theta|y)d\\theta = \\int p(\\tilde y|\\theta,y)p(\\theta|y)d \\theta = \\int p(\\tilde y|\\theta)p(\\theta|y)d \\theta $$\nAgain, $y$ and $\\tilde y$ are conditionally independent given $\\theta$.\nLikelihood $p(y|\\theta)$ is called the likelihood function, which is regarded as a function of $\\theta$.\nodds ratios\n$$\\frac{p(\\theta_1|y)}{p(\\theta_2|y)}=\\frac{p(\\theta_1)p(y|\\theta_1)/p(y)}{p(\\theta_2)p(y|\\theta_2)/p(y)}=\\frac{p(\\theta_1)}{p(\\theta_2)}\\frac{p(y|\\theta_1)}{p(y|\\theta_2)}$$\nposterior odds = prior odds $\\times$ likelihood ratio\nExample 1: inference about a genetic status  males: one X-chromosome + one Y-chromosome females: two X-chromosomes  Hemophilia is a disease that exhibits X-chromosome-linked recessive inheritance. The disease is generally fatal for women who inherit two such genes.\nConsider a woman who has an affected brother and her father is not affected. Let $\\theta$ be the state of the woman: a carrier of the gene ($\\theta=1$) or not ($\\theta=0$).\nPrior distribution: $P(\\theta=1)=P(\\theta=0)=0.5$\nData and model: She has two sons. Let $y_i=1$ or 0 denote the state of her sons. Now observe that her sons are not affected. Given $\\theta$, $y_1$ and $y_2$ are iid.\nExample 1: inference about a genetic status Likelihood function:\n$$P(y_1=0,y_2=0|\\theta=1)=0.5\\times 0.5=0.25$$\n$$P(y_1=0,y_2=0|\\theta=0)=1\\times 1=1$$\nPosterior distribution:\n$$P(\\theta=1|y) = \\frac{p(y|\\theta=1)p(\\theta=1)}{p(y)}=0.2$$\nExample 1: inference about a genetic status Adding more data: suppose that the woman has a third son, who is also unaffacted.\n$$P(\\theta=1|y_1,y_2,y_3) = \\frac{0.5\\times 0.2}{0.5\\times 0.2+1\\times 0.8}=0.111$$\nA key aspect of Bayesian analysis is the ease with which sequential analyses can be performed.\nQuestion: What happen if we suppose that the third son is affected?\nExample 2: spelling correction Classification of words is a problem of managing uncertainty. Suppose someone types radom. How should that be read?\n random radon radom  Data and model: Let $\\theta$ be the word that the person was intending to type, and let $y$ as the data. Now $y=$'radom\u0026rsquo; and $\\theta\\in${$\\theta_1$='random\u0026rsquo;,$\\theta_2$='radon\u0026rsquo;,$\\theta_3$='radom\u0026rsquo;}. The posterior density is\n$$P(\\theta|y=\\text{\u0026lsquo;radom\u0026rsquo;})\\propto p(\\theta)P(y=\\text{\u0026lsquo;radom\u0026rsquo;}|\\theta).$$\nExample 2: spelling correction Prior distribution: Here are probabilities supplied by researchers at Google. Goole Ngram Viewer: https://books.google.com/ngrams\n   $\\theta$ $p(\\theta)$     random $7.60\\times 10^{-5}$   radon $6.05\\times 10^{-6}$   radom $3.12\\times 10^{-7}$    Likelihood: Here are some conditional probabilities from Google\u0026rsquo;s model of spelling and typing errors:\n$\\theta$ | $p(\\text{\u0026lsquo;radom\u0026rsquo;}|\\theta)$ | -|-| random | $0.00193$ | radon | $0.000143$ | radom | $0.975$ |\nExample 2: spelling correction Posterior distribution:\n$\\theta$ | $p(\\theta)P(y=\\text{\u0026lsquo;radom\u0026rsquo;}|\\theta)$ | $P(\\theta|y=\\text{\u0026lsquo;radom\u0026rsquo;})$ | -|-|-| random | $1.47\\times 10^{-7}$ | 0.325 | radon | $8.65\\times 10^{-10}$ | 0.002 | radom | $3.04\\times 10^{-7}$ | 0.673 |\nModel improvement:\n including contextual info in the prior probabilities, e.g., statistical book. let $x$ be the contextual information used by the model.  $$p(\\theta|x,y)\\propto p(\\theta|x)p(y|\\theta,x)$$\n for simplicity, we may assume $p(y|\\theta,x)=p(y|\\theta)$.  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6bbe09d0aae49eeba7ad5f083e2ccbd0","permalink":"/en/courses/bayes/chap2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap2/","section":"courses","summary":"3 steps in BDA   set up the statistical model\n  compute the posterior distribution\n  model checking and model improvement\n  Statistical inference Goal: draw conclusions about unobserved quantities from the data (observed)","tags":null,"title":"ç¬¬2ç« ","type":"docs"},{"authors":null,"categories":null,"content":"\rBinomial models\rEstimating a probability from binomial data\r\rlet \\(\\theta\\) be the proportion of successes in the population\n\rthe data \\((y_1,\\dots,y_n)\\in \\{0,1\\}^n\\)\n\rthe total number of successes in the \\(n\\) trials is denoted by \\(y\\)\n\rthe binomial model is\r\\[p(y|\\theta) = C_n^y\\theta^y(1-\\theta)^{n-y}\\]\n\rthe posterior distribution is\r\\[p(\\theta|y) \\propto p(\\theta)p(y|\\theta)\\propto p(\\theta)\\theta^y(1-\\theta)^{n-y}\\]\n\r\rExample: estimating the probability of a female birth\n\rA total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770.\r\r\rHow to choose a proper prior?\r\rA naive choice for \\(p(\\theta)\\) is uniform on the interval \\([0,1]\\). Then\r\\[p(\\theta|y) \\propto \\theta^y(1-\\theta)^{n-y}\\]\n\rthat is, \\(\\theta|y\\sim Beta(y+1,n-y+1)\\)\n\r\r- \\(P(\\theta\\ge 0.5|y=241945,n=241945+251527)\\approx 1.15\\times 10^{-42}\\)\n\rPrediction\r\rLet \\(\\tilde y\\) be the result of a new trial\r\\[P(\\tilde y =1|y) = \\int_0^1 P(\\tilde y=1|\\theta,y)p(\\theta|y)d \\theta=E[\\theta|y]=\\frac{y+1}{n+2}\\]\r\rPosterior as compromise between data and prior information\n\rprior mean is \\(1/2\\)\rsample mean is \\(y/n\\)\rposterior mean is \\((y+1)/(n+2)\\)\rthe compromise is controlled to a greater extent by the data as the sample size\rincreases.\r\r\rPosterior quantiles and intervals\r\rlet \\(T_1\\) be the \\(\\alpha/2\\) quantile of the posterior distribution\rlet \\(T_2\\) be the \\(1-\\alpha/2\\) quantile of the posterior distribution\r\\(100(1-\\alpha)\\%\\) posterior interval is \\([T_1,T_2]\\)\r\r\rcompare with the usual confidence interval\r\r\rInformative prior distributions\rGoal: assigning a prior distribution that reflects substantive info.\n\rthe likelihood is\r\\[p(y|\\theta) \\propto \\theta^y(1-\\theta)^{n-y}\\]\n\rchoose a prior as a \\(Beta(\\alpha,\\beta)\\) distribution:\r\\[p(\\theta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\n\rthe parameters \\(\\alpha,\\beta\u0026gt;0\\) of the prior distribution is called hyperparameters.\n\rthe posterior is\r\\[p(\\theta|y)\\propto \\theta^{\\alpha+y-1}(1-\\theta)^{n-y+\\beta-1}=Beta(\\alpha+y,\\beta+n-y)\\]\n\r\r\rInformative prior distributions\r\rthe posterior mean is\r\\[E[\\theta|y]=\\frac{\\alpha+y}{\\alpha+\\beta+n}\\]\rwhich lies between the sample proportion \\(y/n\\) and the prior mean \\(\\alpha/(\\alpha+\\beta)\\)\n\rthe posterior variance is\r\\[Var[\\theta|y]=\\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+n+1)}=\\frac{E[\\theta|y](1-E[\\theta|y])}{\\alpha+\\beta+n+1}\\]\n\ras \\(y\\) and \\(n\\) become large with fixed \\(\\alpha,\\beta\\),\r\\[E[\\theta|y]\\approx \\frac yn,\\ Var[\\theta|y]\\approx \\frac 1n\\frac yn(1-\\frac yn).\\]\n\r\r\rConjugate prior distributions\rDefinition: If \\(\\mathcal{F}\\) is a class of sampling distribution \\(p(y|\\theta)\\), and \\(\\mathcal{P}\\) is a class of prior distributions for \\(\\theta\\), then the class \\(\\mathcal{P}\\) is conjugate for \\(\\mathcal{F}\\) if\r\\[p(\\theta|y)\\in \\mathcal{P} \\text{ for all } p(\\cdot|\\theta)\\in\\mathcal{F} \\text{ and }p(\\cdot)\\in\\mathcal{P}.\\]\nAdvantages of conjugate prior distributions\n\rcomputational convenience\rcan be interpreted as additional data\r\r\rExponential families\rDefinition: The class \\(\\mathcal{F}\\) is an exponential family if all its members have the form\r\\[p(y_i|\\theta)=f(y_i)g(\\theta)\\exp[\\phi(\\theta)^\\top u(y_i)].\\]\n\r\\(f(\\cdot)\\ge 0\\)\r\\(\\phi(\\theta)\\) is called the natural parameter\r\rFor iid samples, we have\r\\[p(y|\\theta)=\\left(\\prod_{i=1}^n f(y_i)\\right)g(\\theta)^n\\exp\\left[\\phi(\\theta)^\\top \\sum_{i=1}^nu(y_i)\\right]\r\\]\r\\[p(y|\\theta)\\propto g(\\theta)^n\\exp[\\phi(\\theta)^\\top t(y)]\\]\n\rwhere \\(t(y)=\\sum_{i=1}^nu(y_i)\\) (i.e., a sufficient statistic for \\(\\theta\\)).\r\r\rConjugate prior distribution for exponential families\rIf the prior distribution is specified as\r\\[p(\\theta)\\propto g(\\theta)^\\eta \\exp[\\phi(\\theta)^\\top \\nu],\\]\rthen the posterior density is\r\\[p(\\theta|y)\\propto g(\\theta)^{\\eta+n} \\exp[\\phi(\\theta)^\\top (\\nu+t(y))].\\]\nA list of exponential families\n\rbinomial distributions\rnormal distributions\rexponential distributions\rpossion distributions\r\r\rExample: Probability of a girl birth given placenta previa\rAn early study concerning the sex of placenta previa births in Germany found that of a total of 980 births, 437 were female.\nHow much evidence does this provide for the claim that the proportion of female births in the population of placenta previa births is less than 0.485, the proportion of female births in the general population?\n\rusing a uniform prior: the posterior is \\(Beta(438,544)\\). The central \\(95\\%\\) posterior interval is \\([0.415,0.477]\\).\n\rusing conjugate prior \\(Beta(\\alpha,\\beta)\\)\n\rusing nonconjugate prior\n\r\r\rDifferent conjugate prior distributions\r\r\r\\(\\alpha/(\\alpha+\\beta)\\)\r\\(\\alpha+\\beta\\)\rposterior median\r\\(95\\%\\) posterior interval\r\r\r\r0.500\r2\r0.446\r[0.415, 0.477]\r\r0.485\r2\r0.446\r[0.415, 0.477]\r\r0.485\r5\r0.446\r[0.415, 0.477]\r\r0.485\r10\r0.446\r[0.415, 0.477]\r\r0.485\r20\r0.447\r[0.416, 0.478]\r\r0.485\r100\r0.450\r[0.420, 0.479]\r\r0.485\r200\r0.453\r[0.424, 0.481]\r\r\r\r\rPosterior inferences based on a large sample are not sensitive to the prior distribution.\rAll the \\(95\\%\\) posterior intervals exclude the prior mean.\r\r\rThe effect of prior distributions\r\rUsing a nonconjugate prior distribution\r\\(95\\%\\) posterior interval is [0.419, 0.480]\n\r\rNormal models\rEstimating a normal mean with known variance\rLikelihood function:\r\\[p(y|\\theta) = \\prod_{i=1}^n \\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-\\theta)^2}{2\\sigma^2}}\\propto e^{-\\frac{n\\theta^2}{2\\sigma^2}}e^{\\frac{n\\theta\\bar y}{\\sigma^2}}\\]\nConjugate prior: \\(\\theta\\sim N(\\mu_0,\\tau_0^2)\\)\nPosterior distribution:\r\\[p(\\theta|y)\\propto e^{-\\frac{n\\theta^2}{2\\sigma^2}}e^{\\frac{n\\theta\\bar y}{\\sigma^2}}e^{-\\frac{\\theta^2}{2\\tau_0^2}}e^{\\frac{\\mu_0\\theta}{\\tau_0^2}}=N(\\mu_n,\\tau_n^2)\\]\nwhere\r\\[\\mu_n=\\frac{\\frac 1{\\tau_0^2}\\mu_0+\\frac n{\\sigma^2}\\bar y}{\\frac 1{\\tau_0^2}+\\frac n{\\sigma^2}},\\ \\frac1{\\tau_n^2}=\\frac{1}{\\tau_0^2}+\\frac n{\\sigma^2}.\\]\n\rComments\r\rthe inverse of the variance plays a prominet role and is called the precision\rposterior precision = prior precision + data precision\rthe posterior mean is expressed as a weighted average of the prior mean \\(\\mu_0\\) and the sample mean \\(\\bar y\\), with weights proportional to the precisions.\rwhat happens if \\(n\\to \\infty\\) with \\(\\tau_0^2\\) fixed? data info. dominated!\rwhat happens if \\(\\tau_0\\to \\infty\\) with \\(n\\) fixed? This would result from assuming \\(p(\\theta)\\) is proportional to a constant for \\(\\theta\\in(-\\infty,\\infty)\\). (improper prior, serves as an noninformative prior)\r\r\rNormal distribution with known mean but unknown variance\rLikelihood function:\r\\[p(y|\\sigma^2)=\\prod_{i=1}^n \\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}\\propto \\sigma^{-n}\\exp\\left[-\\frac{n}{2\\sigma^2}\\nu\\right]\\]\rwhere the sufficient statistic is\r\\[\\nu=\\frac 1n\\sum_{i=1}^n(y_i-\\mu)^2.\\]\rConjugate prior density:\r\\[p(\\sigma^2)\\propto (\\sigma^2)^{-(\\alpha+1)}e^{-\\beta/\\sigma^2},\\]\rwhere the hyperparameters is \\((\\alpha,\\beta)\\).\nWe may take \\(\\sigma^2\\sim \\text{Inv-}\\chi^2(\\nu_0,\\sigma^2_0)\\) as a prior (i.e., \\(\\sigma^2\\stackrel d {=}\\sigma_0^2\\nu_0/\\chi^2_{\\nu_0}\\)), whose PDF is given by\r\\[p(\\sigma^2) =\\frac{(\\nu_0/2)^{\\nu_0/2}}{\\Gamma(\\nu_0/2)}\\sigma^{\\nu_0}_0(\\sigma^2)^{-(\\nu_0/2+1)}e^{-\\nu_0\\sigma_0^2/(2\\sigma^2)}.\\]\n\rNormal distribution with known mean but unknown variance\rPrior density:\r\\[p(\\sigma^2)= \\text{Inv-}\\chi^2(\\nu_0,\\sigma^2_0)\\]\nPosterior density:\r\\[p(\\sigma^2|y)=\\text{Inv-}\\chi^2\\left(\\nu_0+n,\\frac{\\nu_0\\sigma_0^2+n\\nu}{\\nu_0+n}\\right)\\]\n\rdegree of freedom = sum of the prior and data\n\rscale = weighted average of the prior and data\n\rif \\(\\nu_0=0\\), \\(p(\\sigma^2|y)=\\text{Inv-}\\chi^2(n,\\nu)\\), as effectively taking \\(p(\\sigma^2)\\propto 1/\\sigma^2\\) (improper prior, serves as an noninformative prior)\n\r\r\r\rPoisson models\rPoisson models\rApplications: The Possion distribution arises naturally in the study of data taking the form of counts.\n\rnumber of customer on the queue over an unit time\repidemiology â€“ the incidence of diseases\r\rLikelihood function:\r\\[p(y|\\theta) = \\prod_{i=1}^n\\frac{\\theta^{y_i}e^{-\\theta}}{y_i!}\\propto \\theta^{t(y)}e^{-n\\theta}\\propto e^{-n\\theta}e^{t(y)\\log \\theta}\\]\n\r\\(t(y)=\\sum_{i=1}^n y_i\\) is the sufficient statistic\rthe natural parameter is \\(\\log \\theta\\)\r\rConjugate prior:\r\\[p(\\theta)\\propto e^{-\\eta\\theta}e^{\\nu\\log \\theta}\\]\nSo we may choose \\(p(\\theta)=Gamma(\\alpha,\\beta)\\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\\)\n\rPoisson models\rPrior density: \\(p(\\theta)=Gamma(\\alpha,\\beta)\\)\nPosterior density: \\(p(\\theta|y)=Gamma(\\alpha+n\\bar y,\\beta+n)\\)\nMarginal density:\r\\[p(y_i)=C_{\\alpha+y_i-1}^{y_i} \\left(\\frac{\\beta}{\\beta+1}\\right)^\\alpha\\left(\\frac{1}{\\beta+1}\\right)^{y_i}\\]\n\r\\(y_i\\sim \\text{Neg-bin}(\\alpha,\\beta)\\), i.e., the negative binomial distribution\r\r\rPossion models: an extension\rIn many applications, it is convenient to extend the Possion model for data pionts \\(y_1,\\dots,y_n\\) to the form\r\\[y_i\\sim Poission(x_i\\theta),\\]\n\rthe values \\(x_i\\) are known positive values of an explanatory variable \\(x\\), called the exposure of the \\(i\\)th unit\n\r\\(\\theta\\) is unknown, called the rate\n\r\rPrior density: \\(p(\\theta)=Gamma(\\alpha,\\beta)\\)\nPosterior density:\r\\[p(\\theta|y)=Gamma(\\alpha+\\sum_{i=1}^ny_i,\\beta+\\sum_{i=1}^nx_i)\\]\nExample: Bayesian inference for the cancer death rates (p.48)\n\r\rExponential models\rExponential models\rApplications: The expoential distribution is commonly used to model â€˜waiting timesâ€™ and other continuous, poisitive, real-valued random variables. It has a â€˜memorylessâ€™ property that makes it a natural model for survival or lifetime data.\nLikelihood function:\r\\[p(y|\\theta) = \\prod_{i=1}^n\\theta \\exp(-y_i\\theta)= \\theta^{n}e^{-n\\bar y \\theta}\\]\nPrior density: \\(p(\\theta)=Gamma(\\alpha,\\beta)\\)\nPosterior density:\r\\[p(\\theta|y)=Gamma(\\alpha+n,\\beta+n\\bar y)\\]\n\rSummary\r\r\rPopulation\rParameter\rConjugate prior\r\r\r\rBinomial\rprobability of success\rBeta dist.\r\rPossion\rmean\rGamma dist.\r\rExponential\rinverse mean\rGamma dist.\r\rNormal (known variance)\rmean\rNormal dist.\r\rNormal (known mean)\rvariance\rInv-Gamma dist.\r\r\r\r\rEnd notes\r\rtwo kinds of prior distributions: uniform (noninformative) and conjugate (informative)\n\rsome other noninformative prior distributions: Jeffreysâ€™ prior etc. See pp.52-56\n\rnoninformative prior are often useful when it does not seem to be worth the effort to quantify oneâ€™s real prior knowledge as a probability distribution\n\rwhen using conjugate prior, it remains to choose the hyperparameters; see Chapter 5 for hierarchical models\n\r\r\r\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"86d5e5d011a52ada9195db5ad4718d9c","permalink":"/en/courses/bayes/chap3/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap3/","section":"courses","summary":"Binomial models\rEstimating a probability from binomial data\r\rlet \\(\\theta\\) be the proportion of successes in the population\n\rthe data \\((y_1,\\dots,y_n)\\in \\{0,1\\}^n\\)\n\rthe total number of successes in the \\(n\\) trials is denoted by \\(y\\)","tags":null,"title":"ç¬¬3ç« ","type":"docs"},{"authors":null,"categories":null,"content":"\rNuisance parameters\r\rthere are more than one unknown or unobservable parameters\n\rconclusions will often be drawn about one, or only a few parameters at a time\n\rthere is no interest in making inferences about many of the unknown parameters â€“ nuisance parameters\n\rsuppose \\(\\theta=(\\theta_1,\\theta_2)\\)\n\rinterest centers only on \\(\\theta_1\\); \\(\\theta_2\\) is a â€˜nuisanceâ€™ parameter.\n\rthe joint posterior density:\r\\[p(\\theta_1,\\theta_2|y)\\propto p(y|\\theta_1,\\theta_2)p(\\theta_1,\\theta_2)\\]\n\rthe marginal posterior density:\r\\[p(\\theta_1|y)=\\int p(\\theta_1,\\theta_2|y)d\\theta_2=\\int p(\\theta_1|\\theta_2,y)p(\\theta_2|y)d\\theta_2\\]\n\r\r\rNormal data with a noninformative prior distribution\rLikelihood function:\r\\[p(y|\\mu,\\sigma^2)=\\prod_{i=1}^n \\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-\\theta)^2}{2\\sigma^2}}\\]\nNoninformative prior distribution:\r\\[p(\\mu,\\sigma^2)\\propto 1\\times (\\sigma^2)^{-1}\\]\nPosterior distribution:\r\\[p(\\mu,\\sigma^2|y)\\propto \\sigma^{-n-2}e^{-\\frac{\\sum_{i=1}^n(y_i-\\theta)^2}{2\\sigma^2}}=\\sigma^{-n-2}e^{-\\frac{(n-1)s^2+n(\\bar y-\\mu)^2}{2\\sigma^2}}\\]\n\r\\(s^2=\\frac 1{n-1}\\sum_{i=1}^n(y_i-\\bar y)^2\\) is the sample variance\r\r\rNormal data with a noninformative prior distribution\rConditional posterior distribution:\r\\[p(\\mu|\\sigma^2,y)\\sim N(\\bar y,\\sigma^2/n)\\]\nMarginal posterior distribution \\(p(\\sigma^2|y)\\):\r\\[p(\\sigma^2|y)\\propto \\int \\sigma^{-n-2}e^{-\\frac{(n-1)s^2+n(\\bar y-\\mu)^2}{2\\sigma^2}} d\\mu=(\\sigma^2)^{-\\frac{n+1}2}e^{-\\frac{(n-1)s^2}{2\\sigma^2}}\\]\n\\[\\sigma^2|y\\sim \\text{Inv-}\\chi^2(n-1,s^2)\\]\n\rNormal data with a noninformative prior distribution\rMarginal posterior distribution \\(p(\\mu|y)\\):\n\\[p(\\mu|y)\\propto \\int_0^\\infty \\sigma^{-n-2}e^{-\\frac{(n-1)s^2+n(\\bar y-\\mu)^2}{2\\sigma^2}} d\\sigma^2\\propto \\left[1+\\frac{n(\\mu-\\bar y)^2}{(n-1)s^2}\\right]^{-\\frac n2}\\]\n\\[\\mu|y\\sim t_{n-1}(\\bar y,s^2/n),\\ \\frac{\\mu-\\bar y}{s/\\sqrt{n}}\\Big|y\\sim t_{n-1}\\]\nPosterior predictive distribution for a future observation\n\\[\\tilde y|y \\sim t_{n-1}(\\bar y,(1+1/n)s^2)\\]\n\rExample: Estimating the speed of light\rSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcom measured the amount of time rquired for light to travel a distance of 7442 meters (66 measurements, from Stigler (1977), the data are recorded as deviations from 24800 nanoseconds).\n\r\\(n=66,\\ \\bar y = 26.2,\\ s = 10.8\\)\r\\((\\mu-26.2)/(10.8/\\sqrt{66})|y\\sim t_{65}\\)\r\\(95\\%\\) central posterior interval for \\(\\mu\\) is \\(26.2\\pm 10.8t_{65,0.975}/\\sqrt{66}=[23.6,28.8]\\)\rthe speed of light is 299792458 m/s, so the true value for \\(\\mu\\) is \\(23.8\\) nanoseconds\r\r\rExample: Estimating the speed of light\r\rNormal data with a conjugate prior distribution\rPrior distribution:\r\\[\\mu|\\sigma^2\\sim N(\\mu_0,\\sigma^2/\\kappa_0),\\]\n\\[\\sigma^2\\sim \\text{Inv-}\\chi^2(\\nu_0,\\sigma_0^2).\\]\n\\[p(\\mu,\\sigma^2)\\propto \\sigma^{-1}(\\sigma^2)^{-(\\nu_0/2+1)}\\exp\\left(-\\frac 1{2\\sigma^2}[\\nu_0\\sigma^2+\\kappa_0(\\mu_0-\\mu)^2]\\right)\\]\n\rdenoted by \\(\\text{N-Inv-}\\chi^2(\\mu_0,\\sigma^2_0/\\kappa_0;\\nu_0,\\sigma_0^2)\\)\r\r\rNormal data with a conjugate prior distribution\rPosterior distribution:\n\\[\\mu,\\sigma^2|y\\sim \\text{N-Inv-}\\chi^2(\\mu_n,\\sigma^2_n/\\kappa_n;\\nu_n,\\sigma_n^2)\\]\n\\[\r\\begin{cases}\r\\mu_n \u0026amp;= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar y\\\\\r\\kappa_n \u0026amp;= \\kappa_0+n\\\\\r\\nu_n\u0026amp;=\\nu_0+n\\\\\r\\nu_n\\sigma_n^2 \u0026amp;= \\nu_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar y-\\mu_0)^2\r\\end{cases}\r\\]\n\r\\(\\mu|\\sigma^2,y\\sim N(\\mu_n,\\sigma^2/\\kappa_n)\\)\r\\(\\sigma^2|y\\sim \\text{Inv-}\\chi^2(\\nu_n,\\sigma_n^2)\\)\r\\(\\mu|y\\sim t_{\\nu_n}(\\mu_n,\\sigma_n^2/\\kappa_n)\\)\r\r\rMultinormal model for categorical data\rThe multinomial sampling distribution is used to describe data for which each observation is one of \\(k\\) possible outcomes. If \\(y\\) is the vector of counts of the number of observations of each outcome, then\r\\[p(y|\\theta)\\propto \\prod_{j=1}^k\\theta_j^{y_j},\\]\rwhere \\(\\sum_{j=1}^k\\theta_j=1\\).\nConjugate prior:\r\\[p(\\theta|\\alpha)\\propto \\prod_{j=1}^k\\theta_j^{\\alpha_j-1}\\]\n\rDirichlet distribution\r\rPosterior distribution:\r\\[p(\\theta|y)\\propto \\prod_{j=1}^k\\theta_j^{y_j+\\alpha_j-1}\\]\n\rMultivariate normal model with known variance\rLikelihood function:\r\\[p(y_1,\\dots,y_n|\\mu,\\Sigma)\\propto |\\Sigma|^{-n/2}\\exp\\left(-\\frac 12\\sum_{i=1}^n(y_i-\\mu)^\\top\\Sigma^{-1}(y_i-\\mu)\\right)\\]\nConjuate prior: \\(\\mu\\sim N(\\mu_0,\\Lambda_0)\\)\nPosterior distribution: \\(\\mu|y\\sim N(\\mu_n,\\Lambda_n)\\)\n\r\\(\\mu_n=(\\Lambda_n^{-1}+n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\mu_0+n\\Sigma^{-1}\\bar y)\\)\r\\(\\Lambda_n^{-1} = \\Lambda_n^{-1}+n\\Sigma^{-1}\\)\r\r\rMultivariate normal model with unknown mean and variance\rPrior distribution: the normal-inverse-Wishart \\((\\mu_0,\\kappa_0;\\nu_0,\\Lambda_0)\\)\n\\[\\Sigma\\sim \\text{Inv-Wishart}_{\\nu_0}(\\Lambda_0^{-1})\\]\r\\[\\mu|\\Sigma\\sim N(\\mu_0,\\Sigma/\\kappa_0)\\]\r\\[p(\\mu,\\Sigma)\\propto |\\Sigma|^{-\\frac{\\nu_0+d}{2}-1}\\exp\\left(-\\frac{1}{2}tr(\\Lambda_0\\Sigma^{-1})-\\frac {\\kappa_0}2(\\mu-\\mu_0)^\\top\\Sigma^{-1}(\\mu-\\mu_0)\\right)\\]\nPosterior distribution: the normal-inverse-Wishart \\((\\mu_n,\\kappa_n;\\nu_0,\\Lambda_n)\\)\n\\[\r\\begin{cases}\r\\mu_n \u0026amp;= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar y\\\\\r\\kappa_n \u0026amp;= \\kappa_0+n\\\\\r\\nu_n\u0026amp;=\\nu_0+n\\\\\r\\Lambda_n \u0026amp;= \\Lambda_0+\\sum_{i=1}^n(y_i-\\bar y)(y_i-\\bar y)^\\top+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar y-\\mu_0)(\\bar y-\\mu_0)^\\top\r\\end{cases}\r\\]\n\rMultivariate normal model with unknown mean and variance\r\r\\(\\Sigma|y\\sim \\text{Inv-Wishart}_{\\nu_n}(\\Lambda_n^{-1}),\\ \\mu|\\Sigma,y\\sim N(\\mu_n,\\Sigma/\\kappa_n)\\)\n\r\\(\\mu|y \\sim t_{\\nu_n-d+1}(\\mu_,\\Lambda_n/(\\kappa_n(\\nu_n-d+1)))\\) multivariate t distriution.\n\r\\(\\tilde y|y \\sim t_{\\nu_n-d+1}(\\mu_,(k_n+1)\\Lambda_n/(\\kappa_n(\\nu_n-d+1)))\\)\n\r\r\rWishart distributions\rLet \\(X_i \\stackrel {iid}\\sim N_p(0, \\Sigma)\\), where \\(\\Sigma\\) is \\(p\\times p\\) definite matrix, and \\(i=1,\\dots,n\\).\n\\[W=\\sum_{i=1}^n X_iX_i\u0026#39;\\in R^{p\\times p}\\]\nis called the Wishart distribution with \\(n\\) degree of freedom, denoted by \\(\\text{Wishart}_n(\\Sigma)\\).\n\rif \\(p=\\Sigma = 1\\), then it is a chi-squared distribution with \\(n\\) degrees of freedom.\n\rFor \\(n \\ge p\\) the matrix \\(W\\) is invertible with probability 1.\n\rif \\(X_i \\stackrel {iid}\\sim N_p(\\mu, \\Sigma)\\), then\n\r\r\\[(n-1)S^2 = \\sum_{i=1}^n (X_i-\\bar X)(X_i-\\bar X)\u0026#39;\\sim \\text{Wishart}_{n-1}(\\Sigma).\\]\n\rInverse-Wishart distributions\rIf \\(W\\sim \\text{Wishart}_n(\\Sigma)\\) with \\(n\\ge p\\), then \\(W^{-1}\\sim \\text{Inv-Wishart}_n(\\Sigma^{-1})\\).\n\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ef4274820c9b9ca5d3e38a963234c506","permalink":"/en/courses/bayes/chap4/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap4/","section":"courses","summary":"Nuisance parameters\r\rthere are more than one unknown or unobservable parameters\n\rconclusions will often be drawn about one, or only a few parameters at a time\n\rthere is no interest in making inferences about many of the unknown parameters â€“ nuisance parameters","tags":null,"title":"ç¬¬4ç« ","type":"docs"},{"authors":null,"categories":null,"content":"\rLarge-sample theory\rAssumptions and notations:\n\rtrue distribution: \\(y_i\\stackrel {iid}{\\sim} f(\\cdot)\\)\n\r\\(\\theta\\in\\Theta\\)\n\rprior distribution: \\(p(\\theta)\\)\n\rmodel distribution: \\(p(y|\\theta)\\)\n\rKullback-Leibler divergence: a measure of â€˜discrepancyâ€™ between the model and the true distribution\r\\[KL(\\theta)= E_f\\left[\\log\\left(\\frac{f(y)}{p(y|\\theta)}\\right)\\right]=\\int \\log\\left(\\frac{f(y)}{p(y|\\theta)}\\right)f(y)dy\\]\n\r\\(\\theta_0\\): the unique minimizer of \\(KL(\\theta)\\)\n\rif \\(f(y) = p(y|\\theta)\\) then \\(\\theta=\\theta_0\\)\n\r\r\rConsistency of the posterior distribution\rDiscrete parmeter space: If the parameter space \\(\\Theta\\) is finite and \\(P(\\theta=\\theta_0)\u0026gt;0\\), then\r\\[P(\\theta=\\theta_0|y)\\to 1\\text{ as }n\\to \\infty,\\]\rwhere \\(\\theta_0=\\arg_{\\theta\\in\\Theta} KL(\\theta)\\).\nContinuous parmeter space: If \\(\\theta\\) is defined on a compace set \\(\\Theta\\) and \\(A\\) is a neighborhood of \\(\\theta_0\\) with \\(P(\\theta\\in A)\u0026gt;0\\), then\r\\[P(\\theta\\in A|y)\\to 1\\text{ as }n\\to \\infty,\\]\rwhere \\(\\theta_0=\\arg_{\\theta\\in\\Theta} KL(\\theta)\\).\nSee the proofs in Appendix B.\n\rNormal approximations to the posterior distribution\r\r\\(\\hat \\theta\\): the posterior mode\n\rTaylor series expansion of \\(\\log p(\\theta|y)\\) gives\r\\[\\log p(\\theta|y) = \\log p(\\hat \\theta|y)-\\frac 12 (\\theta-\\hat\\theta)^\\top I(\\hat \\theta) (\\theta-\\hat\\theta) + \\cdots \\]\n\rwhere \\(I(\\theta)\\) is the observed information\r\\[I(\\theta)=-\\frac{d^2}{d\\theta^2}\\log p(\\theta|y)\\]\n\rNormal approximation: \\(p(\\theta|y)\\approx N(\\hat\\theta,[I(\\hat\\theta)]^{-1})\\)\n\rFisher information:\r\\[J(\\theta)=-E_f\\left[\\frac{d^2}{d\\theta^2}\\log p(y_j|\\theta)\\right]\\]\n\r\r\rConvergence of the posterior distribution to normality\rTheorem: Under some regularity conditions (notably that \\(\\theta\\) not be on the boundary of \\(\\Theta\\)), as \\(n\\to \\infty\\), the posterior distribution of \\(\\theta\\) approaches normality with mean \\(\\theta_0\\) and variance \\([nJ(\\theta_0)]^{-1}\\), where \\(\\theta_0=\\arg_{\\theta\\in\\Theta} KL(\\theta)\\) and \\(J\\) is the Fisher information.\nOberved that:\n\r\\(\\hat\\theta\\to \\theta_0\\) as \\(n\\to \\infty\\)\n\r\\(I(\\hat\\theta)=-\\frac{d^2}{d\\theta^2}\\log p(\\hat\\theta)-\\sum_{i=1}^n\\frac{d^2}{d\\theta^2}\\log p(y_i|\\hat\\theta)\\approx nJ(\\theta_0)\\)\n\r\\(nJ(\\theta_0)=\\frac{d^2}{d\\theta^2} KL(\\theta_0)\u0026gt;0\\)\n\r\rNB: \\(\\frac{d^2}{d\\theta^2} KL(\\theta)= \\frac{d^2}{d\\theta^2} E_f\\left[-\\log p(y|\\theta) \\right]=-n\\frac{d^2}{d\\theta^2} E_f\\left[\\log p(y_i|\\theta) \\right]=-n E_f\\left[\\frac{d^2}{d\\theta^2}\\log p(y_i|\\theta) \\right]\\) if the interchange of expectation and derivative is allowed.\n\rCounterexamples to the theorems\r\runderidentified models: \\(p(y|\\theta)\\) is equal for a range of values of \\(\\theta\\)\n\rnonindentified parameters: for example, consider the model,\r\\[\\left(\r\\begin{matrix}\ru\\\\\rv\r\\end{matrix}\r\\right)\\sim N \\left( \\left(\\begin{matrix}\r0\\\\\r0\r\\end{matrix}\r\\right),\\left(\\begin{matrix}\r1\u0026amp;\\rho\\\\\r\\rho \u0026amp; 1\r\\end{matrix}\r\\right)\\right)\\]\ronly one of \\(u,v\\) is observed from each pair \\((u,v)\\)\n\rnumber of parameters increasing with sample sizes: new latent parameters with each data point\n\r\r\rPoint estimation, consistency, and efficiency\rpoint estimations:\n\rposterior mode \\(\\hat\\theta(y)=\\arg \\max_{\\theta\\in\\Theta} p(\\theta|y)\\)\rposterior mean \\(\\hat\\theta(y)=E[\\theta|y]=\\int \\theta p(\\theta|y)d \\theta\\) (the optimal one under the Bayesian decision rule)\rposterior median \\(\\hat\\theta(y)=F^{-1}_{\\theta|y}(0.5)\\)\r\rconsistency: \\(\\hat\\theta(y)\\to \\theta_0\\) as \\(n\\to \\infty\\)\nasymptotic unbiasedness: \\(E[\\hat\\theta|\\theta_0]\\to\\theta_0\\) as \\(n\\to \\infty\\)\nefficiency:\r\\[\\text{eff}(\\hat\\theta)=\\frac{\\inf_T E[(T(y)-\\theta_0)^2|\\theta_0]}{E[(\\hat\\theta-\\theta_0)^2|\\theta_0]}\\le 1\\]\nasymptotically efficient: \\(\\text{eff}(\\hat\\theta)\\to 1\\) as \\(n\\to \\infty\\)\n\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"815fb36171c91a59db806202e9d24501","permalink":"/en/courses/bayes/chap5/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap5/","section":"courses","summary":"Large-sample theory\rAssumptions and notations:\n\rtrue distribution: \\(y_i\\stackrel {iid}{\\sim} f(\\cdot)\\)\n\r\\(\\theta\\in\\Theta\\)\n\rprior distribution: \\(p(\\theta)\\)\n\rmodel distribution: \\(p(y|\\theta)\\)\n\rKullback-Leibler divergence: a measure of â€˜discrepancyâ€™ between the model and the true distribution\r\\[KL(\\theta)= E_f\\left[\\log\\left(\\frac{f(y)}{p(y|\\theta)}\\right)\\right]=\\int \\log\\left(\\frac{f(y)}{p(y|\\theta)}\\right)f(y)dy\\]","tags":null,"title":"ç¬¬5ç« ","type":"docs"},{"authors":null,"categories":null,"content":"\rIntroduction to hierarchial models\rMany statistical applications involve multiple parameters (say, \\(\\theta_1,\\dots,\\theta_J\\)) that can be regarded as related or connected in some way by the structure of the problem.\n\rfor the group \\(j\\in 1{:}J\\), we have the observed data \\(y_{ij}\\), \\(i=1,\\dots,n_j\\) from the population distribution with unknown parameter \\(\\theta_j\\)\n\rwe use a prior distribution in which the \\(\\theta_j\\)â€™s are viewed as a sample from a common population distribution, say \\(p(\\theta|\\phi)\\), where \\(\\phi\\) is known as hyperparameters. Assume that \\(\\theta_j\\) are iid, i.e.,\r\\[p(\\theta|\\phi)=\\prod_{j=1}^Jp(\\theta_j|\\phi)\\]\n\r\r\rHierarchical model for Rats experiment\rThe experiment is used to estimate the probability \\(\\theta\\) of tumor in a population of female laboratory rats of type â€˜F344â€™ that receive a zero dose of the drug. The data show that 4 out of 14 rats developed a kind of tumor.\n\rassume a binomial model for the number of tumors\rselect a prior from the conjugate family, i.e., \\(\\theta\\sim Beta(\\alpha,\\beta)\\)\rthe posterior is therefore \\(Beta(\\alpha+1,\\beta+10)\\)\r\rThe question is how to determine the hyperparameters \\(\\phi=(\\alpha,\\beta)\\)\n\rhistorical data are available on previous experiments on similar groups of rats: in the jth historical experiments, let the number of rats with tumors be \\(y_j\\) and the total number of rats be \\(n_j\\), the parameters for the populations are \\(\\theta_j\\), \\(j=1,\\dots,70\\).\rfor current experiment, let \\(y_{71},n_{71},\\theta_{71}\\) be the associated notations.\r\r\rHistorical data for the 70 historical experiments\r## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2\r## [24] 2 2 2 2 2 2 2 2 1 5 2 5 3 2 7 7 3 3 2 9 10 4 4\r## [47] 4 4 4 4 4 10 4 4 4 5 11 12 5 5 6 5 6 6 6 6 16 15 15\r## [70] 9 4\r## [1] 20 20 20 20 20 20 20 19 19 19 19 18 18 17 20 20 20 20 19 19 18 18 25\r## [24] 24 23 20 20 20 20 20 20 10 49 19 46 27 17 49 47 20 20 13 48 50 20 20\r## [47] 20 20 20 20 20 48 19 19 19 22 46 49 20 20 23 19 22 20 20 20 52 46 47\r## [70] 24 14\r\rViewed as separate models using uniform priors\r\rViewed as a pooled model using uniform prior\r\rUsing the historical data to estimate the hyperparameters\r\rthe sample mean and standard deviation of the 70 values \\(y_i/n_i\\) are 0.136 and 0.103\n\rlet \\(E[\\theta]=\\frac{\\alpha}{\\alpha+\\beta}=0.136\\) and \\(Var[\\theta]=\\frac{E[\\theta](1-E[\\theta])}{\\alpha+\\beta+1}=0.103\\)\n\r\\(\\hat{\\alpha}=1.4,\\ \\hat{\\beta}=8.6\\)\n\rfor the current exeriment, the posterior for \\(\\theta\\) is \\(Beta(5.4,18.6)\\), posterior mean is \\(0.223\\), standard deviation is 0.083.\n\r\rThere are several logical and practical problems with the approach of directly estimating a prior distribution from existing data:\n\rthe data will be used twice for inference about the first 70 experiments â€“ overestimate our precision\n\rthe point estimate for \\(\\alpha,\\beta\\) seems arbitrary that necessarily ignores some posterior uncertainty\n\rthis is not the logic of Bayesian inference\n\r\r\rThe full Bayesian treatment of the hierarchical model\rSuppose the hyperparameters \\(\\phi\\) has its own prior distribution \\(p(\\phi)\\), which is called hyperprior distribution. The appropriate Bayesian posterior distribution is of the vector \\((\\phi,\\theta)\\).\n\rthe joint prior distribution is\r\\[p(\\phi,\\theta)=p(\\phi)p(\\theta|\\phi)\\]\n\rthe joint posterior distribution is\r\\[p(\\phi,\\theta|y)\\propto p(\\phi,\\theta)p(y|\\phi,\\theta)=p(\\phi)p(\\theta|\\phi)p(y|\\theta)\\]\n\r\rPreviously, we assumed \\(\\phi\\) was known, which is unrealistic; now we include the uncertainty in \\(\\phi\\) in the model.\n\rFully Bayesian analysis of conjugate hierarchical models\rConsider the setting in which \\(p(\\theta|\\phi)\\) is conjugate to the likelihood \\(p(y|\\theta)\\). For this case, it is easy to determine analytically \\[p(\\theta|\\phi,y)\\propto p(\\theta|\\phi)p(y|\\theta)\\]\n\rthe joint posterior density:\r\\[p(\\phi,\\theta|y)\\propto p(\\phi)p(\\theta|\\phi)p(y|\\theta)\\]\rthe marginal posterior density \\(p(\\phi|y)\\) can be computed via\r\\[p(\\phi|y)=\\int p(\\phi,\\theta|y)d \\theta\\]\r\r\\[\\text{or }p(\\phi|y)=\\frac{p(\\phi,\\theta|y)}{p(\\theta|\\phi,y)}\\]\n\rApplication to the model for rat tumors\rThe binomial model:\r\\[y_j\\sim Bin(n_j,\\theta_j),\\ j=1,\\dots,J=71\\]\nThe parameters \\(\\theta_j\\) are assumed to be independent samples from a beta distribution:\r\\[\\theta_j\\sim Beta(\\alpha,\\beta)\\]\nThe joint posterior density is\r\\[p(\\theta,\\alpha,\\beta|y)\\propto p(\\alpha,\\beta)p(\\theta|\\alpha,\\beta)p(y|\\theta)\\]\r\\[\\propto p(\\alpha,\\beta)\\prod_{j=1}^J\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta_j^{\\alpha-1}(1-\\theta_j)^{\\beta-1}\\prod_{j=1}^J\\theta_j^{y_j}(1-\\theta_j)^{n_j-y_j}\\]\n\\[p(\\theta|\\alpha,\\beta,y)=\\prod_{j=1}^J\\frac{\\Gamma(\\alpha+\\beta+n_j)}{\\Gamma(\\alpha+y_j)\\Gamma(\\beta+n_j-y_j)}\\theta_j^{\\alpha+y_i-1}(1-\\theta_j)^{\\beta+n_j-y_j-1}\\]\n\rApplication to the model for rat tumors\rThe marginal posterior density:\r\\[p(\\alpha,\\beta|y)\\propto p(\\alpha,\\beta)\\prod_{j=1}^J\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(\\alpha+y_j)\\Gamma(\\beta+n_j-y_j)}{\\Gamma(\\alpha+\\beta+n_j)}\\]\nChoosing a noninformative hyperprior distribution:\r\\[p(\\alpha,\\beta)\\propto (\\alpha+\\beta)^{-5/2}\\]\nThis implies that \\((\\alpha/(\\alpha+\\beta),(\\alpha+\\beta)^{-1/2})\\) is uniformly distributed.\n\rthe prior mean is \\(\\alpha/(\\alpha+\\beta)\\)\rthe prior variance is approximately \\((\\alpha+\\beta)^{-1}\\)\r\r\rPlot of the marginal posterior density\r\rCompare the separate model and hierarchical model\r\rHierarchical model based on normal distribution\rConsider \\(J\\) independent experiments, with experiment \\(j\\) estimating \\(\\theta_j\\) form \\(n_j\\) independent distributed data points \\(y_{ij}\\), each with known error variance \\(\\sigma^2\\), that is\r\\[y_{ij}|\\theta_j\\stackrel{iid}{\\sim} N(\\theta_j,\\sigma^2), \\text{ for }i=1,\\dots,n_j;\\ j=1,\\dots,J\\]\n\rdenote the sample mean of each group \\(j\\) as\r\\[\\bar{y}_{\\cdot j}=\\frac 1{n_j}\\sum_{i=1}^{n_j}y_{ij}\\]\n\rlet \\(\\sigma^2_j=\\sigma^2/n_j\\)\n\rthe likelihood for each \\(\\theta_j\\):\r\\[\\bar{y}_{\\cdot j}|\\theta_j\\sim N(\\theta_j,\\sigma_j^2)\\]\n\r\r\rHierarchical model based on normal distribution\r\rfor the convenience of conjugacy, assume the paramerters \\(\\theta_j\\) are drawn from a normal distribution with hyperparameters \\(\\mu,\\tau\\):\r\\[p(\\theta_1,\\dots,\\theta_J|\\mu,\\tau)=\\prod_{j=1}^J N(\\theta_j|\\mu,\\tau^2)\\]\n\rassign noninformative uniform hyperprior density to \\(\\mu\\) given \\(\\tau\\):\r\\[p(\\mu,\\tau)=p(\\mu|\\tau)p(\\tau)\\propto p(\\tau)\\]\n\rprior distribution for \\(\\tau\\): \\(p(\\tau)\\propto 1\\)\n\rthe joint posterior density is\r\\[p(\\theta,\\mu,\\tau|y)\\propto p(\\mu,\\tau)p(\\theta|\\mu,\\tau)p(y|\\theta)\\]\n\r\r\\[p(\\theta,\\mu,\\tau|y)\\propto p(\\mu,\\tau)\\prod_{j=1}^J N(\\theta_j|\\mu,\\tau^2)\\prod_{j=1}^JN(\\bar{y}_{\\cdot j}|\\theta_j,\\sigma_j^2)\\]\n\rHierarchical model based on normal distribution\r\rthe conditional posterior distirbution:\r\\[\\theta_j|\\mu,\\tau,y\\sim N(\\hat{\\theta}_j,V_j)\\]\r\rwhere\r\\[\\hat{\\theta}_j=\\frac{\\frac 1{\\sigma^2}\\bar{y}_{\\cdot j}+\\frac 1{\\tau^2}\\mu}{\\frac 1{\\sigma^2}+\\frac 1{\\tau^2}},\\ V_j=\\frac{1}{\\frac 1{\\sigma^2}+\\frac 1{\\tau^2}}\\]\n\rthe marginal posterior density can be computed in a simple way\r\\[p(\\mu,\\tau|y)\\propto p(\\mu,\\tau)p(y|\\mu,\\tau)\\]\n\r\\(\\bar{y}_{\\cdot j}|\\mu,\\tau\\sim N(\\mu,\\sigma_j^2+\\tau^2)\\)\n\r\r\\[p(\\mu,\\tau|y)\\propto p(\\mu,\\tau)\\prod_{j=1}^JN(\\bar{y}_{\\cdot j}|\\mu,\\sigma_j^2+\\tau^2)\\]\n\rHierarchical model based on normal distribution\r\rposterior distribution of \\(\\mu\\) given \\(\\tau\\)\r\\[\\mu|\\tau,y\\sim N(\\hat{\\mu},V_{\\mu})\\]\r\rwhere\r\\[\\hat{\\mu}=\\frac{\\sum_{j=1}^J \\frac 1{\\sigma_j^2+\\tau^2}\\bar{y}_{\\cdot j}}{\\sum_{j=1}^J \\frac 1{\\sigma_j^2+\\tau^2}},\\ V_{\\mu}^{-1}=\\sum_{j=1}^J \\frac 1{\\sigma_j^2+\\tau^2}\\]\n\rposterior distribution of \\(\\tau\\):\r\\[p(\\tau|y)=\\frac{p(\\mu,\\tau|y)}{p(\\mu|\\tau,y)}\\propto \\frac{p(\\tau)\\prod_{j=1}^JN(\\bar{y}_{\\cdot j}|\\mu,\\sigma_j^2+\\tau^2)}{N(\\mu|\\hat{\\mu},V_{\\mu})}\\]\r\r\\[p(\\tau|y)\\propto p(\\tau)V_{\\mu}^{1/2}\\prod_{j=1}^J(\\sigma_j^2+\\tau^2)^{-1/2}\\exp\\left(-\\frac{(\\bar{y}_{\\cdot j}-\\hat{\\mu})^2}{2(\\sigma_j^2+\\tau^2)}\\right)\\]\n\rExample: parallel experiments in eight schools\rA study was performanced for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Seperate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Verbal).\n\r\r\r\rSchool\rEstiamted treatment effect \\(y_j\\)\rStandard error of effect estimate \\(\\sigma_j\\)\r\r\r\rA\r28\r15\r\rB\r8\r10\r\rC\r-3\r16\r\rD\r7\r11\r\rE\r-1\r9\r\rF\r1\r11\r\rG\r18\r10\r\rH\r12\r18\r\r\r\r\rComparisons\r\rPlot the posterior summaries\r\r","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"bb84d9721a76c89184e5afa53efc4280","permalink":"/en/courses/bayes/chap6/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/bayes/chap6/","section":"courses","summary":"Introduction to hierarchial models\rMany statistical applications involve multiple parameters (say, \\(\\theta_1,\\dots,\\theta_J\\)) that can be regarded as related or connected in some way by the structure of the problem.","tags":null,"title":"ç¬¬6ç« ","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/en/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":null,"content":"","date":1599073200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599073200,"objectID":"ac18e1fcd58a87c1c6a90734f11ba84f","permalink":"/en/talk/bayes_comp/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/en/talk/bayes_comp/","section":"talk","summary":"è´å¶æ–¯è®¡ç®—åŠå…¶æŒ‘æˆ˜","tags":[],"title":"è´å¶æ–¯è®¡ç®—åŠå…¶æŒ‘æˆ˜","type":"talk"},{"authors":["Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"2c6f811bd985d01771e98a6eb925817f","permalink":"/en/publication/2020mcom/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/en/publication/2020mcom/","section":"publication","summary":"Quantiles and expected shortfalls are usually used to measure risks of stochastic systems, which are often estimated by Monte Carlo methods. This paper focuses on the use of the quasi-Monte Carlo (QMC) method, whose convergence rate is asymptotically better than Monte Carlo in the numerical integration. We first prove the convergence of QMC-based quantile estimates under very mild conditions, and then establish a deterministic error bound of $ O(N^{-1/d})$ for the quantile estimates, where $ d$ is the dimension of the QMC point sets used in the simulation and $ N$ is the sample size. Under certain conditions, we show that the mean squared error (MSE) of the randomized QMC estimate for expected shortfall is $ o(N^{-1})$. Moreover, under stronger conditions the MSE can be improved to $ O(N^{-1-1/(2d-1)+\\epsilon })$ for arbitrarily small $ \\epsilon 0$.","tags":["QMC"],"title":"Convergence analysis of quasi-Monte Carlo sampling for quantile and expected shortfall","type":"publication"},{"authors":["Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"3f45d91e380ae8564effac47c54d6a46","permalink":"/en/publication/2020ec/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/en/publication/2020ec/","section":"publication","summary":"Quasi-Monte Carlo (QMC) method is a useful numerical tool for pricing and hedging of complex financial derivatives. These problems are usually of high dimensionality and discontinuities. The two factors may significantly deteriorate the performance of the QMC method. This paper develops an integrated method that overcomes the challenges of the high dimensionality and discontinuities concurrently. For this purpose, a smoothing method is proposed to remove the discontinuities for some typical functions arising from financial engineering. To make the smoothing method applicable for more general functions, a new path generation method is designed for simulating the paths of the underlying assets such that the resulting function has the required form. The new path generation method has an additional power to reduce the effective dimension of the target function. Our proposed method caters for a large variety of model specifications, including the Blackâ€“Scholes, exponential normal inverse Gaussian LÃ©vy, and Heston models. Numerical experiments dealing with these models show that in the QMC setting the proposed smoothing method in combination with the new path generation method can lead to a dramatic variance reduction for pricing exotic options with discontinuous payoffs and for calculating optionsâ€™ Greeks. The investigation on the effective dimension and the related characteristics explains the significant enhancement of the combined procedure.","tags":["QMC"],"title":"An Integrated Quasi-Monte Carlo Method for Handling High Dimensional Problems with Discontinuities in Financial Engineering","type":"publication"},{"authors":["Zhijian He"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"c3cb8184491fd93b1926b7fc2f624ec1","permalink":"/en/publication/2019sinum/","publishdate":"2019-04-25T00:00:00Z","relpermalink":"/en/publication/2019sinum/","section":"publication","summary":"This paper studies the rate of convergence for conditional quasi--Monte Carlo (QMC), which is a counterpart of conditional Monte Carlo. We focus on discontinuous integrands defined on the whole of $\\mathbb{R}^d$, which can be unbounded. Under suitable conditions, we show that conditional QMC not only has the smoothing effect (up to infinitely times differentiable) but also can bring orders of magnitude reduction in integration error compared to plain QMC. Particularly, for some typical problems in options pricing and Greeks estimation, conditional randomized QMC that uses $n$ samples yields a mean error of $O(n^{-1+\\epsilon})$ for arbitrarily small $\\epsilon0$. As a byproduct, we find that this rate also applies to randomized QMC integration with all terms of the analysis of variance decomposition of the discontinuous integrand, except the one of highest order.","tags":["QMC"],"title":"On the Error Rate of Conditional Quasi-Monte Carlo for Discontinuous Functions","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\n Click to view the spoiler  You found me!    Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it ğŸ™Œ ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/en/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/en/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Fei Xie","Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1555372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555372800,"objectID":"1411e3c16b556cfacc61385102d6518a","permalink":"/en/publication/2019ejor/","publishdate":"2019-04-16T00:00:00Z","relpermalink":"/en/publication/2019ejor/","section":"publication","summary":"Handling discontinuities in financial engineering is a challenging task when using quasi-Monte Carlo (QMC) method. This paper develops a so-called sequential importance sampling (SIS) method to remove multiple discontinuity structures sequentially for pricing discrete barrier options. The SIS method is a smoothing approach based on importance sampling, which yields an unbiased estimate with reduced variance. However, removing discontinuities still may not recover the superiority of QMC when the dimensionality of the problem is high. In order to handle the impact of high dimensionality on QMC, one promising strategy is to reduce the effective dimension of the problem. To this end, we develop a good path generation method with the smoothed estimator under the Blackâ€“Scholes model and models based on subordinated Brownian motion (e.g., Variance Gamma process). We find that the order of path generation influences the variance of the SIS estimator, and show how to choose optimally the first generation step. As confirmed by numerical experiments, the SIS method combined with a carefully chosen path generation method can significantly reduce the variance with improved rate of convergence. In addition, we show that the effective dimension is greatly reduced by the combined method, explaining the superiority of the proposed procedure from another perspective. The SIS method is also applicable for general models (with the Euler discretization). The smoothing effect of the SIS method facilitates the use of general dimension reduction techniques in reclaiming the efficiency of QMC.","tags":["QMC","Finance"],"title":"An importance sampling-based smoothing approach for quasi-Monte Carlo simulation of discrete barrier options","type":"publication"},{"authors":["Zhijian He","Lingjiong Zhu"],"categories":null,"content":"","date":1553040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553040000,"objectID":"8826caf8fdc1dc62d05b3fd52094d933","permalink":"/en/publication/2019cs/","publishdate":"2019-03-20T00:00:00Z","relpermalink":"/en/publication/2019cs/","section":"publication","summary":"Recently, He and Owen (J R Stat Soc Ser B 78(4):917â€“931, 2016) proposed the use of Hilbertâ€™s space filling curve (HSFC) in numerical integration as a way of reducing the dimension from d1 to d=1. This paper studies the asymptotic normality of the HSFC-based estimate when using one-dimensional stratification inputs. In particular, we are interested in using scrambled van der Corput sequence in any base bâ‰¥2 with sample sizes of the form n=bm, for which the sampling scheme is extensible in the sense of multiplying the sample size by a factor of b. We show that the estimate has an asymptotic normal distribution for functions in C1([0,1]d), excluding the trivial case of constant functions. The asymptotic normality also holds for discontinuous functions under mild conditions. Previously, it was only known that scrambled (0, m, d)-net quadratures enjoy the asymptotic normality for smooth enough functions, whose mixed partial gradients satisfy a HÃ¶lder condition. As a by-product, we find lower bounds for the variance of the HSFC-based estimate. Particularly, for non-trivial functions in C1([0,1]d), the lower bound is of order nâˆ’1âˆ’2/d, which matches the rate of the upper bound established in He and Owen (2016).","tags":["QMC"],"title":"Asymptotic normality of extensible grid sampling","type":"publication"},{"authors":["Zhijian He"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/en/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/en/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/en/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/en/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Zhijian He"],"categories":null,"content":"","date":1518998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518998400,"objectID":"8898cf540742f4dd21b46d6ff67227f3","permalink":"/en/publication/2018mcom/","publishdate":"2018-02-19T00:00:00Z","relpermalink":"/en/publication/2018mcom/","section":"publication","summary":"This paper studies randomized quasi-Monte Carlo (QMC) sampling for discontinuous integrands having singularities along the boundary of the unit cube $ [0,1]^d$. Both discontinuities and singularities are extremely common in the pricing and hedging of financial derivatives and have a tremendous impact on the accuracy of QMC. It was previously known that the root mean square error of randomized QMC is only $ o(n^{1/2})$ for discontinuous functions with singularities. We find that under some mild conditions, randomized QMC yields an expected error of $ O(n^{-1/2-1/(4d-2)+\\epsilon })$ for arbitrarily small $ \\epsilon 0$. Moreover, one can get a better rate if the boundary of discontinuities is parallel to some coordinate axes. As a by-product, we find that the expected error rate attains $ O(n^{-1+\\epsilon })$ if the discontinuities are QMC-friendly, in the sense that all the discontinuity boundaries are parallel to coordinate axes. The results can be used to assess the QMC accuracy for some typical problems from financial engineering.","tags":["QMC"],"title":"Quasi-Monte Carlo for Discontinuous Integrands with Singularities along the Boundary of the Unit Cube","type":"publication"},{"authors":["Chengfeng Weng","Xiaoqun Wang","Zhijian He"],"categories":null,"content":"","date":1490054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490054400,"objectID":"8516606bccfff907f1e9ecd5dcf31ff8","permalink":"/en/publication/2017sisc/","publishdate":"2017-03-21T00:00:00Z","relpermalink":"/en/publication/2017sisc/","section":"publication","summary":"Discontinuities and high dimensionality are common in the problems of pricing and hedging of derivative securities. Both factors have a tremendous impact on the accuracy of the quasi--Monte Carlo (QMC) method. An ideal approach to improve the QMC method is to transform the functions to make them smoother and having smaller effective dimension. This paper develops a two-step procedure to tackle the challenging problems of both the discontinuity and the high dimensionality concurrently. In the first step, we adopt the smoothing method to remove the discontinuities of the payoff function, improving the smoothness. In the second step, we propose a general dimension reduction method (called the CQR method) to reduce the effective dimension such that the better quality of QMC points in their initial dimensions can be fully exploited. The CQR method is based on the combination of the $k$-means clustering algorithm and the QR decomposition. The $k$-means clustering algorithm, a classical algorithm of machine learning, is used to find some representative linear structures inherent in the function, which are used to construct a matching function of the smoothed function. The matching function serves as an approximation of the smoothed function but has a simpler form, and it is used to find the required transformation. Extensive numerical experiments on option pricing and Greeks estimation demonstrate that the combination of the smoothing method and dimension reduction in QMC achieves substantially larger variance reduction even in high dimension than dealing with either discontinuities or high dimensionality single sidedly.","tags":["QMC","Finance"],"title":"Efficient Computation of Option Prices and Greeks by Quasi-Monte Carlo Method with Smoothing and Dimension reduction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/en/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/en/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Zhijian He","å³æ©é”"],"categories":["Demo","æ•™ç¨‹"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n ğŸ‘‰ Get Started ğŸ“š View the documentation ğŸ’¬ Ask a question on the forum ğŸ‘¥ Chat with the community ğŸ¦ Twitter: @source_themes @GeorgeCushen #MadeWithAcademic ğŸ’¡ Request a feature or report a bug â¬†ï¸ Updating? View the Update Guide and Release Notes â¤ï¸ Support development of Academic:  â˜•ï¸ Donate a coffee ğŸ’µ Become a backer on Patreon ğŸ–¼ï¸ Decorate your laptop or journal with an Academic sticker ğŸ‘• Wear the T-shirt ğŸ‘©â€ğŸ’» Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, ä¸­æ–‡, and PortuguÃªs Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/en/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/en/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","å¼€æº"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Chengfeng Weng","Xiaoqun Wang","Zhijian He"],"categories":null,"content":"","date":1458950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458950400,"objectID":"40c0e92f08cc5b0dcf5f387ddabe9701","permalink":"/en/publication/2016ejor/","publishdate":"2016-03-26T00:00:00Z","relpermalink":"/en/publication/2016ejor/","section":"publication","summary":"Discontinuities are common in the pricing of financial derivatives and have a tremendous impact on the accuracy of quasi-Monte Carlo (QMC) method. While if the discontinuities are parallel to the axes, good efficiency of the QMC method can still be expected. By realigning the discontinuities to be axes-parallel, [Wang \\\u0026 Tan, 2013] succeeded in recovering the high efficiency of the QMC method for a special class of functions. Motivated by this work, we propose an auto-realignment method to deal with more general discontinuous functions. The k-means clustering algorithm, a classical algorithm of machine learning, is used to select the most representative normal vectors of the discontinuity surface. By applying this new method, the discontinuities of the resulting function are realigned to be friendly for the QMC method. Numerical experiments demonstrate that the proposed method significantly improves the performance of the QMC method.","tags":["QMC","Finance"],"title":"An Auto-Realignment Method in Quasi-Monte Carlo for Pricing Financial Derivatives with Jump Structures","type":"publication"},{"authors":["Zhijian He","Art Owen"],"categories":null,"content":"","date":1458259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458259200,"objectID":"b5099d86c503769215544a71b997eec6","permalink":"/en/publication/2016jrssb/","publishdate":"2016-03-18T00:00:00Z","relpermalink":"/en/publication/2016jrssb/","section":"publication","summary":"We study the properties of points in $[0,1]^d$ generated by applying Hilbert's space-filling curve to uniformly distributed points in $[0,1]$. For deterministic sampling we obtain a discrepancy of $O(n^{-1/d})$ for $d\\ge2$. For random stratified sampling, and scrambled van der Corput points, we get a mean squared error of $O(n^{-1-2/d})$ for integration of  Lipschitz continuous integrands, when $d\\ge3$. These rates are the same as one gets by sampling on $d$ dimensional grids and they show a deterioration with increasing $d$.  The rate for Lipschitz functions is however best possible at that level of smoothness and is better than plain IID sampling. Unlike grids, space-filling curve sampling provides points at any desired sample size, and the van der Corput version is extensible in $n$. We also introduce a class of piecewise Lipschitz functions whose discontinuities are in rectifiable sets described via Minkowski content. Although these functions may have infinite variation in the sense of Hardy and Krause, they  can be integrated with a mean squared error of $O(n^{-1-1/d})$. It was previously known only that the rate was $o(n^{-1})$. Other space-filling curves, such as those due to Sierpinski and Peano, also attain these rates, while upper bounds for the Lebesgue curve are somewhat worse, as if the dimension were $\\log_2(3)$ times as high.","tags":["QMC"],"title":"Extensible Grids -- Uniform Sampling on a Space-Filling Curve","type":"publication"},{"authors":["Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1446076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446076800,"objectID":"3f4e7b31652e546b341fcbe7ace28d41","permalink":"/en/publication/2015sinum/","publishdate":"2015-10-29T00:00:00Z","relpermalink":"/en/publication/2015sinum/","section":"publication","summary":"This paper studies the convergence rate of randomized quasi--Monte Carlo (RQMC) for discontinuous functions, which are often of infinite variation in the sense of Hardy and Krause. It was previously known that the root mean square error (RMSE) of RQMC is only $o(n^{-1/2})$ for discontinuous functions. For certain discontinuous functions in $d$ dimensions, we prove that the RMSE of RQMC is $O(n^{-1/2-1/(4d-2)+\\epsilon})$ for any $\\epsilon0$ and arbitrary $n$. If some discontinuity boundaries are parallel to some coordinate axes, the rate can be improved to $O(n^{-1/2-1/(4d_u-2)+\\epsilon})$, where $d_u$ denotes the so-called irregular dimension, that is, the number of axes which are not parallel to the discontinuity boundaries. Moreover, this paper shows that the RMSE is $O(n^{-1/2-1/(2d)})$ for certain indicator functions.","tags":["QMC"],"title":"On the Convergence Rate of Randomized Quasi-Monte Carlo for Discontinuous Functions","type":"publication"},{"authors":null,"categories":["R"],"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/en/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/en/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1395100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395100800,"objectID":"d7deb53560592c606a36a761fa0ef360","permalink":"/en/publication/2014sisc/","publishdate":"2014-03-18T00:00:00Z","relpermalink":"/en/publication/2014sisc/","section":"publication","summary":"Quasi-Monte Carlo (QMC) method is a powerful numerical tool for pricing complex derivative securities, whose accuracy is affected by the smoothness of the integrands. The payoff functions of many financial derivatives involve two types of non-smooth factors --- an indicator function (called jump structure) and a positive part of a smooth function (called kink structure). This paper develops a good path generation method (PGM) for recovering the superiority of QMC method on problems involving multiple such structures. This is achieved by realigning these structures such that the associated non-smooth surfaces are parallel to as many coordinate axes as possible. The proposed method has the advantage of addressing different structures according to their importance. We also offer a systematic measurement of different structures for quantifying and then ranking their importance. Numerical experiments demonstrate that the proposed method is more efficient than traditional PGMs for pricing exotic options, such as straddle Asian options, digital options and barrier options. The numerical results confirm that both the jumps and kinks have tremendous impacts on the performance of QMC method.","tags":["QMC","Finance"],"title":"Good Path Generation Methods in Quasi-Monte Carlo for Pricing Financial Derivatives","type":"publication"}]