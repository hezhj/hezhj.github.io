<!DOCTYPE html><html lang="zh-Hans" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="何志坚">

  
  
  
    
  
  <meta name="description" content="Markov chainsConsider the discrete chain:\[P(X_i\in A|X_0=x_0,\dots,X_{i-1}=x_{i-1})=P(X_i\in A|X_{i-1}=x_{i-1})\]
\(X_i\in\Omega=\{\omega_1,\dots,\omega_M\}\)
Transition distribution:\[P(X_i=y|X_{i-1}=x)=T_i(y|x)\]
Distribution of this chain is now determined by \(p_0(x)=P(X_0=x)\) and \(T_i(y|x)\)
Homogeneous chain:\[P(X_i=y|X_{i-1}=x)=P(X_1=y|X_0=x)=T(y|x)\]">

  
  <link rel="alternate" hreflang="en" href="/en/courses/bayes/chap11/">
  
  <link rel="alternate" hreflang="zh-Hans" href="/courses/bayes/chap11/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/courses/bayes/chap11/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="何志坚">
  <meta property="og:url" content="/courses/bayes/chap11/">
  <meta property="og:title" content="第11章 | 何志坚">
  <meta property="og:description" content="Markov chainsConsider the discrete chain:\[P(X_i\in A|X_0=x_0,\dots,X_{i-1}=x_{i-1})=P(X_i\in A|X_{i-1}=x_{i-1})\]
\(X_i\in\Omega=\{\omega_1,\dots,\omega_M\}\)
Transition distribution:\[P(X_i=y|X_{i-1}=x)=T_i(y|x)\]
Distribution of this chain is now determined by \(p_0(x)=P(X_0=x)\) and \(T_i(y|x)\)
Homogeneous chain:\[P(X_i=y|X_{i-1}=x)=P(X_1=y|X_0=x)=T(y|x)\]"><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="zh-Hans">
  
    
      <meta property="article:published_time" content="2019-05-05T00:00:00&#43;01:00">
    
    <meta property="article:modified_time" content="2019-05-05T00:00:00&#43;01:00">
  

  



  


  


  





  <title>第11章 | 何志坚</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>搜索</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="搜索..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">何志坚</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="切换导航">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">何志坚</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>博客</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/project/"><span>项目</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>文章</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/courses/"><span>课程</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>联系方式</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="搜索"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      <li class="nav-item dropdown i18n-dropdown">
        <a href="#" class="nav-link " data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-globe mr-1" aria-hidden="true"></i></a>
        <div class="dropdown-menu">
          <div class="dropdown-item dropdown-item-active">
            <span>中文 (简体)</span>
          </div>
          
          <a class="dropdown-item" href="/en/courses/bayes/chap11/">
            <span>English</span>
          </a>
          
        </div>
      </li>
      

    </ul>

  </div>
</nav>



  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





  
    
  




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="搜索..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  

  
  
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/courses/bayes/">课程简介</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/courses/bayes/chap1/">主要内容</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/courses/bayes/chap1/">第1章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/chap2/">第2章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/chap3/">第3章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/chap4/">第4章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/chap5/">第5章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/chap6/">第6章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/chap10/">第10章</a>
      </li>
      
      <li class="active">
        <a href="/courses/bayes/chap11/">第11章</a>
      </li>
      
      <li >
        <a href="/courses/bayes/vi/">变分推断</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          <h1>第11章</h1>

          <div class="article-style">
            


<div id="markov-chains" class="section level2">
<h2>Markov chains</h2>
<p>Consider the discrete chain:
<span class="math display">\[P(X_i\in A|X_0=x_0,\dots,X_{i-1}=x_{i-1})=P(X_i\in A|X_{i-1}=x_{i-1})\]</span></p>
<ul>
<li><p><span class="math inline">\(X_i\in\Omega=\{\omega_1,\dots,\omega_M\}\)</span></p></li>
<li><p>Transition distribution:
<span class="math display">\[P(X_i=y|X_{i-1}=x)=T_i(y|x)\]</span></p></li>
<li><p>Distribution of this chain is now determined by <span class="math inline">\(p_0(x)=P(X_0=x)\)</span> and <span class="math inline">\(T_i(y|x)\)</span></p></li>
<li><p>Homogeneous chain:
<span class="math display">\[P(X_i=y|X_{i-1}=x)=P(X_1=y|X_0=x)=T(y|x)\]</span></p></li>
</ul>
</div>
<div id="where-does-this-chain-go" class="section level2">
<h2>Where does this chain go?</h2>
<p><span class="math display">\[p_{i}(\omega_k) = P(X_i=\omega_k) = \sum_{j=1}^Mp_{i-1}(\omega_j)T(\omega_k|\omega_j)\]</span></p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{p}_i=(p_i(\omega_1),\dots,p_i(\omega_M))\)</span></p></li>
<li><p><span class="math inline">\(P\)</span> is the transition matrix with entries <span class="math inline">\(P_{ij}=P(\omega_j|\omega_i)\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{p}_i = \boldsymbol{p}_{i-1} P\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{p}_i = \boldsymbol{p}_{0} P^n\)</span></p></li>
</ul>
</div>
<div id="example-a-portion-of-the-montréal-métro" class="section level2">
<h2>Example: A portion of the Montréal métro</h2>
<p><img src="/media/img/metro.png" width="65%" /></p>
</div>
<div id="transition-matrix" class="section level2">
<h2>Transition matrix</h2>
<p><img src="/media/img/transition.png" width="65%" /></p>
</div>
<div id="after-100-steps" class="section level2">
<h2>After 100 steps</h2>
<p><img src="/media/img/100step.png" width="65%" /></p>
<ul>
<li><p>No matter where you start <span class="math inline">\(p_{100}(\text{Berri})\doteq 0.31\)</span></p></li>
<li><p>These are almost IID from the <strong>stationary distribution</strong>.</p></li>
</ul>
</div>
<div id="stationary-distribution" class="section level2">
<h2>Stationary distribution</h2>
<p><span class="math display">\[\pi = \pi P \text{ so } \pi^\top = P^\top\pi^\top\]</span></p>
<ul>
<li><span class="math inline">\(\pi^{\top}\)</span> is an eigenvector of <span class="math inline">\(P^\top\)</span> with eigenvalue 1</li>
</ul>
</div>
<div id="law-of-large-numbers" class="section level2">
<h2>Law of large numbers</h2>
<p>Let <span class="math inline">\(X_i\)</span> be a time-homogenous Markov chain on a finite set <span class="math inline">\(\Omega\)</span></p>
<p><strong>Theorem</strong>: If <span class="math inline">\(P\)</span> is <strong>irreducible</strong>, then</p>
<p><span class="math display">\[P_{\omega_0}\left(\lim_{n\to\infty}\frac 1n\sum_{i=1}f(X_i)=\sum_{\omega\in\Omega}\pi(\omega)f(\omega)\right)=1\]</span></p>
<ul>
<li>Irreducibility: <span class="math inline">\(P_x(\tau_y&lt;\infty)&gt;0\)</span> for any <span class="math inline">\(x,y\in \Omega\)</span>, where <span class="math inline">\(\tau_y\)</span> is the first time <span class="math inline">\(y\)</span> is visited, i.e., <span class="math display">\[\tau_y:=\inf\{i\ge 1:X_i=y,X_0=x\}\]</span></li>
</ul>
</div>
<div id="what-we-will-do" class="section level2">
<h2>What we will do</h2>
<ul>
<li><p>Given <span class="math inline">\(\pi\)</span> we will find a transition matrix <span class="math inline">\(P\)</span> with <span class="math inline">\(\pi P=\pi\)</span></p></li>
<li><p>Then sample <span class="math inline">\(X_1,\dots,X_n\)</span> via <span class="math inline">\(P\)</span></p></li>
</ul>
<p>Here is what could go wrong:</p>
<ul>
<li><p>It might take a long time before <span class="math inline">\(\boldsymbol{p}_n\approx \pi\)</span> (slow convergence)</p></li>
<li><p><span class="math inline">\(X_n\)</span> might get stuck for a long time (slow mixing)</p></li>
</ul>
<p>The goal is to find the good one for <span class="math inline">\(P\)</span>.</p>
</div>
<div id="detailed-balance" class="section level2">
<h2>Detailed balance</h2>
<ul>
<li><p>Stationarity balances flow into <span class="math inline">\(y\)</span> with flow out of <span class="math inline">\(y\)</span>
<span class="math display">\[\sum_{x\in\Omega} \pi(x)P(y|x)=\pi(y) = \sum_{x\in\Omega}\pi(y)P(x|y)\]</span>
NB: <span class="math inline">\(\pi = \pi P\)</span></p></li>
<li><p>Detailed balance is stronger:
<span class="math display">\[\pi(x)P(y|x)=\pi(y)P(x|y),\forall x,y\in\Omega\]</span></p></li>
<li><p>detailed balance <span class="math inline">\(\rightarrow\)</span> balance</p></li>
</ul>
</div>
<div id="the-road-map" class="section level2">
<h2>The road map</h2>
<p>The goal is to build a Markov chain with a unique stationary distribution which equals the targe distribution.</p>
<ol style="list-style-type: decimal">
<li><p>build a Markov chain with a unique stationary distribution. This holds if the Markov chian is irreducible, aperiodic, and not transient. E.g., random walk has a positive probablility of eventually reaching any state from any other state.</p></li>
<li><p>stationary distribution = the targe distribution (detailed balance transition)</p></li>
</ol>
</div>
<div id="the-metropolis-algorithm" class="section level2">
<h2>The Metropolis algorithm</h2>
<p>The Metropolis algorithm (Metropolis et al. 1953) is an adaptation of a random walk with an acceptance/rejection rule to converge to the specified target distribution.</p>
<ul>
<li><p>target distribution: <span class="math inline">\(p(x)\)</span></p></li>
<li><p>symmetric proposal distribution at time <span class="math inline">\(t\)</span>: <span class="math inline">\(q(y|x)=q(x|y)\)</span></p></li>
</ul>
<p>The algorithm goes below:</p>
<ol style="list-style-type: decimal">
<li>for <span class="math inline">\(t=0\)</span>, draw a starting poing <span class="math inline">\(x_0\sim p_0(x)\)</span></li>
<li>for <span class="math inline">\(t=1,2,\dots\)</span>, sample <span class="math inline">\(y_t\sim q(y_t|x_{t-1})\)</span>, accept <span class="math inline">\(y_t\)</span> as an output of <span class="math inline">\(x_t\)</span> with probability
<span class="math display">\[r(x_{t-1},y_t)=\min\left(\frac{p(y_t)}{p(x_{t-1})},1\right)\]</span>
Otherwise, taking <span class="math inline">\(x_{t}=x_{t-1}\)</span></li>
</ol>
</div>
<div id="the-transition-for-the-metropolis-algorithm" class="section level2">
<h2>The transition for the Metropolis algorithm</h2>
<p>The transition is a mixture of a point and a proposal distribution.</p>
<p><span class="math display">\[T(y|x) = r(x,y)q(y|x)+\left[1-\int r(x,y) q(y|x)dy\right]1\{y=x\}\]</span></p>
<p><span class="math display">\[r(x,y)=\min\left(\frac{p(y)}{p(x)},1\right)\]</span></p>
<p><strong>Detailed balance</strong>: <span class="math inline">\(p(x)T(y|x)=p(y)T(x|y)\)</span></p>
<p>If <span class="math inline">\(x\neq y\)</span>,</p>
<p><span class="math display">\[\frac{p(x)T(y|x)}{p(y)T(x|y)}=\frac{p(x)r(x,y)}{p(y)r(y,x)}\frac{q(y|x)}{q(x|y)}=\frac{p(x)\min(p(y)/p(x),1)}{p(y)\min(p(x)/p(y),1)}=1\]</span></p>
</div>
<div id="random-walk-metropolis" class="section level2">
<h2>Random walk Metropolis</h2>
<ul>
<li>the proposal density:</li>
</ul>
<p><span class="math display">\[y_{t+1}=x_t+ N(0,\sigma^2I_d)\]</span></p>
<p><span class="math display">\[y_{t+1}=x_t+ U[-\sigma,\sigma]^d\]</span></p>
<p>How large a step <span class="math inline">\(\sigma\)</span>?</p>
<ul>
<li><p>Tiny step: large <span class="math inline">\(p(y_{t+1})/p(x_t)\)</span>, high acceptance</p></li>
<li><p>Large step: small <span class="math inline">\(p(y_{t+1})/p(x_t)\)</span>, low acceptance</p></li>
</ul>
<p>We might have wanted high acceptance and large moves. But there’s a tradeoff.</p>
<p><strong>The rule of thumb</strong>: <span class="math inline">\(\sigma=2.38/\sqrt{d}\)</span>, see Gelman, Roberts, Gilks (1996)</p>
</div>
<div id="improvements-on-rwm" class="section level2">
<h2>Improvements on RWM</h2>
<ul>
<li><p>for the target distribution <span class="math inline">\(p\approx N(\mu,\Sigma)\)</span></p></li>
<li><p>the proposal: <span class="math inline">\(y_{t+1}\sim N(x_{t},\lambda\hat{\Sigma})\)</span></p></li>
<li><p>use sample <span class="math inline">\(x_i\)</span> to estimate <span class="math inline">\(\Sigma\)</span></p></li>
<li><p>a tune parameter <span class="math inline">\(\lambda\)</span></p></li>
</ul>
</div>
<div id="example-bivariate-unit-normal-with-normal-proposal" class="section level2">
<h2>Example: Bivariate unit normal with normal proposal</h2>
<p><img src="/media/img/metropolis.png" width="80%" /></p>
<ul>
<li><p>target distribution: bivariate unit normal <span class="math inline">\(N(0,I_2)\)</span></p></li>
<li><p>symmetric proposal distribution: <span class="math inline">\(q(y|x)=N(y|x,0.2^2I_2)\)</span></p></li>
</ul>
</div>
<div id="the-metropolis-hastings-algorithm" class="section level2">
<h2>The Metropolis-Hastings algorithm</h2>
<p>The Metropolis-Hastings (MH) algorithm (Hastings, 1970) generalizes the basic Metropolis algorithm. The proposal needs no longer be symmetric.</p>
<p>The detailed balance:</p>
<p><span class="math display">\[p(x)r(x,y)q(y|x)=p(y)r(y,x)q(x|y)\]</span></p>
<p>How to choose <span class="math inline">\(r(x,y)\)</span> subject to <span class="math inline">\(0\le r(x,y)\le 1\)</span>?</p>
<p>The result:</p>
<p><span class="math display">\[r(x,y)=\min\left(\frac{p(y)q(x|y)}{p(x)q(y|x)},1\right)\]</span></p>
</div>
<div id="the-independent-mh-algorithm" class="section level2">
<h2>The independent MH algorithm</h2>
<ul>
<li>the proposal: <span class="math inline">\(q(y|x)=q(y)\)</span></li>
</ul>
<p><span class="math display">\[r(x,y)=\min\left(\frac{p(y)q(x)}{p(x)q(y)},1\right)\]</span></p>
<ul>
<li><p>Although the proposal variates are iid, the states are not independent.</p></li>
<li><p>What-if <span class="math inline">\(q(x)=p(x)\)</span>?</p></li>
</ul>
</div>
<div id="unnormalized-target-density" class="section level2">
<h2>Unnormalized target density</h2>
<p>The target density <span class="math inline">\(p(x)=C\tilde{p}(x)\)</span>, where <span class="math inline">\(C&gt;0\)</span> is unknown constant.</p>
<p><span class="math display">\[\frac{p(y)q(x|y)}{p(x)q(y|x)} = \frac{C\tilde{p}(y)q(x|y)}{C\tilde{p}(x)q(y|x)}=\frac{\tilde{p}(y)q(x|y)}{\tilde{p}(x)q(y|x)}\]</span></p>
<p>The MH algorithm also works for unnormalized proposal density.</p>
</div>
<div id="estimating-the-expectation" class="section level2">
<h2>Estimating the expectation</h2>
<ul>
<li><p>The law of large numbers supports:
<span class="math display">\[\hat\mu = \frac 1n\sum_{i=1}^n f(X_i)\]</span></p></li>
<li><p>Burn-in (skipping the first <span class="math inline">\(b\)</span> observations, e.g., <span class="math inline">\(b=n/2\)</span>)
<span class="math display">\[\hat\mu_b = \frac 1{n-b}\sum_{i=b+1}^n f(X_i)\]</span></p></li>
<li><p>Thinning (just use every <span class="math inline">\(k\)</span>th observation, <span class="math inline">\(k&gt;1\)</span>)
<span class="math display">\[\hat\mu_k = \frac 1{n/k}\sum_{i=1}^{n/k} f(X_{ki})\]</span></p></li>
</ul>
<p>See <a href="https://doi.org/10.1080/10618600.2017.1336446">Owen (2017)</a></p>
</div>
<div id="the-gibbs-sampler" class="section level2">
<h2>The Gibbs sampler</h2>
<p>Suppose the targe distribution <span class="math inline">\(X=(x_1,\dots,x_d)\sim p(X)\)</span>. Maybe we can sample one <span class="math inline">\(x_j\)</span> at a time, with others fixed, i.e., <span class="math inline">\(x_j|x_{-j}\)</span>, where <span class="math inline">\(x_{-j}=(x_1,\dots,x_{j-1},x_{j+1},\dots,x_d)\)</span>.</p>
<p><strong>Random scan Gibbs</strong>: for <span class="math inline">\(t=1,\dots,n\)</span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(j\sim U\{1,\dots,d\}\)</span></li>
<li><span class="math inline">\(x_{-j}^{(t)}=x_{-j}^{(t-1)}\)</span></li>
<li><span class="math inline">\(x_{j}^{(t)}\sim p(x_j|x_{-j}^{(t)})\)</span></li>
</ol>
<p><strong>Deterministic scan</strong>: <span class="math inline">\(j\)</span> cycles through <span class="math inline">\(1,\dots,d\)</span> repeatedly, i.e., <span class="math display">\[j=1+(t-1) \mod d\]</span></p>
</div>
<div id="one-step-of-gibbs" class="section level2">
<h2>One step of Gibbs</h2>
<p>Is a Metropolis-Hastings that always accepts, i.e., <span class="math inline">\(r(x,y)\equiv 1\)</span>.</p>
<p>Proposal <span class="math inline">\(q\)</span> just changes component <span class="math inline">\(x_j\)</span>: <span class="math inline">\(p(x^{(t)}_j|x_{-j}^{(t-1)})\)</span></p>
<p><strong>Detailed balance</strong>:</p>
<p><span class="math display">\[\frac{p(y)q(x|y)}{p(x)q(y|x)}=\frac{p(y_{-j})p(y_j|y_{-j})p(x_j|y_{-j})}{p(x_{-j})p(x_j|x_{-j})p(y_j|x_{-j})}=1\]</span></p>
<p>NB: <span class="math inline">\(x_{-j}=y_{-j}\)</span></p>
</div>
<div id="example-bivariate-normal-distribution" class="section level2">
<h2>Example: Bivariate normal distribution</h2>
<ul>
<li><p>target distribution: <span class="math inline">\((x_1,x_2)^\top\sim N(\mu,\Sigma)\)</span></p></li>
<li><p><span class="math inline">\(\mu=(\mu_1,\mu_2)^\top,\sigma_{11}=\sigma_{22}=1,\sigma_{12}=\sigma_{21}=\rho\in (-1,1)\)</span></p></li>
<li><p>conditional distribution:
<span class="math display">\[x_1|x_2\sim N(\mu_1+\rho(x_2-\mu_2),1-\rho^2)\]</span></p></li>
</ul>
<p><span class="math display">\[x_2|x_1\sim N(\mu_2+\rho(x_1-\mu_1),1-\rho^2)\]</span></p>
</div>
<div id="the-simulation" class="section level2">
<h2>The simulation</h2>
<p><img src="/media/img/gibbs.png" /></p>
</div>
<div id="chanllenges-of-monitoring-convergence-mixing-and-stationarity" class="section level2">
<h2>Chanllenges of monitoring convergence: mixing and stationarity</h2>
<p><img src="/media/img/convergence.png" /></p>
</div>
<div id="did-the-chain-mix-well" class="section level2">
<h2>Did the chain mix well?</h2>
<p>We can use the ACF or a trace.</p>
<p>The <strong>autocorrelation function</strong> (ACF) is a measure of the correlation between observations of a time series that are separated by <span class="math inline">\(k\)</span> time units.</p>
<p><img src="/media/img/ACF.png" width="60%" /></p>
<p>Recent promising work by Gorham &amp; Mackey using <strong>Stein discrepancy</strong> can
provide a “Yes” (but it’s expensive).</p>
</div>
<div id="example-hierarchical-normal-model" class="section level2">
<h2>Example: hierarchical normal model</h2>
<p>The data <span class="math inline">\(y_{ij},i=1,\dots,n_j,j=1,\dots,J\)</span>:</p>
<p><img src="/media/img/data_D.png" width="90%" /></p>
<p>Under the hierarchical normal model:</p>
<p><span class="math display">\[y_{ij}\sim N(\theta_j,\sigma^2)\]</span></p>
<p><span class="math display">\[\theta_j\sim N(\mu,\tau^2)\]</span></p>
<p>Uniform prior distribution: <span class="math inline">\((\mu,\log \sigma,\tau)\propto 1\)</span>
<span class="math display">\[(\mu,\log \sigma,\log \tau)\propto \tau\]</span></p>
</div>
<div id="posterior-distribution" class="section level2">
<h2>Posterior distribution</h2>
<p>The joint posterior density of all the parameters is
<span class="math display">\[p(\theta,\mu,\log \sigma,\tau|y) \propto \tau\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\prod_{j=1}^J\prod_{i=1}^{n_j}N(y_{ij}|\theta_j,\sigma^2)\]</span></p>
</div>
<div id="conditional-posterior-distribution" class="section level2">
<h2>Conditional posterior distribution</h2>
<ul>
<li><p>for <span class="math inline">\(\theta_j\)</span>
<span class="math display">\[\theta_j|\mu,\sigma,\tau,y\sim N(\hat{\theta}_j,V_{\theta_j})\]</span></p></li>
<li><p>for <span class="math inline">\(\mu\)</span>
<span class="math display">\[\mu|\theta,\sigma,\tau,y\sim N(\hat{\mu},\tau^2/J),\ \hat{\mu}=\frac 1J\sum_{j=1}^J\theta_j\]</span></p></li>
<li><p>for <span class="math inline">\(\sigma^2\)</span>
<span class="math display">\[\sigma^2|\theta,\mu,\tau,y=\sigma^2|\theta,y\sim \mathrm{Inv-}\chi^2(n,\hat{\sigma}^2)\]</span></p></li>
<li><p>for <span class="math inline">\(\tau^2\)</span>
<span class="math display">\[\tau^2|\theta,\mu,\sigma,y=\tau^2|\theta,\mu,y\sim \mathrm{Inv-}\chi^2(J-1,\hat{\tau}^2)\]</span></p></li>
</ul>
<p><span class="math display">\[\hat{\sigma}^2 = \frac 1 n\sum_{j=1}^J\sum_{i=1}^{n_j}(y_{ij}-\theta_j)^2,\ \hat{\tau}^2=\frac{1}{J-1}\sum_{j=1}^J(\theta_j-\mu)^2\]</span></p>
</div>
<div id="the-results" class="section level2">
<h2>The results</h2>
<p><img src="/media/img/normrel.png" /></p>
<ul>
<li><p>Starting pionts: <span class="math inline">\(\theta_j^{(0)}\sim U\{y_{ij},i=1,\dots,n_j\},\mu^{(0)}=\frac 1 J\sum_{j=1}^T \theta_j^{(0)},(\tau^2)^{(0)}\sim \mathrm{Inv-}\chi^2(J-1,\hat{\tau}^2), (\sigma^2)^{(0)}\sim \mathrm{Inv-}\chi^2(n,\hat{\sigma}^2)\)</span></p></li>
<li><p>The potential scale reduction <span class="math inline">\(\hat{R}\)</span>, which declines to <span class="math inline">\(1\)</span> as <span class="math inline">\(n\to\infty\)</span></p></li>
</ul>
</div>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">上一页</div>
    <a href="/courses/bayes/chap10/" rel="next">第10章</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">下一页</div>
    <a href="/courses/bayes/chap2/" rel="prev">第2章</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>最近更新于 May 5, 2019</p>

          





          


          


  
  



        </div>

      </article>

      <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    Zhijian He 2020
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic Website Builder</a>
    

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js" integrity="sha512-7t8APmYpzEsZP7CYoA7RfMPV9Bb+PJHa9x2WiUnDXZx3XHveuyWUtvNOexhkierl5flZ3tr92dP1mMS+SGlD+A==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"没有找到结果","placeholder":"搜索...","results":"搜索结果"};
      const content_type = {
        'post': "文章",
        'project': "项目",
        'publication' : "出版物",
        'talk' : "演讲",
        'slides' : "演示文稿"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.66c553246b0f279a03be6e5597f72b52.js"></script>

    






  
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">引用</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> 复制
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> 下载
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
