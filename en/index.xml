<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhijian He</title>
    <link>/en/</link>
      <atom:link href="/en/index.xml" rel="self" type="application/rss+xml" />
    <description>Zhijian He</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Zhijian He 2020</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Zhijian He</title>
      <link>/en/</link>
    </image>
    
    <item>
      <title>æ•°ç†ç»Ÿè®¡</title>
      <link>/en/courses/stat/hw/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0100</pubDate>
      <guid>/en/courses/stat/hw/</guid>
      <description>&lt;h2 id=&#34;ä½œä¸šè¯´æ˜-&#34;&gt;ä½œä¸šè¯´æ˜ ğŸ’¬&lt;/h2&gt;
&lt;p&gt;é€šè¿‡é•¿æ±Ÿé›¨è¯¾å ‚æäº¤ä½œä¸šï¼Œä¸€èˆ¬ä¸ºæ¯å‘¨ä¸€æ¬¡ä½œä¸šï¼Œæœ‰ç›¸åº”çš„deadlineã€‚å­¦ç”Ÿé€šè¿‡é›¨è¯¾å ‚ï¼ˆå…¬ä¼—å·/å°ç¨‹åº/ç”µè„‘ç«¯ï¼‰æäº¤ä½œä¸šã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;è¯·å‡†å¤‡ä¸€ä¸ªä½œä¸šæœ¬å†™ä¸‹æ¯æ¬¡çš„ä½œä¸šï¼Œå¹¶é€šè¿‡æ‹ç…§çš„æ–¹å¼åœ¨é›¨è¯¾å ‚ä¸Šæäº¤ã€‚æ³¨æ„&lt;strong&gt;ç…§ç‰‡éœ€è¦æ¸…æ™°å¯é˜…è¯»&lt;/strong&gt;ï¼Œæäº¤çš„ç­”æ¡ˆè¦å¯¹åº”ç›¸åº”çš„é¢˜å·ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æœ¬è¯¾ç¨‹è¿˜æœ‰äº›ä¸Šæœºç¼–ç¨‹çš„ä½œä¸šå’Œprojectï¼Œå¯é€šè¿‡é›¨è¯¾å ‚çš„ç”µè„‘ç«¯&lt;strong&gt;ä»¥PDFçš„å½¢å¼æäº¤ç”µå­ç‰ˆä½œä¸š&lt;/strong&gt;ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½œä¸šè¿˜æä¾›
&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmarkdown&lt;/a&gt;å’Œ
&lt;a href=&#34;https://www.latex-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latex&lt;/a&gt;ç‰ˆæœ¬ï¼Œä¾›ç†Ÿæ‚‰çš„åŒå­¦ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    æ³¨æ„ï¼šä½œä¸šåªæ¥å—å›¾ç‰‡å’ŒPDFä¸¤ç§æ ¼å¼ï¼Œåˆ‡å‹¿ä¸Šä¼ å‹ç¼©æ–‡ä»¶ï¼Œå¦åˆ™å½±å“åŠ©æ•™æ‰¹æ”¹æ•ˆç‡ã€‚åŸåˆ™ä¸Šä¸å…è®¸è¿Ÿäº¤ä½œä¸šï¼Œé™¤éæœ‰æ­£å½“ç†ç”±ã€‚
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;åŠ©æ•™-&#34;&gt;åŠ©æ•™ ğŸ‘&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;éƒ‘é“®ï¼ŒğŸ“§ &lt;a href=&#34;mailto:zhengzhengyv@126.com&#34;&gt;zhengzhengyv@126.com&lt;/a&gt;  (è´Ÿè´£å‰å…«æ¬¡ä½œä¸š)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æå‡Œæ¥ ï¼ŒğŸ“§ &lt;a href=&#34;mailto:1144747170@qq.com&#34;&gt;1144747170@qq.com&lt;/a&gt; (è´Ÿè´£åå…«æ¬¡ä½œä¸š)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æœ‰å…³ä½œä¸šé—®é¢˜è¯·å’¨è¯¢åŠ©æ•™ï¼&lt;/p&gt;
&lt;h2 id=&#34;ä½œä¸šé¢˜ç›®-&#34;&gt;ä½œä¸šé¢˜ç›® ğŸ’¯&lt;/h2&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    æ³¨æ„ï¼šå¦‚æœä¸‹é¢é“¾æ¥æ— æ³•æ‰“å¼€ï¼Œæœ‰å¯èƒ½æ˜¯å¾…å¸ƒç½®çš„ä½œä¸šè¿˜æ²¡ä¸Šä¼ åˆ°æœåŠ¡å™¨æ‰€è‡´ã€‚å¦‚æœé›¨è¯¾å ‚å‘å¸ƒäº†ä½œä¸šï¼Œä½†å¯¹åº”ä½œä¸šçš„é“¾æ¥æ— æ•ˆï¼Œè¯·é‚®ä»¶å‘ŠçŸ¥æˆ‘ï¼
  &lt;/div&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;åºå·&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;PDFç‰ˆ&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Rmarkdownç‰ˆ&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Latexç‰ˆ&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw1.pdf&#34;&gt;hw1.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw1.Rmd&#34;&gt;hw1.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw1.tex&#34;&gt;hw1.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw2.pdf&#34;&gt;hw2.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw2.Rmd&#34;&gt;hw2.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw2.tex&#34;&gt;hw2.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw3.pdf&#34;&gt;hw3.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw3.Rmd&#34;&gt;hw3.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw3.tex&#34;&gt;hw3.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw4.pdf&#34;&gt;hw4.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw4.Rmd&#34;&gt;hw4.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw4.tex&#34;&gt;hw4.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw5.pdf&#34;&gt;hw5.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw5.Rmd&#34;&gt;hw5.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw5.tex&#34;&gt;hw5.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw6.pdf&#34;&gt;hw6.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw6.Rmd&#34;&gt;hw6.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw6.tex&#34;&gt;hw6.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw7.pdf&#34;&gt;hw7.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw7.Rmd&#34;&gt;hw7.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw7.tex&#34;&gt;hw7.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw8.pdf&#34;&gt;hw8.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw8.Rmd&#34;&gt;hw8.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw8.tex&#34;&gt;hw8.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw9.pdf&#34;&gt;hw9.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw9.Rmd&#34;&gt;hw9.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw9.tex&#34;&gt;hw9.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw10.pdf&#34;&gt;hw10.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw10.Rmd&#34;&gt;hw10.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw10.tex&#34;&gt;hw10.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw11.pdf&#34;&gt;hw11.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw11.Rmd&#34;&gt;hw11.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw11.tex&#34;&gt;hw11.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw12.pdf&#34;&gt;hw12.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw12.Rmd&#34;&gt;hw12.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw12.tex&#34;&gt;hw12.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw13.pdf&#34;&gt;hw13.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw13.Rmd&#34;&gt;hw13.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw13.tex&#34;&gt;hw13.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw14.pdf&#34;&gt;hw14.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw14.Rmd&#34;&gt;hw14.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw14.tex&#34;&gt;hw14.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw15.pdf&#34;&gt;hw15.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw15.Rmd&#34;&gt;hw15.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw15.tex&#34;&gt;hw15.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw16.pdf&#34;&gt;hw16.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw16.Rmd&#34;&gt;hw16.Rmd&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;/media/hw/hw16.tex&#34;&gt;hw16.tex&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬1ç« </title>
      <link>/en/courses/bayes/chap1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap1/</guid>
      <description>&lt;h2 id=&#34;é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯å­¦æ´¾&#34;&gt;é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯å­¦æ´¾&lt;/h2&gt;
&lt;h3 id=&#34;é¢‘ç‡å­¦æ´¾ä¼ ç»Ÿå­¦æ´¾&#34;&gt;é¢‘ç‡å­¦æ´¾ï¼ˆä¼ ç»Ÿå­¦æ´¾ï¼‰&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;é¢‘ç‡å­¦æ´¾è®¤ä¸ºæ ·æœ¬ä¿¡æ¯æ¥è‡ªæ€»ä½“ï¼Œä»…é€šè¿‡ç ”ç©¶&lt;strong&gt;æ ·æœ¬ä¿¡æ¯&lt;/strong&gt;å¯ä»¥å¯¹&lt;strong&gt;æ€»ä½“ä¿¡æ¯&lt;/strong&gt;åšå‡ºåˆç†çš„æ¨æ–­å’Œä¼°è®¡ï¼Œå¹¶ä¸”æ ·æœ¬è¶Šå¤šï¼Œå°±è¶Šå‡†ç¡®ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä»£è¡¨æ€§äººç‰©ï¼šè´¹å¸Œå°” (R. A. Fisher, 1890-1962)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;è´å¶æ–¯å­¦æ´¾&#34;&gt;è´å¶æ–¯å­¦æ´¾&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;èµ·æºäºè‹±å›½å­¦è€…è´å¶æ–¯(T. Bayes, 1702-1761)åœ¨1763å¹´å‘è¡¨çš„è‘—åè®ºæ–‡ã€Šè®ºæœ‰å…³æœºé‡é—®é¢˜çš„æ±‚è§£ã€‹&lt;/li&gt;
&lt;li&gt;æœ€åŸºæœ¬è§‚ç‚¹ï¼šä»»ä½•ä¸€ä¸ªæœªçŸ¥é‡éƒ½å¯ä»¥çœ‹ä½œæ˜¯éšæœºçš„ï¼Œåº”è¯¥ç”¨ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå»æè¿°æœªçŸ¥å‚æ•°ï¼Œè€Œä¸æ˜¯é¢‘ç‡æ´¾è®¤ä¸ºçš„å›ºå®šå€¼ã€‚è¿™ç§ä¿¡æ¯ç§°ä¸º&lt;strong&gt;å…ˆéªŒä¿¡æ¯&lt;/strong&gt;ï¼Œæ˜¯ä¸»è§‚ä¿¡æ¯ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è´å¶æ–¯å…¬å¼&#34;&gt;è´å¶æ–¯å…¬å¼&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/bigbang_bayes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;æ¡ˆä¾‹ä¸€å¤©èå·æ ¸æ½œè‰‡æœæ•‘&#34;&gt;æ¡ˆä¾‹ä¸€ï¼šå¤©èå·æ ¸æ½œè‰‡æœæ•‘&lt;/h2&gt;
&lt;p&gt;1968å¹´5æœˆï¼Œç¾å›½æµ·å†›çš„å¤©èå·æ ¸æ½œè‰‡åœ¨å¤§è¥¿æ´‹äºšé€Ÿæµ·æµ·åŸŸçªç„¶å¤±è¸ªï¼Œæ½œè‰‡å’Œè‰‡ä¸Šçš„99åæµ·å†›å®˜å…µå…¨éƒ¨æ³æ— éŸ³ä¿¡ã€‚æŒ‰ç…§äº‹åè°ƒæŸ¥æŠ¥å‘Šçš„è¯´æ³•ï¼Œç½ªé­ç¥¸é¦–æ˜¯è¿™è‰˜æ½œè‰‡ä¸Šçš„ä¸€æšå¥‡æ€ªçš„é±¼é›·ï¼Œå‘å°„å‡ºå»åç«Ÿç„¶æ•Œæˆ‘ä¸åˆ†ï¼Œæ‰­å¤´å°„å‘è‡ªå·±ï¼Œè®©æ½œè‰‡ä¸­å¼¹çˆ†ç‚¸ã€‚&lt;/p&gt;
&lt;p&gt;ä¸ºäº†å¯»æ‰¾å¤©èå·çš„ä½ç½®ï¼Œç¾å›½æ”¿åºœä»å›½å†…è°ƒé›†äº†åŒ…æ‹¬å¤šä½ä¸“å®¶çš„æœç´¢éƒ¨é˜Ÿå‰å¾€ç°åœºï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä½åå«John Cravençš„æ•°å­¦å®¶ï¼Œä»–çš„å¤´è¡”æ˜¯â€œç¾å›½æµ·å†›ç‰¹åˆ«è®¡åˆ’éƒ¨é¦–å¸­ç§‘å­¦å®¶â€ã€‚åœ¨æœå¯»æ½œè‰‡çš„é—®é¢˜ä¸Šï¼ŒCravenæå‡ºçš„æ–¹æ¡ˆä½¿ç”¨äº†è´å¶æ–¯å…¬å¼ã€‚&lt;/p&gt;
&lt;p&gt;è¿™ç§æ–¹æ³•å·²ç»ç”¨äº2009å¹´æ³•èˆª447å’Œ2014å¹´é©¬èˆª370çš„æœæ•‘ã€‚
å…·ä½“åŸç†å‚è€ƒï¼š
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian_search_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian search theory&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;20è‹±é‡Œæµ·åŸŸçš„æ¦‚ç‡å›¾&#34;&gt;20è‹±é‡Œæµ·åŸŸçš„æ¦‚ç‡å›¾&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/scorpion.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;æ¡ˆä¾‹äºŒè”é‚¦å…šäººæ–‡é›†ä½œè€…å…¬æ¡ˆ&#34;&gt;æ¡ˆä¾‹äºŒï¼šè”é‚¦å…šäººæ–‡é›†ä½œè€…å…¬æ¡ˆ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1787å¹´5æœˆï¼Œç¾å›½å„å·çš„ä»£è¡¨åœ¨è´¹åŸå¬å¼€åˆ¶å®ªä¼šè®®ã€‚&lt;/li&gt;
&lt;li&gt;1787å¹´9æœˆï¼Œç¾å›½çš„å®ªæ³•è‰æ¡ˆè¢«åˆ†å‘åˆ°å„å·è¿›è¡Œè®¨è®ºã€‚ä¸€æ‰¹åå¯¹æ´¾ä»¥â€œåè”é‚¦ä¸»ä¹‰è€…â€ä¸ºç¬”åï¼Œå‘è¡¨äº†å¤§é‡æ–‡ç« å¯¹è¯¥è‰æ¡ˆæå‡ºæ‰¹è¯„ã€‚å®ªæ³•èµ·è‰äººä¹‹ä¸€&lt;strong&gt;äºšå†å±±å¤§Â·æ±‰å¯†å°”é¡¿&lt;/strong&gt;ç€æ€¥äº†ï¼Œä»–æ‰¾åˆ°æ›¾ä»»å¤–äº¤å›½åŠ¡ç§˜ä¹¦ï¼ˆå³åæ¥çš„å›½åŠ¡å¿ï¼‰çš„çº¦ç¿°Â·æ°ä¼Šï¼Œä»¥åŠçº½çº¦å¸‚å›½ä¼šè®®å‘˜&lt;strong&gt;éº¦è¿ªé€Š&lt;/strong&gt;ï¼Œä¸€åŒä»¥&lt;strong&gt;Publiusçš„ç¬”åå‘è¡¨æ–‡ç« &lt;/strong&gt;ï¼Œå‘å…¬ä¼—è§£é‡Šä¸ºä»€ä¹ˆç¾å›½éœ€è¦ä¸€éƒ¨å®ªæ³•ã€‚ä»–ä»¬èµ°ç¬”å¦‚é£ï¼Œé€šå¸¸åœ¨ä¸€å‘¨ä¹‹å†…å°±ä¼šå‘è¡¨3-4ç¯‡æ–°çš„è¯„è®ºã€‚&lt;/li&gt;
&lt;li&gt;1788å¹´ï¼Œä»–ä»¬æ‰€å†™çš„85ç¯‡æ–‡ç« ç»“é›†å‡ºç‰ˆï¼Œè¿™å°±æ˜¯ç¾å›½å†å²ä¸Šè‘—åçš„ã€Šè”é‚¦å…šäººæ–‡é›†ã€‹ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;æ¡ˆä¾‹äºŒè”é‚¦å…šäººæ–‡é›†ä½œè€…å…¬æ¡ˆ-1&#34;&gt;æ¡ˆä¾‹äºŒï¼šè”é‚¦å…šäººæ–‡é›†ä½œè€…å…¬æ¡ˆ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1810å¹´ï¼Œæ±‰å¯†å°”é¡¿æ¥å—äº†ä¸€ä¸ªæ”¿æ•Œçš„å†³æ–—æŒ‘æˆ˜ã€‚åœ¨å†³æ–—ä¹‹å‰æ•°æ—¥ï¼Œæ±‰å¯†å°”é¡¿è‡ªçŸ¥æ—¶æ—¥ä¸å¤šï¼Œä»–åˆ—å‡ºäº†ä¸€ä»½ã€Šè”é‚¦å…šäººæ–‡é›†ã€‹çš„ä½œè€…åå•ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1818å¹´ï¼Œéº¦è¿ªé€Šåˆæå‡ºäº†å¦ä¸€ä»½ä½œè€…åå•ã€‚è¿™ä¸¤ä»½åå•å¹¶ä¸ä¸€è‡´ã€‚åœ¨85ç¯‡æ–‡ç« ä¸­ï¼Œæœ‰73ç¯‡æ–‡ç« çš„ä½œè€…èº«ä»½è¾ƒä¸ºæ˜ç¡®ï¼Œå…¶ä½™&lt;strong&gt;12ç¯‡å­˜åœ¨äº‰è®®&lt;/strong&gt;ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1955å¹´ï¼Œå“ˆä½›å¤§å­¦ç»Ÿè®¡å­¦æ•™æˆFredrick Mostelleræ‰¾åˆ°èŠåŠ å“¥å¤§å­¦çš„å¹´è½»ç»Ÿè®¡å­¦å®¶David Wallanceï¼Œå»ºè®®ä»–è·Ÿè‡ªå·±ä¸€èµ·åšä¸€ä¸ªå°è¯¾é¢˜ï¼Œä»–æƒ³ç”¨ç»Ÿè®¡å­¦çš„æ–¹æ³•ï¼Œ&lt;strong&gt;é‰´å®šå‡ºã€Šè”é‚¦å…šäººæ–‡é›†ã€‹çš„ä½œè€…èº«ä»½&lt;/strong&gt;ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é‡‡ç”¨çš„æ–¹æ³•æ˜¯ä»¥&lt;strong&gt;è´å¶æ–¯å…¬å¼ä¸ºæ ¸å¿ƒçš„åˆ†ç±»ç®—æ³•&lt;/strong&gt;ã€‚å…ˆæŒ‘é€‰ä¸€äº›èƒ½å¤Ÿåæ˜ ä½œè€…å†™ä½œé£æ ¼çš„è¯æ±‡ï¼Œåœ¨å·²ç»ç¡®å®šäº†ä½œè€…çš„æ–‡æœ¬ä¸­ï¼Œå¯¹è¿™äº›ç‰¹å¾è¯æ±‡çš„å‡ºç°é¢‘ç‡è¿›è¡Œç»Ÿè®¡ï¼Œç„¶åå†ç»Ÿè®¡è¿™äº›è¯æ±‡åœ¨é‚£äº›ä¸ç¡®å®šä½œè€…çš„æ–‡æœ¬ä¸­çš„å‡ºç°é¢‘ç‡ï¼Œä»è€Œæ ¹æ®è¯é¢‘çš„å·®åˆ«æ¨æ–­ä½œè€…å½’å±ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è´å¶æ–¯çš„å‘å±•&#34;&gt;è´å¶æ–¯çš„å‘å±•&lt;/h2&gt;
&lt;h3 id=&#34;ç»å…¸ç»Ÿè®¡å­¦çš„å›°éš¾&#34;&gt;ç»å…¸ç»Ÿè®¡å­¦çš„å›°éš¾&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ç»å…¸ç»Ÿè®¡å­¦æ¯”è¾ƒé€‚åˆäºè§£å†³å°å‹çš„é—®é¢˜ï¼ŒåŒæ—¶éœ€è¦è¶³å¤Ÿå¤šçš„æ ·æœ¬æ•°æ®&lt;/li&gt;
&lt;li&gt;éƒ½å¤§æ•°æ®æ—¶ä»£äº†ï¼Œè¿˜å­˜åœ¨æ•°æ®ç¨€ç–æ€§é—®é¢˜å—ï¼Ÿ&lt;strong&gt;è‡´ç—…åŸºå› &lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;è´å¶æ–¯ç½‘ç»œå¸¦æ¥å·¥å…·é©å‘½&#34;&gt;è´å¶æ–¯ç½‘ç»œå¸¦æ¥å·¥å…·é©å‘½&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;é¾™å·é£çš„å½¢æˆã€æ˜Ÿç³»çš„èµ·æºã€è‡´ç—…åŸºå› ã€å¤§è„‘çš„è¿ä½œæœºåˆ¶ç­‰ï¼Œè¦æ­ç¤ºéšè—åœ¨è¿™äº›é—®é¢˜èƒŒåçš„è§„å¾‹ï¼Œå°±å¿…é¡»ç†è§£å®ƒä»¬çš„&lt;strong&gt;æˆå› ç½‘ç»œ&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;è´å¶æ–¯å…¬å¼+å›¾è®º&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è´å¶æ–¯ç»Ÿè®¡çš„å‘å±•&#34;&gt;è´å¶æ–¯ç»Ÿè®¡çš„å‘å±•&lt;/h2&gt;
&lt;h3 id=&#34;åº”ç”¨é¢†åŸŸ&#34;&gt;åº”ç”¨é¢†åŸŸ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;è‡ªç„¶è¯­è¨€å¤„ç†ï¼šè®¡ç®—æœºç¿»è¯‘è¯­è¨€ã€è¯†åˆ«è¯­éŸ³ã€è®¤è¯†æ–‡å­—å’Œæµ·é‡æ–‡çŒ®çš„æ£€ç´¢&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥æ¬¢è¿æ‚¨!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;äººå·¥æ™ºèƒ½ã€æ— äººé©¾é©¶&lt;/li&gt;
&lt;li&gt;åƒåœ¾çŸ­ä¿¡ã€åƒåœ¾é‚®ä»¶è¯†åˆ«&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;è´å¶æ–¯å†³ç­–&#34;&gt;è´å¶æ–¯å†³ç­–&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;å¦‚ä½•åœ¨ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹æ‰¾é¤é¦†åƒé¥­ï¼Ÿ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ä¸€äº›è¯„ä»·&#34;&gt;ä¸€äº›è¯„ä»·&lt;/h2&gt;
&lt;p&gt;**Berger (1985)**è¯´ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œé˜²æ­¢è¯¯ç”¨çš„æœ€å¥½æ–¹æ³•æ˜¯ç»™äººä»¬åœ¨å…ˆéªŒä¿¡æ¯æ–¹é¢ä»¥é€‚å½“çš„æ•™è‚²ï¼Œå¦å¤–åœ¨è´å¶æ–¯åˆ†æçš„æœ€åæŠ¥å‘Šä¸­ï¼Œåº”å°†å…ˆéªŒåˆ†å¼€æ¥å†™ï¼Œä»¥ä¾¿ä½¿å…¶ä»–äººå¯¹ä¸»è§‚è¾“å…¥çš„åˆç†æ€§åšå‡ºè¯„ä»·ã€‚â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;**Good (1973)**æ›´æ˜¯ç›´æˆªäº†å½“çš„è¯´ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œä¸»è§‚ä¸»ä¹‰è€…ç›´æŠ’ä»–ä»¬çš„åˆ¤æ–­ï¼Œè€Œå®¢è§‚ä¸»ä¹‰è€…ä»¥å‡è®¾æ¥æ©ç›–å…¶åˆ¤æ–­ï¼Œå¹¶ä»¥æ­¤äº«å—ç§‘å­¦å®¢è§‚æ€§çš„è£è€€ã€‚â€&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>å˜åˆ†æ¨æ–­</title>
      <link>/en/courses/bayes/vi/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/vi/</guid>
      <description>


&lt;p&gt;This note is adapted to the paper entitled â€œVariation Inference: A Review for Statisticansâ€ by Blei et al.Â (2017).&lt;/p&gt;
&lt;div id=&#34;basic-ideas-of-vi&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic ideas of VI&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(z=z_{1:m}\)&lt;/span&gt; be the latent variables that govern the distribution of the data (observations) &lt;span class=&#34;math inline&#34;&gt;\(x=x_{1:n}\)&lt;/span&gt;.
The prior is denoted by &lt;span class=&#34;math inline&#34;&gt;\(p(z)\)&lt;/span&gt;. The likelihood is &lt;span class=&#34;math inline&#34;&gt;\(p(x|z)\)&lt;/span&gt;. The posterior thus is given by
&lt;span class=&#34;math display&#34;&gt;\[p(z|x)=\frac{p(z)p(x|z)}{p(x)}\propto p(z)p(x|z).\]&lt;/span&gt;
The denominator &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; contains the marginal density of the obsevations, also called the &lt;em&gt;evidence&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ABC algorithms provide a kind of approximations of the posterior in the context of simulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;VI provides another kind of approximations of the posterior by minizing the &lt;em&gt;Kullback-Leibler (KL) divergence&lt;/em&gt; to the exact posterior over a family of approximate densities &lt;span class=&#34;math inline&#34;&gt;\(\mathcal Q\)&lt;/span&gt;. That is
&lt;span class=&#34;math display&#34;&gt;\[q^*(z) = \arg \min_{q(z)\in\mathcal Q} KL (q(z)||p(z|x)).\]&lt;/span&gt;
The KL divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. The formal definition of KL divergence is &lt;span class=&#34;math display&#34;&gt;\[KL(p_1||p_2)=E_{p_1}[\log (p_1(x)/p_2(x))]=E_{p_1}[\log p_1(x)]-E_{p_1}[\log p_2(x)].\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(KL(p_1||p_2)\ge 0\)&lt;/span&gt;, where the equality holds iff &lt;span class=&#34;math inline&#34;&gt;\(p_1(z)=p_2(z)\)&lt;/span&gt; w.p.1. This can be proved via Jensen inequality. Noting that
&lt;span class=&#34;math display&#34;&gt;\[KL(p_1||p_2)=-E_{p_1}[\log (p_2(x)/p_1(x))]\ge -\log E_{p_1}[p_2(x)/p_1(x)]=-\log \int \frac{p_2(x)}{p_1(x)} p_1(x) dx =0.\]&lt;/span&gt;
The equality holds iff &lt;span class=&#34;math inline&#34;&gt;\(p_2(x)/p_1(x)\)&lt;/span&gt; is constant w.p.1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread, i.e, &lt;span class=&#34;math inline&#34;&gt;\(KL(p_1||p_2)\neq KL(p_2||p_1)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It also does not satisfy the triangle inequality &lt;span class=&#34;math inline&#34;&gt;\(KL(p_1||p_2)+KL(p_2||p_3)\ge KL(p_1||p_3)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the objective is not computable because it requires computing &lt;span class=&#34;math inline&#34;&gt;\(\log p(x)\)&lt;/span&gt;, which is typically infeasible. To see why,
&lt;span class=&#34;math display&#34;&gt;\[KL (q(z)||p(z|x))=E_{q}[\log q(z)]-E_{q}[\log p(z|x)]=E_{q}[\log q(z)]-E_{q}[\log p(z)]-E_{q}[\log p(x|z)]+\log p(x).\]&lt;/span&gt;
As a result, we optimize an alternative objective that is equivalent to the KL up to an added constant,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
ELBO(q)=E_{q}[\log(p(x,z)/q(z))]=E_{q}[\log p(z)]+E_{q}[\log p(x|z)]-E_{q}[\log q(z)].\label{eq:elbo}
\end{equation}\]&lt;/span&gt;
This function is called the &lt;em&gt;evidence lower bound (ELBO)&lt;/em&gt;. It is easy to see that
&lt;span class=&#34;math display&#34;&gt;\[ELBO(q)=-KL (q(z)||p(z))+\log p(x).\]&lt;/span&gt;
Since the log-evidence is constant,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
q^*(z) = \arg \min_{q(z)\in\mathcal Q} KL (q(z)||p(z|x))=\arg\max_{q(z)\in\mathcal Q} ELBO(q).
\label{eq:elboopt}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It follows from &lt;span class=&#34;math inline&#34;&gt;\(KL(p_1||p_2)\ge 0\)&lt;/span&gt; that &lt;span class=&#34;math inline&#34;&gt;\(ELBO(q)\le \log p(x)\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;. This means the ELBO is a lower-bound of the log-evidence, explaining its name.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From the second equality in , maximazing the ELBO mirrors the usual balance between likelihood and prior.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mean-field-variational-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Mean-Field Variational Family&lt;/h2&gt;
&lt;p&gt;The complexity of the family determines the complexity of the optimization; it is more difficulty to optimize over a complex family than a simple family. We next focus on the &lt;em&gt;mean-field variational family&lt;/em&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
q(z) = \prod_{j=1}^m q_j(z_j).\label{eq:mfvf}
\end{equation}\]&lt;/span&gt;
Each latent variable &lt;span class=&#34;math inline&#34;&gt;\(z_j\)&lt;/span&gt; is governed by its own variational factor, the density &lt;span class=&#34;math inline&#34;&gt;\(q_j\)&lt;/span&gt;. That is &lt;span class=&#34;math inline&#34;&gt;\(z_j\stackrel{ind}\sim q_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One may specify the parametric form of the individual variational factors. In principle, each can take on any parametric form appropriate to the corresponding random variable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A continous variable might have a Gaussian factor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A categorical variable will typically have a categorical factor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;coordinate-ascent-mean-field-vi&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coordinate Ascent Mean-Field VI&lt;/h2&gt;
&lt;p&gt;This section describe one of the most commonly used algorithms for solving the optimizatin problem  subject to the mean-field variational family . The coordinate ascent VI (CAVI) iteratively optimizes each factor of the mean-field variation density, while holding the others fixed. It climbs the ELBO to a local optimum.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(z_{-j}\)&lt;/span&gt; be the vector of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; by removing the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th component &lt;span class=&#34;math inline&#34;&gt;\(z_j\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(p(z_j|z_{-j},x)\)&lt;/span&gt; be the &lt;em&gt;complete conditional&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(z_j\)&lt;/span&gt; given all of the other latent variables in the model and the observations.
Fixing the other variational factors, &lt;span class=&#34;math inline&#34;&gt;\(q_\ell(z_\ell)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\ell\neq j\)&lt;/span&gt;, the optimal &lt;span class=&#34;math inline&#34;&gt;\(q_j(z_j)\)&lt;/span&gt; is then propotional to the exponentiated expected log of the complete conditional,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
q_j^*(z_j) \propto \exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}\propto \exp\{E_{-j}[\log p(x,z)]\},\label{eq:cavi}
\end{equation}\]&lt;/span&gt;
where the expectation is with respective to the currently fixed variational density over &lt;span class=&#34;math inline&#34;&gt;\(z_{-j}\)&lt;/span&gt;, i.e, &lt;span class=&#34;math inline&#34;&gt;\(\prod_{\ell\neq j} q_\ell (z_\ell)\)&lt;/span&gt;. To see why, when fixing the other variational factors, &lt;span class=&#34;math inline&#34;&gt;\(q_\ell(z_\ell)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\ell\neq j\)&lt;/span&gt;, it follows from  that
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
ELBO(q) &amp;amp;= ELBO(q_j) = E_{q}[\log(p(x,z)/q(z))] \\&amp;amp;= E_{q}[\log(p(z_{j}|z_{-j},x)p(z_{-j},x)/q_{j}(z_j)/q_{-j}(z_{-j}))]\\
&amp;amp; = E_{q}[\log(p(z_{j}|z_{-j},x)/q_j(z_j)] + \text{const}\\
&amp;amp;=E_{q_j}[E_{-j}[\log(p(z_{j}|z_{-j},x)]-\log q_j(z_j)] + \text{const}\\
&amp;amp;=E_{q_j}[\log (\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}/q_j(z_j))]+ \text{const}\\
&amp;amp;= - KL(q_j(z_j)||c\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\})+ \text{const},
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is a normalized constant such that &lt;span class=&#34;math inline&#34;&gt;\(c\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\)&lt;/span&gt; is a PDF.
Since &lt;span class=&#34;math inline&#34;&gt;\(KL\ge 0\)&lt;/span&gt;, the maximization of ELBO attains at &lt;span class=&#34;math inline&#34;&gt;\(q_j(z_j)=c\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}\)&lt;/span&gt; w.p.1. Therefore, the optimal &lt;span class=&#34;math inline&#34;&gt;\(q_j(z_j)\)&lt;/span&gt; is propotional to &lt;span class=&#34;math inline&#34;&gt;\(\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;CAVI goes as follows: Initizlize the variational factors &lt;span class=&#34;math inline&#34;&gt;\(q_j(z_j)\)&lt;/span&gt;; Update each factor of the mean-field variation density by , while holding the others fixed, until the ELBO converges. To check the convergence, we may compute the ELBO after a (few) loop of all the factors.&lt;/p&gt;
&lt;p&gt;The ELBO is (generally) a nonconvex objective function. CAVI only guarantees to a local optimum, which can be sensitive to iniitialization. Also, the updated variational factor should have a closed form.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-i-bayesian-mixture-of-gaussians&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application I: Bayesian mixture of Gaussians&lt;/h2&gt;
&lt;p&gt;As a concrete example, we consider a Bayesian mixture of &lt;em&gt;unit-variance univariate Gaussians&lt;/em&gt;. There are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; mixture components, corresponding to &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; Gaussian distributions with means &lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\dots,\mu_K)\)&lt;/span&gt;. Given the means, the data is generated via
&lt;span class=&#34;math display&#34;&gt;\[x_i|\mu,\alpha\stackrel{iid}{\sim} \sum_{k=1}^K \alpha_{k} N(\mu_k,1),\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{k}&amp;gt;0\)&lt;/span&gt; is the probablity drawn from the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th Guassian with &lt;span class=&#34;math inline&#34;&gt;\(\sum_{k=1}^K \alpha_{k} =1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We now add some latent variables to reformulate the model. This is actually a technique of &lt;em&gt;data augment&lt;/em&gt;. Let the latent variable &lt;span class=&#34;math inline&#34;&gt;\(c_i\)&lt;/span&gt; be an indicator &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;-vector, all zeros expect for a one in the position corresponding to &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;â€™s cluster. There are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; possible values for &lt;span class=&#34;math inline&#34;&gt;\(c_i\)&lt;/span&gt;. As a result, &lt;span class=&#34;math inline&#34;&gt;\(x_i|\mu,c_i\sim N(c_i^\top \mu,1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(c_i\sim \text{categorical}(\alpha)=:CG(\alpha)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\alpha=(\alpha_{1},\dots,\alpha_{K})\)&lt;/span&gt;. Assume that the mean parameters are drawn independently from a common prior &lt;span class=&#34;math inline&#34;&gt;\(p(\mu_k)\sim N(0,\sigma^2)\)&lt;/span&gt;; the prior variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; ia a hyperparameter; and the prior for the latent indicators is &lt;span class=&#34;math inline&#34;&gt;\(c_i\sim CG(1/K,1/K,\dots,1/K)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The full hierarchical model is
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\mu_k&amp;amp;\stackrel{iid}{\sim} N(0,\sigma^2), &amp;amp; k=1,\dots,K,\\
c_i&amp;amp;\stackrel{iid}{\sim} \text{categorical}(1/K,1/K,\dots,1/K), &amp;amp; i=1,\dots, n,\\
x_i|\mu,c_i&amp;amp;\stackrel{ind}{\sim} N(c_i^\top \mu,1), &amp;amp;i=1,\dots, n.
\end{align}\]&lt;/span&gt;
The latent variables are &lt;span class=&#34;math inline&#34;&gt;\(z=(\mu, c)\)&lt;/span&gt;. The joint density of latent and observed variables is
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,c,x) = p(\mu) \prod_{i=1}^n p(c_i)p(x_i|c_i,\mu).\]&lt;/span&gt;
The evidence is
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
p(x)= \int p(\mu) \prod_{i=1}^n \sum_{c_i} p(c_i)p(x_i|c_i,\mu) d\mu=\sum_{c_1,\dots,c_n}\prod_{i=1}^n p(c_i) \int p(\mu) \prod_{i=1}^n p(x_i|c_i,\mu) d\mu.\label{eq:gmmevi}
\end{align}\]&lt;/span&gt;
Thanks to conjugacy between the Gaussian prior on the components and the Gaussian likelihood, each individual integral &lt;span class=&#34;math inline&#34;&gt;\(I(c_1,\dots,c_n):=\int p(\mu) \prod_{i=1}^n p(x_i|c_i,\mu) d\mu\)&lt;/span&gt; is computable. However, the total cases of the configuration &lt;span class=&#34;math inline&#34;&gt;\((c_1,\dots,c_n)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(K^n\)&lt;/span&gt;. As a result, the complexity of computing  is &lt;span class=&#34;math inline&#34;&gt;\(O(K^n)\)&lt;/span&gt;, which is infeasible for moderate sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. For example, when &lt;span class=&#34;math inline&#34;&gt;\(K=3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(K^n = 3^{100}\approx 5.2\times 10^{47}\)&lt;/span&gt;. In this sense, we can say that the evidence  is intractable.&lt;/p&gt;
&lt;p&gt;In VI, we choose the mean-field variational family as the form
&lt;span class=&#34;math display&#34;&gt;\[q(\mu,c) = \prod_{k=1}^K q(\mu_k;m_k,s_k^2)\prod_{i=1}^nq(c_i;\psi_i),\]&lt;/span&gt;
where the variational factor &lt;span class=&#34;math inline&#34;&gt;\(q(\mu_k;m_k,s_k^2)\)&lt;/span&gt; for the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is a Guassian &lt;span class=&#34;math inline&#34;&gt;\(N(m_k,s_k^2)\)&lt;/span&gt;, and the variational factor &lt;span class=&#34;math inline&#34;&gt;\(q(c_i;\psi_i)\)&lt;/span&gt; for the indicator is &lt;span class=&#34;math inline&#34;&gt;\(CG(\psi_i)\)&lt;/span&gt;.
By , we have
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
ELBO(m,s^2,\psi)&amp;amp;=E_{q}[\log p(z)]+E_{q}[\log p(x|z)]-E_{q}[\log q(z)]\notag\\
&amp;amp;=\sum_{k=1}^K E_{\mu_k\sim N(m_k,s_k^2)}[\log p(\mu_k)]\notag\\
&amp;amp;\quad+\sum_{i=1}^n (E_{c_i\sim CG(\psi_i)}[\log p(c_i)]+E_{c_i\sim CG(\psi_i),\mu\sim N(m,\text{diag}(s^2))}[\log p(x_i|c_i,\mu)])\notag\\
&amp;amp;\quad-\sum_{i=1}^n E_{c_i\sim CG(\psi_i)}[\log q(c_i;\psi_i)]-\sum_{k=1}^K E_{\mu_k\sim N(m_k,s_k^2)}[\log q(\mu_k;m_k,s_k^2)]\notag\\
&amp;amp;=\frac K 2-K\log\sigma-n\log K-\frac 12 n\log(2\pi)+\frac 1 2\sum_{i=1}^n x_i^2+\sum_{k=1}^K\left[\log(s_k)-\frac{m_k^2+s_k^2}{2\sigma^2}\right] \notag\\
&amp;amp;\quad-\sum_{i=1}^n\sum_{k=1}^K\psi_{ik}\left[\frac{m_k^2+s_k^2}2-x_im_k+\log(\psi_{ik}) \right]\notag\\
&amp;amp;=\sum_{k=1}^K\left[\log(s_k)-\frac{m_k^2+s_k^2}{2\sigma^2}\right] -\sum_{i=1}^n\sum_{k=1}^K\psi_{ik}\left[\frac{m_k^2+s_k^2}2-x_im_k+\log(\psi_{ik}) \right]+\text{const}.\label{eq:BMGelbo}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that all the expectation in the ELBO  can be computed in closed form. There are many methods to find a local optimum of .&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Newton-Raphson algorithm.&lt;/strong&gt; It suffices to find the root of &lt;span class=&#34;math inline&#34;&gt;\(\nabla ELBO(m,s^2,\psi) = 0\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\lambda = (m,s^2,\psi)\in \mathbb{R}^{2K+n(K-1)}\)&lt;/span&gt; be a vector of parameters. The Newton-Raphson method uses the iteration
&lt;span class=&#34;math display&#34;&gt;\[\lambda^{(t+1)}=\lambda^{(t)}-(D^2 ELBO(\lambda^{(t)}))^{-1}  \nabla ELBO(\lambda^{(t)}),\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(D^2 ELBO(\lambda^{(t)})\)&lt;/span&gt; is the Hessian matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gradient ascent algorithm.&lt;/strong&gt; It is a first-order iterative optimization algorithm for finding a local maximum. The iteration is
&lt;span class=&#34;math display&#34;&gt;\[\lambda^{(t+1)}=\lambda^{(t)} + \eta_t \nabla ELBO(\lambda^{(t)}),\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\eta_t&amp;gt;0\)&lt;/span&gt; is the learning rate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CAVI.&lt;/strong&gt; The iteration is
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\psi_{t+1,ik} &amp;amp;\propto \exp\{E[\mu_k]x_i-E[\mu_k^2]/2\}\propto \exp\{m_{t,k}x_i-(m_{t,k}^2+s_{t,k}^2)/2\},\\
m_{t+1,k}&amp;amp;=\frac{\sum_{i=1}^n \psi_{t+1,ik}x_i}{1/\sigma^2+\sum_{i=1}^n\psi_{t+1,ik}}, \label{eq:miter} \\
s_{t+1,k}^2&amp;amp;=\frac{1}{1/\sigma^2+\sum_{i=1}^n\psi_{t+1,ik}}, k=1,\dots,K, i=1,\dots,n, \label{eq:siter}
\end{align}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\psi_{t,\cdot}, m_{t,\cdot},s^2_{t,\cdot}\)&lt;/span&gt; denote the parameters at the step &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Note that the algorithm does not need the initial varitional factors for &lt;span class=&#34;math inline&#34;&gt;\(\psi_i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, we implement the CAVI algorithm for &lt;span class=&#34;math inline&#34;&gt;\(K=5\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n=10^3\)&lt;/span&gt;. After a few steps, we can see that the ELBO converges.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
## data generation
K = 5 # the number of clusters
n = 1000 # the number of data x_i
mu = matrix(rnorm(K,mean=0.2,sd=2),ncol = 1) # the means of the K clusters
c = sample(1:K,n,replace = T) # the indicator
x = matrix(mu[c]+rnorm(n),ncol = 1)
plot(density(x),xlab = &amp;#39;x&amp;#39;,main=&amp;#39;kernel density of the data&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/VI_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sig = 1 # hyperparameter for the variance of mu_i
## ELBO minus a constant
elbo &amp;lt;- function(m,s,psi){
  re = sum(log(s)-(m^2+s^2)/(2*sig^2))- sum(psi%*%(m^2+s^2)/2)+
    t(x)%*%psi%*%m-sum(log(psi)*psi)
  return(re)
}
## iteration for CAVI
cavi &amp;lt;- function(m,s){
  psi = matrix(0,n,K)
  for(i in 1:n){
    tmp = x[i]*m-(m^2+s^2)/2
    mtmp = max(tmp)
    logsum = mtmp+log(sum(exp(tmp-mtmp)))
    psi[i,] = exp(tmp-logsum)
  }
  de = 1/sig^2+colSums(psi)
  m = t(x)%*%psi/de
  s = sqrt(1/de)
  return(list(m_next=matrix(m,ncol = 1),s_next=matrix(s,ncol = 1),psi_next=psi))
}
## initialization
nstep = 1e4 # maximal steps
tolerance = 1e-6 # tolerance for the relative change
m = matrix(rnorm(K,0,1),K,1) 
s = matrix(5,K,1)
ELBO = matrix(0,nstep,1)
step = 1
relative_change = tolerance+1
while(TRUE){
  para = cavi(m,s)
  m = para$m_next
  s = para$s_next
  psi = para$psi_next
  ELBO[step] = elbo(m,s,psi)
  if(step&amp;gt;1)
    relative_change = (ELBO[step]-ELBO[step-1])/ELBO[step]
  if(step==nstep | abs(relative_change)&amp;lt;tolerance){ # stopping rule
    break
  }
  else{
    step = step+1
  }
}
hatc = apply(psi, 1,which.max)
center = cbind(sort(mu),sort(m))
colnames(center) = c(&amp;#39;True Centers&amp;#39;,&amp;#39;VI Means&amp;#39;)
knitr::kable(center)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;True Centers&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;VI Means&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.4712572&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.5346393&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.0529076&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.9663787&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.5672866&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3772178&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.8590155&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9019340&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3.3905616&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.4156061&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(1:step,ELBO[1:step],type = &amp;#39;b&amp;#39;,xlab = &amp;#39;Step&amp;#39;,ylab=&amp;#39;ELBO&amp;#39;,pch = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/VI_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x,col=hatc)
abline(h=m,col=1:5,lty=2,lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/VI_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-family-conditionals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential Family Conditionals&lt;/h2&gt;
&lt;p&gt;Are there specific forms for the local variational
approximations in which we can easily compute closed-form conditional ascent updates? Yes, the answer is exponential family conditionals.&lt;/p&gt;
&lt;p&gt;Consider the generic model &lt;span class=&#34;math inline&#34;&gt;\(p(z,x)\)&lt;/span&gt; and suppose each complete conditional is in the exponential family:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(z_j|z_{-j},x) = h(z_j)\exp\{\eta_j(z_{-j},x)^\top t(z_j)-a(\eta_j(z_{-j},x))\},
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(t(z_j)\)&lt;/span&gt; is the sufficient statistic, and &lt;span class=&#34;math inline&#34;&gt;\(\eta_j(z_{-j},x)\)&lt;/span&gt; are the natural parameters.&lt;/p&gt;
&lt;p&gt;Consider mean-field VI for this class of models, where &lt;span class=&#34;math inline&#34;&gt;\(q(z)\)&lt;/span&gt; is given by . The update  becomes
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
q_j^*(z_j) &amp;amp;\propto \exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}\\
&amp;amp;=\exp\{\log h(z_j)+ E_{-j}[\eta_j(z_{-j},x)^\top t(z_j)]-E_{-j}[a(\eta_j(z_{-j},x))]\}\\
&amp;amp;\propto h(z_j)\exp\{E_{-j}[\eta_j(z_{-j},x)]^\top t(z_j)\}.
\end{align}\]&lt;/span&gt;
This updata reveals the parametric form of the optimal VI factors. Each one is in the same exponential family as its corresponding complete conditional. Let &lt;span class=&#34;math inline&#34;&gt;\(\nu_j\)&lt;/span&gt; denote the variational parameter for the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th variational factor. When we update each factor, we set its parameter equal to the expected parameter of the complete conditional,
&lt;span class=&#34;math display&#34;&gt;\[\nu_j = E_{-j}[\eta_j(z_{-j},x)].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are many popular models fall into this category, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian mixtures of exponential family models with conjugate priors.&lt;/li&gt;
&lt;li&gt;Hierarchical hidden Markov models.&lt;/li&gt;
&lt;li&gt;Kalman filter models and switching Kalman filters.&lt;/li&gt;
&lt;li&gt;Mixed-membership models of exponential families.&lt;/li&gt;
&lt;li&gt;Factorial mixtures / hidden Markov models of exponential families.&lt;/li&gt;
&lt;li&gt;Bayesian linear regression.&lt;/li&gt;
&lt;li&gt;Any model containing only conjugate pairs and multinomials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some popular models do not fall into this category, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian logistic regression and other nonconjugate Bayesian generalized linear models.&lt;/li&gt;
&lt;li&gt;Correlated topic model, dynamic topic model.&lt;/li&gt;
&lt;li&gt;Discrete choice models.&lt;/li&gt;
&lt;li&gt;Nonlinear matrix factorization models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-gradient-variational-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic Gradient Variational Inference&lt;/h2&gt;
&lt;p&gt;CAVI may require interating thought the entire dataset at each iteration. As the dataset size grows, each
iteration becomes more computationally expensive (see  and ). In more realistic models, the gradient of the ELBO  is rarely available in closed form. Stochastic gradient methods (Robbins
and Monro, 1951) are useful for optimizing an objective function whose gradient can be unbiasedly estimated. Stochastic gradient variational inference (SGVI) becomes an alternative to coordinate ascent. SGVI combines gradients and stochastic optimazation.&lt;/p&gt;
&lt;p&gt;Now we rewrite the ELBO as a function of variational parameters &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (a vector), denoted by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L(\lambda)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda\mathcal L(\lambda)\)&lt;/span&gt; be the gradient vector of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L(\lambda)\)&lt;/span&gt; w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Gradient ascent algorithm iterates
&lt;span class=&#34;math display&#34;&gt;\[\lambda^{(t+1)}=\lambda^{(t)} + \eta_t \nabla_\lambda\mathcal L(\lambda^{(t)}),\quad t=0,\dots,T.\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\nabla_\lambda\mathcal L(\lambda)}\)&lt;/span&gt; be an unibased estimate of &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda\mathcal L(\lambda)\)&lt;/span&gt;. SGVI iterates as follow,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\lambda^{(t+1)}=\lambda^{(t)} + \eta_t \widehat{\nabla_\lambda\mathcal L(\lambda^{(t)})},\quad t=1,\dots,T.\label{eq:sgdite}
\end{equation}\]&lt;/span&gt;
Under certain regularity conditions, and the learning rates satisfy the &lt;strong&gt;Robbins-Monro&lt;/strong&gt; conditions
&lt;span class=&#34;math display&#34;&gt;\[\sum_{t=0}^\infty \eta_t=\infty,\ \sum_{t=0}^\infty \eta_t^2&amp;lt;\infty,\]&lt;/span&gt;
the iterations converge to a local optimum (Robbins and Monro, 1951). Many sequences will satisfy these conditions, for example, &lt;span class=&#34;math inline&#34;&gt;\(\eta_t=t^{-\kappa}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\kappa\in(0.5,1]\)&lt;/span&gt;. Adaptive learning rates are currently
popular (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2015; Kingma and Ba, 2015). &lt;strong&gt;Adam&lt;/strong&gt; is a promising method, which is pulished in&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the International Conference on Learning Representations. (Cited by 43201 on 2020/5/27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The name Adam
is derived from adaptive moment estimation.&lt;/p&gt;
&lt;p&gt;One important thing is to obtain an unbiased estimate of the gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda\mathcal L(\lambda)\)&lt;/span&gt;.
The variational density is now written as &lt;span class=&#34;math inline&#34;&gt;\(q(z;\lambda)\)&lt;/span&gt;. Then,
&lt;span class=&#34;math display&#34;&gt;\[\nabla_\lambda\mathcal L(\lambda) = \nabla_\lambda E_q[\log p(z,x)]-\nabla_\lambda E_q[\log q(z;\lambda)].\]&lt;/span&gt;
Note that in some cases (such as mean-field variational family with Gaussian or categorical factors) of variational densities, &lt;span class=&#34;math inline&#34;&gt;\(E_q[\log q(z;\lambda)]\)&lt;/span&gt; is analytically solvable, and so does its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda E_q[\log q(z;\lambda)]\)&lt;/span&gt;. We thus focus on estimating &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda E_q[\log p(z,x)]\)&lt;/span&gt;. Suppose that
&lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda E_q[\log p(z,x)]\)&lt;/span&gt; can be written as an expectation, i.e.,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\nabla_\lambda E_q[\log p(z,x)]=E[h(z)].\label{eq:expform}
\end{equation}\]&lt;/span&gt;
Then one can easily find an unbiased estimate of &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda\mathcal L(\lambda)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\widehat{\nabla_\lambda\mathcal L(\lambda^{(t)})}=\frac 1 N \sum_{i=1}^N h(z_i) -\nabla_\lambda E_q[\log q(z;\lambda)],\label{eq:sgd}
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; are Monte Carlo samples or quasi-Monte Carlo samples.&lt;/p&gt;
&lt;p&gt;There are two tricks to obtain the expectation form . Allowing the interchange of integration and differentiation, &lt;strong&gt;the score function gradient method&lt;/strong&gt;
makes use of
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\nabla_\lambda E_q[\log p(z,x)]&amp;amp;=\nabla_\lambda\int \log p(z,x) q(z;\lambda)d z\\
&amp;amp;= \int \log p(z,x) \nabla_\lambda q(z;\lambda)d z\\
&amp;amp;= \int \log p(z,x) (\nabla_\lambda \log q(z;\lambda)) q(z;\lambda)d z\\
&amp;amp;=E_q[\log p(z,x) \nabla_\lambda \log q(z;\lambda)],
\end{align}\]&lt;/span&gt;
achieving the form  with &lt;span class=&#34;math inline&#34;&gt;\(h(z)=\log p(z,x) \nabla_\lambda \log q(z;\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\sim q(z;\lambda)\)&lt;/span&gt;. On the other hand, &lt;strong&gt;the reparameterization method&lt;/strong&gt; rewrites the expectation &lt;span class=&#34;math inline&#34;&gt;\(E_q[\log p(z,x)]\)&lt;/span&gt;
as an expectation w.r.t. a density independently of the parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, say, &lt;span class=&#34;math inline&#34;&gt;\(E_{p_0}[\log p(h(z;\lambda),x)]\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(h(z;\lambda)\sim q(z;\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\sim p_0(z)\)&lt;/span&gt; independent of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. As a result,
by allowing the interchange of integration and differentiation,
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\nabla_\lambda E_q[\log p(z,x)]&amp;amp;=\nabla_\lambda E_{p_0}[\log p(h(z;\lambda),x)]\\
&amp;amp;= \nabla_\lambda \int \log (p(h(z;\lambda),x)) p_0(z) d z\\
&amp;amp;=  \int \nabla_\lambda \log (p(h(z;\lambda),x)) p_0(z) d z\\
&amp;amp;=E_{p_0}[\nabla_\lambda \log (p(h(z;\lambda),x))],
\end{align}\]&lt;/span&gt;
achieving the form  with &lt;span class=&#34;math inline&#34;&gt;\(h(z)=\nabla_\lambda \log (p(h(z;\lambda),x))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\sim p_0(z)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The reparameterization gradient typically exhibits &lt;em&gt;lower variance&lt;/em&gt; than the score function gradient but
is restricted to models where the variational family can be reparametrized via a differentiable mapping. We refer to the article&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Xu, M., Quiroz, M., Kohn, R., &amp;amp; Sisson, S. A. (2018). Variance reduction properties of the reparameterization trick. arXiv preprint arXiv:1809.10330.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The convergence of the gradient ascent scheme in  tends
to be slow when gradient estimators  have a high variance.
Therefore, various approaches for reducing the variance of
both gradient estimators exist; e.g.Â control variates,
Rao-Blackwellization, importance sampling as well as quasi-Monte Carlo. For the use of qausi-Monte Carlo in VI, we refer to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Buchholz, A., Wenzel, F., &amp;amp; Mandt, S. (2018). Quasi-monte carlo variational inference. arXiv preprint arXiv:1807.01604.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-multinomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian multinomial logistic regression&lt;/h2&gt;
&lt;p&gt;The famous (Fisherâ€™s or Andersonâ€™s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y_i\in \{0,\dots,K\}\)&lt;/span&gt; be the categorial data, which relates to &lt;span class=&#34;math inline&#34;&gt;\(x_i = (x_{i1},\dots,x_{ip})^\top\)&lt;/span&gt;. The multinomial logistic regression is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
P(y_i=k|\beta) = \frac{\exp\{x_i^\top \beta_k\}}{\sum_{j=0}^K \exp\{x_i^\top \beta_j\}},\quad k=0,\dots,K,
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_k\in \mathbb{R}^{p\times 1}\)&lt;/span&gt; and the parameters are &lt;span class=&#34;math inline&#34;&gt;\(\beta=(\beta_1,\dots,\beta_K)\)&lt;/span&gt;, and we set &lt;span class=&#34;math inline&#34;&gt;\(\beta_0=0\)&lt;/span&gt; for indentifying the model.
The prior we used is &lt;span class=&#34;math inline&#34;&gt;\(\beta_k\stackrel{iid}\sim N(0,\sigma_0^2I_p),k=1,\dots,K\)&lt;/span&gt;.
We treated the designed matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as a constant matrix.&lt;/p&gt;
&lt;p&gt;The variational density we used is Gaussian, i.e., &lt;span class=&#34;math inline&#34;&gt;\(q(\beta_{ij})\sim N(\mu_{ij},\sigma_{ij}^2)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\psi_{ij}=\log (\sigma_{ij})\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(q(\beta_{ij})\sim N(\mu_{ij},\exp(2\psi_{ij})).\)&lt;/span&gt; Now the variational parameters are &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ij}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi_{ij}\)&lt;/span&gt;. We encapsulate them in a vector &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Denote the ELBO by &lt;span class=&#34;math inline&#34;&gt;\(L(\lambda)\)&lt;/span&gt;. We thus have
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
L(\lambda) &amp;amp;= E_q[\log p(\beta)] + E_q[\log p(y|\theta)] - E_q[\log q(\beta)]\\
&amp;amp;=\sum_{ik}\left(\psi_{ik}-\frac {\mu_{ik}^2+\exp(2\psi_{ik})}{2\sigma^2_0}\right) + \sum_{i=1}^n E_q\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top \beta_k\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top \beta_k\}}\right)\right]+\text{const}\\
&amp;amp;=L_1(\lambda)+L_2(\lambda)+\text{const}.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial L_1(\lambda)}{\partial \mu_{ik}}=-\frac{\mu_{ik}}{\sigma_0^2},\quad \frac{\partial L_1(\lambda)}{\partial \psi_{ik}}=1-\frac{\exp(2\psi_{ik})}{\sigma_0^2}.\]&lt;/span&gt;
This implies &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\lambda L_1(\lambda)\)&lt;/span&gt; has a close form.&lt;/p&gt;
&lt;p&gt;The score function gradient for &lt;span class=&#34;math inline&#34;&gt;\(L_2(\lambda)\)&lt;/span&gt; is given by
&lt;span class=&#34;math display&#34;&gt;\[\nabla_\lambda L_2(\lambda)=\sum_{i=1}^n E_q\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top \beta_k\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top \beta_k\}}\right)\nabla_\lambda \log q(\beta;\lambda)\right],\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial  \log q(\beta;\lambda)}{\partial \mu_{ik}}=\frac{\beta_{ik}-\mu_{ik}}{\exp(2\psi_{ik})},\ \frac{\partial  \log q(\beta;\lambda)}{\partial \psi_{ik}}=\frac{(\beta_{ik}-\mu_{ik})^2}{\exp(2\psi_{ik})}-1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now rewrites the expectation &lt;span class=&#34;math inline&#34;&gt;\(L_2(\lambda)\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
L_2(\lambda)&amp;amp;=\sum_{i=1}^n E_q\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top \beta_k\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top \beta_k\}}\right)\right]\\
&amp;amp;=\sum_{i=1}^n E\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}\right)\right],\\
\end{align}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(z_k\stackrel{iid}\sim N(0,I_p)\)&lt;/span&gt;.
The
reparameterization gradient is given by
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\nabla_\lambda L_2(\lambda)&amp;amp;=  \sum_{i=1}^n  E\left[\nabla_\lambda\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}\right)\right]\\
&amp;amp;=:\sum_{i=1}^n E[\nabla_\lambda h_{i}(z;\lambda)].
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\frac{\partial  h_{i}(z;\lambda)}{\mu_{jk}}&amp;amp;= x_{ij}1\{y_i=k\} -\frac{x_{ij}\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}},
\end{align}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\frac{\partial  h_{i}(z;\lambda)}{\psi_{jk}}&amp;amp;=x_{ij}z_{jk}\exp(\psi_{jk})1\{y_i=k\}-\frac{x_{ij}z_{jk}\exp(\psi_{jk})\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}.
\end{align}\]&lt;/span&gt;
As a result,&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;iris.png&#34; alt=&#34;The species are Iris setosa, versicolor, and virginica.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The species are Iris setosa, versicolor, and virginica.
&lt;/p&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sepal.Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sepal.Width&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Petal.Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Petal.Width&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Species&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;103&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We next show the results for the iris data with the score function gradient based Adam algorithm.&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## data generation
set.seed(0)
data(iris)
n &amp;lt;- nrow(iris)
p &amp;lt;- ncol(iris)
K &amp;lt;- nlevels(iris$Species)
X &amp;lt;- model.matrix(Species ~ ., data=iris) # design matrix
Y &amp;lt;- model.matrix(~ Species - 1, data=iris)

sigma0 = 10
elbo_hat&amp;lt;- function(mu,psi,N){
  L1 = sum(psi-(mu^2+exp(2*psi))/(2*sigma0^2))
  beta = matrix(0,p,K) # the first column is zero 
  est = matrix(0,N,1)
  for (i in 1:N){
    beta[,-1] = mu + exp(psi) * matrix(rnorm(p*(K-1)),p,K-1)
    tmp = exp(X%*%beta)
    den = rowSums(tmp)
    num = rowSums(tmp*Y)
    est[i] = sum(log(num/den))
  }
  return(mean(est)+L1)
}

score_fun_gradient &amp;lt;- function(mu,psi,N){
  dmu = -mu/sigma0^2
  dpsi = 1-exp(2*psi)/sigma0^2
  beta = matrix(0,p,K) # the first column is zero 
  for (i in 1:N){
    beta[,-1] = mu + exp(psi) * matrix(rnorm(p*(K-1)),p,K-1)
    tmp = exp(X%*%beta)
    den = rowSums(tmp)
    num = rowSums(tmp*Y)
    sumlog = sum(log(num/den))
    dmu = dmu + sumlog*(beta[,-1]-mu)/exp(2*psi)/N
    dpsi = dpsi + sumlog*((beta[,-1]-mu)^2/exp(2*psi)-1)/N
  }
  return(list(dmu=-dmu,dpsi=-dpsi)) # return negative gradient for adapting the Adam
}

#Adam: See the paper &amp;#39;ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION&amp;#39;

alpha = 0.1
beta1 = 0.9
beta2 = 0.999
eps = 1e-8

#initial parameters
mut = matrix(0,p,K-1)
psit = matrix(0,p,K-1)
mu_mt = matrix(0,p,K-1) #first moment
psi_mt = matrix(0,p,K-1)
mu_vt = matrix(0,p,K-1) #second moment
psi_vt = matrix(0,p,K-1)
T = 500
Ng = 100 #sample size for gradient
Nelbo = 10000 #sample size for estimating elbo
elbo = matrix(0,T,1)
for(t in 1:T){
  gt = score_fun_gradient(mut,psit,Ng)
  mu_mt = beta1*mu_mt + (1-beta1)*gt$dmu
  mu_vt = beta2*mu_vt + (1-beta2)*gt$dmu^2
  hat_mu_mt = mu_mt/(1-beta1^t)
  hat_mu_vt = mu_vt/(1-beta2^t)
  mut = mut - alpha*hat_mu_mt/(sqrt(hat_mu_vt)+eps)
  
  psi_mt = beta1*psi_mt + (1-beta1)*gt$dpsi
  psi_vt = beta2*psi_vt + (1-beta2)*gt$dpsi^2
  hat_psi_mt = psi_mt/(1-beta1^t)
  hat_psi_vt = psi_vt/(1-beta2^t)
  psit = psit - alpha*hat_psi_mt/(sqrt(hat_psi_vt)+eps) #update the state
  
  elbo[t] = elbo_hat(mut,psit,Nelbo)
}
plot(elbo,type=&amp;#39;b&amp;#39;,xlab = &amp;#39;Iteration&amp;#39;,ylab=&amp;quot;ELBO&amp;quot;,pch=16)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬10ç« </title>
      <link>/en/courses/bayes/chap10/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap10/</guid>
      <description>


&lt;div id=&#34;introduction-to-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to Bayesian computation&lt;/h2&gt;
&lt;p&gt;The goals are to estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\propto p(\theta)p(y|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior predictive distribution
&lt;span class=&#34;math display&#34;&gt;\[p(\tilde y|y) = \int p(\tilde y|\theta)p(\theta|y)d \theta =E[p(\tilde y|\theta)|y]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are therefore insterested in estimating the posterior expectation
&lt;span class=&#34;math display&#34;&gt;\[\mu=E[h(\theta)|y]=\int h(\theta)p(\theta|y)d \theta\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;moments: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=\theta^k\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;probability: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=1_A(\theta)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A\subseteq \Theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;predictive density: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=p(\tilde y|\theta)\)&lt;/span&gt; for fixed &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo methods&lt;/h2&gt;
&lt;p&gt;Suppose we can simulate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(1)},\dots,\theta^{(N)}\sim p(\theta|y)\)&lt;/span&gt; independently. Monte Carlo (MC) esimate is then the sample average:
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_N = \frac1N\sum_{i=1}^N h(\theta^{(i)})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLN: &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_N\to \mu\)&lt;/span&gt; w.p.1 as &lt;span class=&#34;math inline&#34;&gt;\(N\to \infty\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;CLT: &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_N-\mu=O_p(N^{-1/2})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional quadrature rulesâ€™ have error rate &lt;span class=&#34;math inline&#34;&gt;\(O(N^{-r/d})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(r\ge 1\)&lt;/span&gt; depends on the smoothness of the functions, and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the dimension of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This suffers &lt;strong&gt;the curse of dimensionality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;MC has an error rate &lt;span class=&#34;math inline&#34;&gt;\(O(N^{-1/2})\)&lt;/span&gt; independently of the smoothness and the dimension of the functions. The task is to simulate iid samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim p(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-number-generators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random number generators&lt;/h2&gt;
&lt;p&gt;We start with a pseudo-random number generator:
&lt;span class=&#34;math display&#34;&gt;\[u_1,\dots,u_n,\dots\stackrel{iid}\sim U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mersenne Twister&lt;/strong&gt; by Matsumoto &amp;amp; Nishimura (1998), whose period is &lt;span class=&#34;math inline&#34;&gt;\(2^{19937}-1&amp;gt;10^{6000}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RngStreams&lt;/strong&gt; by Lâ€™Ecuyer, Simard, Chen, Kelton (2002)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They arenâ€™t really uniform random, but good ones are close enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-uniform-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-uniform random variables&lt;/h2&gt;
&lt;p&gt;Some common distributions (such as Normal, exponential, binomial, Poisson etc.) are already in some scientific softwares (R, Python, Matlab, Julia, Mathematica, etc.)&lt;/p&gt;
&lt;p&gt;We are now concerned with a general distribution. Principled approaches are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inversion&lt;/li&gt;
&lt;li&gt;acceptance-rejection&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;inversion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inversion&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; be the CDF of the distribution of interest. We can simulate the distribution via iid uniforms &lt;span class=&#34;math inline&#34;&gt;\(U_1,\dots,U_N\stackrel{iid}\sim U(0,1)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_i=F^{-1}(U_i),i=1,\dots,N\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(F^{-1}\)&lt;/span&gt; is the inverse of the CDF &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, definited by
&lt;span class=&#34;math display&#34;&gt;\[F^{-1}(u)=\inf\{x\in\mathbb{R}|F(x)\ge u\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is easy to see that &lt;span class=&#34;math inline&#34;&gt;\(X_i\stackrel{iid}\sim F\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;inversion-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inversion: examples&lt;/h2&gt;
&lt;div id=&#34;gaussian&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gaussian&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\Phi^{-1}(U)\sim N(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\mu+\sigma\Phi^{-1}(U) \sim N(\mu,\sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exponential&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X= -\frac 1\lambda \log (1-U)\sim Exp(\lambda)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bernoulli&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bernoulli&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=1\{U\le p\}\sim Bin(1,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-inverse-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate inverse transformation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,\dots,x_d)\)&lt;/span&gt; be the PDF of &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_d\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(F_i(x_i)\)&lt;/span&gt; be the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=2,\dots,d\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(F_i(x_i|x_1,\dots,x_{i-1})\)&lt;/span&gt; be the conditional CDF&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The multivariate inverse transformation is proposed by Rosenblatt (1952), which simulates the components &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; recursively, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1=F_1^{-1}(U_1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X_i=F_i^{-1}(U_i|X_1,\dots,X_{i-1}),\ i=2,\dots,d\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the output has the destribution &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the order of simulating the components can be arbitrary&lt;/li&gt;
&lt;li&gt;the critical issue is to know the conditional CDFs in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;suppose the target distrubtion is &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; with the support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we can sample &lt;span class=&#34;math inline&#34;&gt;\(Y\sim g\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is another density satisfying: there exists &lt;span class=&#34;math inline&#34;&gt;\(M&amp;gt;0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[\frac{f(x)}{g(x)}\le M\ \forall x\in \mathcal{X}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we can compute &lt;span class=&#34;math inline&#34;&gt;\(f(x)/g(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(Y\sim g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: accept &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as a draw from &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(f(Y)/(Mg(Y))\)&lt;/span&gt;. If the draw is rejected, return to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2â€™: simulate &lt;span class=&#34;math inline&#34;&gt;\(U\sim U(0,1)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\text{accept } Y &amp;amp; U\le f(Y)/(Mg(Y))\\
\text{go to Step 1 }&amp;amp; else
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figs/AR.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the acceptance probability: &lt;span class=&#34;math display&#34;&gt;\[E[f(Y)/(Mg(Y))]=\frac 1 M\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we may choose the smallest &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\le Mg(x)\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(x\in\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-for-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection for Bayesian computation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=\frac{p(\theta)p(y|\theta)}{p(y)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the constant &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; is unknown&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the AR algorithm works well if taking &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=p(\theta)p(y|\theta)\)&lt;/span&gt;
and using proposal density &lt;span class=&#34;math inline&#34;&gt;\(\propto g(\theta)\)&lt;/span&gt; with
&lt;span class=&#34;math display&#34;&gt;\[\frac{p(\theta)p(y|\theta)}{g(\theta)}\le M\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-gamma-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Gamma distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\lambda)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;0\)&lt;/span&gt; is the shape, &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; is the rate&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}1\{x&amp;gt; 0\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Gamma(\alpha,\lambda)\stackrel{d}{=}\frac 1 \lambda Gamma(\alpha,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Gamma(\alpha,1)\stackrel{d}{=}U(0,1)^{1/\alpha}Gamma(\alpha+1,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;so our target is &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,1)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1\)&lt;/span&gt;. For this case, the density is bounded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal
&lt;span class=&#34;math display&#34;&gt;\[g(x)=?\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;gamma-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gamma density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ahrens and Dieter (1974) took proposals from a density that combines a
Gaussian density in the center and an exponential density in the right tail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Marsaglia and Tsang (2000) present an AR algorithm from a truncated
&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-beta-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Beta distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt; density
&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{\Gamma(\alpha,\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}1\{0&amp;lt;x&amp;lt;1\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generate a Beta from two independent Gammas
&lt;span class=&#34;math display&#34;&gt;\[Beta(\alpha,\beta)\stackrel{d}{=}\frac{Gamma(\alpha,\lambda)}{Gamma(\alpha,\lambda)+Gamma(\beta,\lambda)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta&amp;gt;1\)&lt;/span&gt;, the beta density is unimodal and achieves its maximum at &lt;span class=&#34;math inline&#34;&gt;\(x^*=(\alpha-1)/(\alpha+\beta-2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposal distribution: &lt;span class=&#34;math inline&#34;&gt;\(U(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(M=f(x^*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;accept &lt;span class=&#34;math inline&#34;&gt;\(U\sim U(0,1)\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(f(U)/M\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-generator-r-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta generator: R code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myBeta &amp;lt;- function(n,alpha,beta){
  if(alpha&amp;lt;=1 | beta&amp;lt;=1)
    stop(&amp;quot;alpha, beta cannot be &amp;lt;= 1&amp;quot;)
  M = dbeta((alpha-1)/(alpha+beta-2),alpha,beta)
  x = rep(0,n)
  for(i in 1:n){
    while (TRUE){
      U = runif(1)
      if(dbeta(U,alpha,beta)&amp;gt;= M*runif(1)){
        x[i] = U
        break
      }
    }
  }
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 1: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;myBeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4001780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1968478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;dbeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3996059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;true values&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mu=E_f[h(X)]\)&lt;/span&gt; w.r.t. the density &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal density &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(g(x)&amp;gt;0\)&lt;/span&gt; whenever &lt;span class=&#34;math inline&#34;&gt;\(h(x)f(x)&amp;gt;0\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu=\int h(x)f(x)dx=\int h(x)\frac{f(x)}{g(x)}g(x)dx=E_g[h(X)f(X)/g(X)]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)/g(x)\)&lt;/span&gt; called the &lt;strong&gt;likelihood ratio (LR)&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The IS algorithm goes below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Step 2: compute the sample average:
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{IS}=\frac 1N\sum_{i=1}^N \frac{h(X_i)f(X_i)}{g(X_i)}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the proposal&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[\hat{\mu}_{IS}] = \frac{\sigma^2_g}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_g = \int \left(\frac{h(x)f(x)}{g(x)}-\mu\right)^2g(x)d x=\int\frac{(h(x)f(x)-\mu g(x))^2}{g(x)}dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g(x)=h(x)f(x)/\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h\ge 0\)&lt;/span&gt;, then we have &lt;strong&gt;the optimal case&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g=0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;but unattainable: &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is unknown constant&lt;/li&gt;
&lt;li&gt;we may find &lt;span class=&#34;math inline&#34;&gt;\(g(x)\approx h(x)f(x)/\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-weight-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The weight function&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(w(x)=f(x)/g(x)\)&lt;/span&gt; be the LR
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_g =\int \frac{(hf)^2}{g}dx -\mu^2\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\int \frac{(hf)^2}{g}dx=E_f[w(X)h(X)^2]=E_g[w(X)^2h(X)^2]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(w(x)\)&lt;/span&gt; is bounded, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g\)&lt;/span&gt; is bounded&lt;/li&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(w(x)\)&lt;/span&gt; is unbounded, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g\)&lt;/span&gt; may be unbounded (&lt;strong&gt;the worst case!&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;self-normalized-is-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Self-normalized IS (SNIS)&lt;/h2&gt;
&lt;p&gt;What if we cannot compute &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;? Suppose that
&lt;span class=&#34;math display&#34;&gt;\[f(x)=c_f\tilde{f}(x),\ g(x)=c_g\tilde{g}(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can compute &lt;span class=&#34;math inline&#34;&gt;\(\tilde f,\tilde g\)&lt;/span&gt; but not the constants &lt;span class=&#34;math inline&#34;&gt;\(c_f,c_g\)&lt;/span&gt;. Then we use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{SNIS}= \frac{\frac 1 N\sum_{i=1}^nh(X_i)\tilde{f}(X_i)/\tilde{g}(X_i)}{\frac 1 N\sum_{i=1}^n\tilde{f}(X_i)/\tilde{g}(X_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or, equivalently,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{SNIS}= \frac{\frac 1 N\sum_{i=1}^Nh(X_i){f}(X_i)/{g}(X_i)}{\frac 1 N\sum_{i=1}^N{f}(X_i)/{g}(X_i)}=\frac{\frac 1 N\sum_{i=1}^Nh(X_i)w(X_i)}{\frac 1 N\sum_{i=1}^Nw(X_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-of-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance of SNIS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Taylor expansions
&lt;span class=&#34;math display&#34;&gt;\[f(\bar X,\bar Y)\approx f(\mu_1,\mu_2)+f_x(\mu_1,\mu_2)(\bar X-\mu_1)+f_y(\mu_1,\mu_2)(\bar Y-\mu_2)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[f(\bar X,\bar Y)]\approx f(\mu_1,\mu_2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[f(\bar X,\bar Y)]\approx f_x^2Var[\bar X]+f_y^2Var[\bar Y]+2f_xf_yCov(\bar X,\bar Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(f(x,y)=x/y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f_x=1/y,f_y=-x/y^2\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[Var[f(\bar X,\bar Y)]\approx \frac{\sigma_X^2}{N\mu_2^2}+\frac{\mu_1^2\sigma_Y^2}{N\mu_2^4}-\frac{2\mu_1}{N\mu_2^3}Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{\mu}_{SNIS}]\approx \frac{1}{N}E_g[w(X)^2(h(X)-\mu)^2]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{\mu}_{IS}]= \frac{1}{N}E_g[(h(X)w(X)-\mu)^2]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;optimal-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimal SNIS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SNIS: &lt;span class=&#34;math inline&#34;&gt;\(g_{opt}(x)\propto f(x)|h(x)-\mu|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS: &lt;span class=&#34;math inline&#34;&gt;\(g_{opt}(x)\propto f(x)|h(x)|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;effective-sample-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Effective sample size&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Unequal weighting raises variance, see, Kong (1992), Evans and Swartz (1995)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for iid &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; with variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and fixed &lt;span class=&#34;math inline&#34;&gt;\(w_i\ge 0\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[Var\left(\frac{\sum_{i}w_iY_i}{\sum_iw_i}\right)=\frac{\sum_iw_i^2\sigma^2}{(\sum_iw_i)^2}=\frac{\sigma^2}{N_{eff}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the effective sample size &lt;span class=&#34;math inline&#34;&gt;\(N_{eff}\)&lt;/span&gt; is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[N_{eff} = \frac{(\sum_{i=1}^Nw_i)^2}{\sum_{i=1}^Nw_i^2}\in [1,N]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_{eff}\)&lt;/span&gt; is small if there are few extremely high weights which would unduly influence the distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for equal weights, we have &lt;span class=&#34;math inline&#34;&gt;\(N_{eff}=N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Suppose the posterior distribution is &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, the proposal distribution is &lt;span class=&#34;math inline&#34;&gt;\(t_3(\mu,\sigma^2)\)&lt;/span&gt;. Consider &lt;span class=&#34;math inline&#34;&gt;\(\mu=\sigma=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Effective sample size is  9178 / 10000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 2: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=100&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=1000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=10000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exact_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.214328&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.011347&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.945335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.069694&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.688183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.052519&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;Suppose the posterior distribution is &lt;span class=&#34;math inline&#34;&gt;\(t_3(\mu,\sigma^2)\)&lt;/span&gt;, the proposal distribution is &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Effective sample size is  6180 / 10000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 3: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=100&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=1000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=10000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exact_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.802681&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.784954&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.019707&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.630305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.931088&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.875331&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;is-vs-acceptance-rejection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS vs acceptance rejection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Acceptance-rejection requires bounded LR &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also have to know a bound&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS and SNIS require us to keep track of weights&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plain IS requires normalized &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Acceptance-rejection samples cost more (due to rejections)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;is-for-rare-events&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS for rare events&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rare events:
&lt;span class=&#34;math display&#34;&gt;\[h(x)=1_A(x), \mu = E_f[h(x)]=\int_A f(x) dx=\epsilon\approx 0\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;coefficient of variation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[cv:=\frac{\sigma/\sqrt{N}}{\mu}=\frac{\sqrt{\epsilon(1-\epsilon)}}{\sqrt{n}\epsilon}\approx \frac{1}{\sqrt{n\epsilon}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;to get &lt;span class=&#34;math inline&#34;&gt;\(cv=0.1\)&lt;/span&gt; takes &lt;span class=&#34;math inline&#34;&gt;\(N\ge 100/\epsilon\)&lt;/span&gt;, e.g., &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 10^{-5}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(N\ge 10^7\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Taking &lt;span class=&#34;math inline&#34;&gt;\(X\sim f\)&lt;/span&gt; does not get enough data from the important region &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get more data from &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (from a proper proposal &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;), and then correct the bias (the LR function)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-a-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Changing a parameter&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;norminal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta_0)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{1}{N}\sum_{i=1}^N h(X_i) \frac{p(X_i;\theta_0)}{p(X_i;\theta)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The importance ratio often simplifies, e.g., in exponential families.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-tilting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential tilting&lt;/h2&gt;
&lt;p&gt;Many important distributions can be written in the form
&lt;span class=&#34;math display&#34;&gt;\[p(x;\theta) = a(\theta)\exp[\eta(\theta)^\top T(x)]b(x), \theta\in \Theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{a(\theta_0)}{a(\theta)}\frac{1}{N}\sum_{i=1}^N h(X_i) \exp[(\eta(\theta_0)-\eta(\theta))^\top T(X_i)]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eta(\theta)\)&lt;/span&gt; is the natrual parameter&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This is called the â€˜exponential twistingâ€™.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal is to choose &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\mu_\theta]\)&lt;/span&gt; is minimized.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simple-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A simple example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;norminal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta_0)=N(x;0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta)=N(x;\theta,1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\mathbb{R}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;target function &lt;span class=&#34;math inline&#34;&gt;\(h(x) = 1\{x&amp;gt;c\}\)&lt;/span&gt;, for large &lt;span class=&#34;math inline&#34;&gt;\(c&amp;gt;0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu=E[h(X)]=1-\Phi(c)\approx 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{1}{N}\sum_{i=1}^N h(X_i) \frac{N(X_i;0,1)}{N(X_i;\theta,1)}=\frac{1}{N}\sum_{i=1}^N h(X_i) e^{-\frac{2\theta X_i-\theta^2}{2}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS variance &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\mu_\theta]=\sigma^2_\theta/N\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\theta=\frac{e^{\theta^2}}{\sqrt{2\pi}}\int_c^\infty e^{-\frac{(x+\theta)^2}{2}}dx-\mu^2=e^{\theta^2}[1-\Phi(c+\theta)]-\mu^2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the optimal parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta^*=\arg \min_{\theta\in \mathbb{R}} e^{\theta^2}[1-\Phi(c+\theta)]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effect-of-different-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The effect of different parameters&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap10_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the threshold c = 3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the true value is 0.00135&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the optimal theta is 3.155&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;variance reduction factor is 222&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;applications-in-computational-finance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applications in Computational Finance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, P. Heidelberger, and P. Shahabuddin. Asymptotically optimal importance
sampling and stratification for pricing path-dependent options. &lt;em&gt;Mathematical Finance&lt;/em&gt;, 9
(2):117â€“152, 1999.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, P. Heidelberger, and P. Shahabuddin. Variance reduction techniques for
estimating value-at-risk. &lt;em&gt;Management Science&lt;/em&gt;, 46(10):1349â€“1364, 2000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, J. Li. Importance Sampling for Portfolio Credit Risk. &lt;em&gt;Management Science&lt;/em&gt;, 51(11):1643â€“1656, 2005.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Xie, Fei, &lt;strong&gt;Zhijian He&lt;/strong&gt;, and Xiaoqun Wang. An Importance Sampling-Based Smoothing Approach for Quasi-Monte Carlo Simulation of Discrete Barrier Options. &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, October 17, 2018.
&lt;a href=&#34;https://doi.org/10.1016/j.ejor.2018.10.030&#34;&gt;https://doi.org/10.1016/j.ejor.2018.10.030&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling-for-portfolio-credit-risk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling for Portfolio Credit Risk&lt;/h2&gt;
&lt;p&gt;Our interest centers on the distribution of losses
from default over a fixed horizon.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;: number of obligors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt;: default indicator for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor, &lt;span class=&#34;math inline&#34;&gt;\(Y_k=1\)&lt;/span&gt; denotes the default; &lt;span class=&#34;math inline&#34;&gt;\(Y_k=0\)&lt;/span&gt; otherwise&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt;: marginal probability that &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor defaults&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;: loss resulting from default of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L=c_1Y_1+\dots+c_mY_m\)&lt;/span&gt;: total loss from defaults&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our goal is to estimate tail probabilities &lt;span class=&#34;math inline&#34;&gt;\(P(L&amp;gt;x)\)&lt;/span&gt;, especially at large values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-copula-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal copula model&lt;/h2&gt;
&lt;p&gt;In the normal copula model, dependence
is introduced through a multivariate normal vector &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_m\)&lt;/span&gt; of latent variables. Each default indicator is represented as
&lt;span class=&#34;math display&#34;&gt;\[Y_k = 1\{X_k&amp;gt; x_k\},\ k=1,\dots,m.\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X_k = a_{k1}Z_1+\dots+a_{kd}Z_d+b_k\epsilon_k\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; are chosen to match &lt;span class=&#34;math inline&#34;&gt;\(P(X_k&amp;gt;x_k)=p_k\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\stackrel{iid}{\sim} N(0,1)\)&lt;/span&gt; are systematic risk factors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_k\stackrel{iid}{\sim} N(0,1)\)&lt;/span&gt; is an idiosyncratic risk&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(a_{k1},\dots,a_{kd}\)&lt;/span&gt; are the loading factors satisfying &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^d a_{kj}^2\le 1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_k=\sqrt{1-\sum_{j=1}^d a_{kj}^2}\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(X_k\sim N(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;is-for-independent-obligors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS for independent obligors&lt;/h2&gt;
&lt;p&gt;Consider the simple case of independent obligors: &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}=0,\ b_k=1\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Y_k\sim Bin(1,p_k)\)&lt;/span&gt; independently. The idea is to replace each default probability &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt; by some other default probability &lt;span class=&#34;math inline&#34;&gt;\(q_k\)&lt;/span&gt;, the basic IS identity is
&lt;span class=&#34;math display&#34;&gt;\[P(L&amp;gt;x)= \tilde{E}\left[1\{L&amp;gt;x\}\prod_{k=1}^m\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}\right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exponential Twisting&lt;/strong&gt;: Glasserman and Li (2005) chooses
&lt;span class=&#34;math display&#34;&gt;\[q_{k,\theta} = \frac{p_ke^{\theta c_k}}{1+p_k(e^{\theta c_k}-1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The original probabilities correspond to &lt;span class=&#34;math inline&#34;&gt;\(\theta=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;gt;0\)&lt;/span&gt;, this does indeed increase the default
probabilities; a larger exposure &lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt; results in a greater
increase in the default probability.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-optimal-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the optimal parameter&lt;/h2&gt;
&lt;p&gt;The LR is reduced to
&lt;span class=&#34;math display&#34;&gt;\[\prod_{k=1}^m\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}=\exp(-\theta L+\psi(\theta))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\psi(\theta)=\log E[e^{\theta L}]=\sum_{k=1}^m \log(1+p_k(e^{\theta c_k}-1))\]&lt;/span&gt;
is the cumulant generating function (CGF) of L.&lt;/p&gt;
&lt;p&gt;The optimal parameter is
&lt;span class=&#34;math display&#34;&gt;\[\theta^* = \arg \min_{\theta\ge 0} \{M_2(\theta)=E_\theta[1\{L&amp;gt;x\}e^{-2\theta L+2\psi(\theta)}]\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-sub-optimal-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the sub-optimal parameter&lt;/h2&gt;
&lt;p&gt;Observe that for &lt;span class=&#34;math inline&#34;&gt;\(\theta\ge 0\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[M_2(\theta)\le e^{-2\theta x+2\psi(\theta)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing &lt;span class=&#34;math inline&#34;&gt;\(M_2(\theta)\)&lt;/span&gt; is difficult, but minimizing
the upper bound is easy:
&lt;span class=&#34;math display&#34;&gt;\[\theta_x = \arg \min_{\theta\ge 0}e^{-2\theta x+2\psi(\theta)}=\arg \max_{\theta\ge 0} \{\theta x-\psi(\theta)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;span class=&#34;math inline&#34;&gt;\(\psi(\theta)\)&lt;/span&gt; is strictly convex and passes through the origin, so the maximum
is attained at
&lt;span class=&#34;math display&#34;&gt;\[\theta_x = 
\begin{cases}
\text{unique solution to }\psi&amp;#39;(\theta)=x,\ &amp;amp;x&amp;gt;\psi&amp;#39;(0)\\
0,\ &amp;amp;x\le \psi&amp;#39;(0).
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the first case, &lt;span class=&#34;math inline&#34;&gt;\(E_{\theta_x}[L]=\psi&amp;#39;(\theta_x)=x\)&lt;/span&gt;, thus, we have shifted the distribution of L so that x is now its mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the second case, the event &lt;span class=&#34;math inline&#34;&gt;\(\{L&amp;gt;x\}\)&lt;/span&gt; is not rare, so we do not change the probabilities.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;dependent-obligors-conditional-importance-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dependent Obligors: Conditional Importance Sampling&lt;/h2&gt;
&lt;p&gt;For general factor models, &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; are dependent; but they are independent conditinal on the systematic risk factors &lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\)&lt;/span&gt;. So we can apply the so-called conditional IS.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\sim N(0,1)\)&lt;/span&gt; and compute the default probability
&lt;span class=&#34;math inline&#34;&gt;\(p_k=p_k(Z_1,\dots,Z_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: for simulated &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt;, obtain the twisting parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_x=\theta_x(Z_1,\dots,Z_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: compute the LR for the &lt;span class=&#34;math inline&#34;&gt;\(\theta_x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 4: repeat Steps 1â€“4 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; times and then obtain the final IS estimate&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numerical results&lt;/h2&gt;
&lt;p&gt;The numerical results were reported in Glasserman and Li (2005).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(21\)&lt;/span&gt;-factor model with &lt;span class=&#34;math inline&#34;&gt;\(m=1000\)&lt;/span&gt; obligors&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_k = 0.01(1+\sin(16\pi k/m))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_k=(\lceil5k/m\rceil)^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;VRF = â€œVariance Reduction Factorâ€&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(L&amp;gt;x)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;VRF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;10,000&lt;/td&gt;
&lt;td&gt;0.0114&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;14,000&lt;/td&gt;
&lt;td&gt;0.0065&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;18,000&lt;/td&gt;
&lt;td&gt;0.0037&lt;/td&gt;
&lt;td&gt;83&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;22,000&lt;/td&gt;
&lt;td&gt;0.0021&lt;/td&gt;
&lt;td&gt;125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30,000&lt;/td&gt;
&lt;td&gt;0.0006&lt;/td&gt;
&lt;td&gt;278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;40,000&lt;/td&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;td&gt;977&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;The defual indicators&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_k=1\{X_k&amp;gt;x_k\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; follow t copula model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Joshua C.C. Chan, Dirk P. Kroese. Efficient estimation of large portfolio loss probabilities in t-copula models. &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, 205:361â€“367, 2010.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; follow another advanced models, e.g., self-exciting model, Giesecke et al.Â (2010)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Random default exposures: &lt;span class=&#34;math inline&#34;&gt;\(c_k=e_k\ell_k\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\ell_k\in[0,1]\)&lt;/span&gt; denotes a random
percentage loss, and &lt;span class=&#34;math inline&#34;&gt;\(e_k&amp;gt;0\)&lt;/span&gt; are constants.
&lt;span class=&#34;math display&#34;&gt;\[L = \sum_{k=1}^m e_k\ell_k1\{X_k&amp;gt;x_k\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell_k\)&lt;/span&gt; are iid truncated normals or betas&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-entropy&lt;/h2&gt;
&lt;p&gt;The optimal proposal
density is obtained by locating the member &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta),\theta\in\Theta\)&lt;/span&gt; that minimizes
its cross-entropy distance to the zero-variance proposal
density &lt;span class=&#34;math inline&#34;&gt;\(q^*(x)\propto h(x)p(x;\theta_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The minimization of the cross-entropy is equivalent to solving
the following maximization problem
&lt;span class=&#34;math display&#34;&gt;\[\max_{\theta\in\Theta} \int h(x)p(x;\theta_0)\log p(x;\theta)d x=\max_{\theta\in\Theta}  E_{\theta_0}[h(X)\log p(X;\theta)]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since most often an analytical solution to the above maximization
problem is not available, we consider instead its stochastic
counterpart
&lt;span class=&#34;math display&#34;&gt;\[\theta^*=\arg \max_{\theta\in\Theta}\frac 1{N_0}\sum_{i=1}^{N_0}h(X_i)\log p(X_i;\theta),\ X_i\stackrel{iid}{\sim} p(x;\theta_0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More detials see Rubinstein (1997), Rubinstein &amp;amp; Kroese (2004).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬11ç« </title>
      <link>/en/courses/bayes/chap11/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap11/</guid>
      <description>


&lt;div id=&#34;markov-chains&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Markov chains&lt;/h2&gt;
&lt;p&gt;Consider the discrete chain:
&lt;span class=&#34;math display&#34;&gt;\[P(X_i\in A|X_0=x_0,\dots,X_{i-1}=x_{i-1})=P(X_i\in A|X_{i-1}=x_{i-1})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_i\in\Omega=\{\omega_1,\dots,\omega_M\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Transition distribution:
&lt;span class=&#34;math display&#34;&gt;\[P(X_i=y|X_{i-1}=x)=T_i(y|x)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distribution of this chain is now determined by &lt;span class=&#34;math inline&#34;&gt;\(p_0(x)=P(X_0=x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_i(y|x)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Homogeneous chain:
&lt;span class=&#34;math display&#34;&gt;\[P(X_i=y|X_{i-1}=x)=P(X_1=y|X_0=x)=T(y|x)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;where-does-this-chain-go&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where does this chain go?&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{i}(\omega_k) = P(X_i=\omega_k) = \sum_{j=1}^Mp_{i-1}(\omega_j)T(\omega_k|\omega_j)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{p}_i=(p_i(\omega_1),\dots,p_i(\omega_M))\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is the transition matrix with entries &lt;span class=&#34;math inline&#34;&gt;\(P_{ij}=P(\omega_j|\omega_i)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{p}_i = \boldsymbol{p}_{i-1} P\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{p}_i = \boldsymbol{p}_{0} P^n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-a-portion-of-the-montrÃ©al-mÃ©tro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: A portion of the MontrÃ©al mÃ©tro&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/metro.png&#34; width=&#34;65%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transition-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transition matrix&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/transition.png&#34; width=&#34;65%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;after-100-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;After 100 steps&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/100step.png&#34; width=&#34;65%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;No matter where you start &lt;span class=&#34;math inline&#34;&gt;\(p_{100}(\text{Berri})\doteq 0.31\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;These are almost IID from the &lt;strong&gt;stationary distribution&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationary-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationary distribution&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi = \pi P \text{ so } \pi^\top = P^\top\pi^\top\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi^{\top}\)&lt;/span&gt; is an eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(P^\top\)&lt;/span&gt; with eigenvalue 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;law-of-large-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Law of large numbers&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; be a time-homogenous Markov chain on a finite set &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is &lt;strong&gt;irreducible&lt;/strong&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\omega_0}\left(\lim_{n\to\infty}\frac 1n\sum_{i=1}f(X_i)=\sum_{\omega\in\Omega}\pi(\omega)f(\omega)\right)=1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Irreducibility: &lt;span class=&#34;math inline&#34;&gt;\(P_x(\tau_y&amp;lt;\infty)&amp;gt;0\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(x,y\in \Omega\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\tau_y\)&lt;/span&gt; is the first time &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is visited, i.e., &lt;span class=&#34;math display&#34;&gt;\[\tau_y:=\inf\{i\ge 1:X_i=y,X_0=x\}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-we-will-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What we will do&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; we will find a transition matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\pi P=\pi\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then sample &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; via &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is what could go wrong:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It might take a long time before &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{p}_n\approx \pi\)&lt;/span&gt; (slow convergence)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt; might get stuck for a long time (slow mixing)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to find the good one for &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;detailed-balance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Detailed balance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stationarity balances flow into &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with flow out of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sum_{x\in\Omega} \pi(x)P(y|x)=\pi(y) = \sum_{x\in\Omega}\pi(y)P(x|y)\]&lt;/span&gt;
NB: &lt;span class=&#34;math inline&#34;&gt;\(\pi = \pi P\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Detailed balance is stronger:
&lt;span class=&#34;math display&#34;&gt;\[\pi(x)P(y|x)=\pi(y)P(x|y),\forall x,y\in\Omega\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;detailed balance &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; balance&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-road-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The road map&lt;/h2&gt;
&lt;p&gt;The goal is to build a Markov chain with a unique stationary distribution which equals the targe distribution.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;build a Markov chain with a unique stationary distribution. This holds if the Markov chian is irreducible, aperiodic, and not transient. E.g., random walk has a positive probablility of eventually reaching any state from any other state.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;stationary distribution = the targe distribution (detailed balance transition)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-metropolis-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Metropolis algorithm&lt;/h2&gt;
&lt;p&gt;The Metropolis algorithm (Metropolis et al.Â 1953) is an adaptation of a random walk with an acceptance/rejection rule to converge to the specified target distribution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;target distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;symmetric proposal distribution at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(q(y|x)=q(x|y)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt;, draw a starting poing &lt;span class=&#34;math inline&#34;&gt;\(x_0\sim p_0(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(t=1,2,\dots\)&lt;/span&gt;, sample &lt;span class=&#34;math inline&#34;&gt;\(y_t\sim q(y_t|x_{t-1})\)&lt;/span&gt;, accept &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; as an output of &lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt; with probability
&lt;span class=&#34;math display&#34;&gt;\[r(x_{t-1},y_t)=\min\left(\frac{p(y_t)}{p(x_{t-1})},1\right)\]&lt;/span&gt;
Otherwise, taking &lt;span class=&#34;math inline&#34;&gt;\(x_{t}=x_{t-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-transition-for-the-metropolis-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The transition for the Metropolis algorithm&lt;/h2&gt;
&lt;p&gt;The transition is a mixture of a point and a proposal distribution.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T(y|x) = r(x,y)q(y|x)+\left[1-\int r(x,y) q(y|x)dy\right]1\{y=x\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r(x,y)=\min\left(\frac{p(y)}{p(x)},1\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detailed balance&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(x)T(y|x)=p(y)T(x|y)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(x\neq y\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{p(x)T(y|x)}{p(y)T(x|y)}=\frac{p(x)r(x,y)}{p(y)r(y,x)}\frac{q(y|x)}{q(x|y)}=\frac{p(x)\min(p(y)/p(x),1)}{p(y)\min(p(x)/p(y),1)}=1\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-walk-metropolis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random walk Metropolis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the proposal density:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{t+1}=x_t+ N(0,\sigma^2I_d)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{t+1}=x_t+ U[-\sigma,\sigma]^d\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How large a step &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tiny step: large &lt;span class=&#34;math inline&#34;&gt;\(p(y_{t+1})/p(x_t)\)&lt;/span&gt;, high acceptance&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Large step: small &lt;span class=&#34;math inline&#34;&gt;\(p(y_{t+1})/p(x_t)\)&lt;/span&gt;, low acceptance&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We might have wanted high acceptance and large moves. But thereâ€™s a tradeoff.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The rule of thumb&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sigma=2.38/\sqrt{d}\)&lt;/span&gt;, see Gelman, Roberts, Gilks (1996)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improvements-on-rwm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improvements on RWM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the target distribution &lt;span class=&#34;math inline&#34;&gt;\(p\approx N(\mu,\Sigma)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal: &lt;span class=&#34;math inline&#34;&gt;\(y_{t+1}\sim N(x_{t},\lambda\hat{\Sigma})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use sample &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to estimate &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;a tune parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-bivariate-unit-normal-with-normal-proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Bivariate unit normal with normal proposal&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/metropolis.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;target distribution: bivariate unit normal &lt;span class=&#34;math inline&#34;&gt;\(N(0,I_2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;symmetric proposal distribution: &lt;span class=&#34;math inline&#34;&gt;\(q(y|x)=N(y|x,0.2^2I_2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-metropolis-hastings-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Metropolis-Hastings algorithm&lt;/h2&gt;
&lt;p&gt;The Metropolis-Hastings (MH) algorithm (Hastings, 1970) generalizes the basic Metropolis algorithm. The proposal needs no longer be symmetric.&lt;/p&gt;
&lt;p&gt;The detailed balance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(x)r(x,y)q(y|x)=p(y)r(y,x)q(x|y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How to choose &lt;span class=&#34;math inline&#34;&gt;\(r(x,y)\)&lt;/span&gt; subject to &lt;span class=&#34;math inline&#34;&gt;\(0\le r(x,y)\le 1\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The result:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r(x,y)=\min\left(\frac{p(y)q(x|y)}{p(x)q(y|x)},1\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-independent-mh-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The independent MH algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the proposal: &lt;span class=&#34;math inline&#34;&gt;\(q(y|x)=q(y)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r(x,y)=\min\left(\frac{p(y)q(x)}{p(x)q(y)},1\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Although the proposal variates are iid, the states are not independent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What-if &lt;span class=&#34;math inline&#34;&gt;\(q(x)=p(x)\)&lt;/span&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;unnormalized-target-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unnormalized target density&lt;/h2&gt;
&lt;p&gt;The target density &lt;span class=&#34;math inline&#34;&gt;\(p(x)=C\tilde{p}(x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(C&amp;gt;0\)&lt;/span&gt; is unknown constant.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{p(y)q(x|y)}{p(x)q(y|x)} = \frac{C\tilde{p}(y)q(x|y)}{C\tilde{p}(x)q(y|x)}=\frac{\tilde{p}(y)q(x|y)}{\tilde{p}(x)q(y|x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The MH algorithm also works for unnormalized proposal density.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-the-expectation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating the expectation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The law of large numbers supports:
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu = \frac 1n\sum_{i=1}^n f(X_i)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Burn-in (skipping the first &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; observations, e.g., &lt;span class=&#34;math inline&#34;&gt;\(b=n/2\)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_b = \frac 1{n-b}\sum_{i=b+1}^n f(X_i)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thinning (just use every &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th observation, &lt;span class=&#34;math inline&#34;&gt;\(k&amp;gt;1\)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_k = \frac 1{n/k}\sum_{i=1}^{n/k} f(X_{ki})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See &lt;a href=&#34;https://doi.org/10.1080/10618600.2017.1336446&#34;&gt;Owen (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-gibbs-sampler&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Gibbs sampler&lt;/h2&gt;
&lt;p&gt;Suppose the targe distribution &lt;span class=&#34;math inline&#34;&gt;\(X=(x_1,\dots,x_d)\sim p(X)\)&lt;/span&gt;. Maybe we can sample one &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt; at a time, with others fixed, i.e., &lt;span class=&#34;math inline&#34;&gt;\(x_j|x_{-j}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(x_{-j}=(x_1,\dots,x_{j-1},x_{j+1},\dots,x_d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random scan Gibbs&lt;/strong&gt;: for &lt;span class=&#34;math inline&#34;&gt;\(t=1,\dots,n\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(j\sim U\{1,\dots,d\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{-j}^{(t)}=x_{-j}^{(t-1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{j}^{(t)}\sim p(x_j|x_{-j}^{(t)})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Deterministic scan&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; cycles through &lt;span class=&#34;math inline&#34;&gt;\(1,\dots,d\)&lt;/span&gt; repeatedly, i.e., &lt;span class=&#34;math display&#34;&gt;\[j=1+(t-1) \mod d\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-step-of-gibbs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One step of Gibbs&lt;/h2&gt;
&lt;p&gt;Is a Metropolis-Hastings that always accepts, i.e., &lt;span class=&#34;math inline&#34;&gt;\(r(x,y)\equiv 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Proposal &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; just changes component &lt;span class=&#34;math inline&#34;&gt;\(x_j\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(x^{(t)}_j|x_{-j}^{(t-1)})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detailed balance&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{p(y)q(x|y)}{p(x)q(y|x)}=\frac{p(y_{-j})p(y_j|y_{-j})p(x_j|y_{-j})}{p(x_{-j})p(x_j|x_{-j})p(y_j|x_{-j})}=1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;NB: &lt;span class=&#34;math inline&#34;&gt;\(x_{-j}=y_{-j}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-bivariate-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Bivariate normal distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;target distribution: &lt;span class=&#34;math inline&#34;&gt;\((x_1,x_2)^\top\sim N(\mu,\Sigma)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\mu_2)^\top,\sigma_{11}=\sigma_{22}=1,\sigma_{12}=\sigma_{21}=\rho\in (-1,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;conditional distribution:
&lt;span class=&#34;math display&#34;&gt;\[x_1|x_2\sim N(\mu_1+\rho(x_2-\mu_2),1-\rho^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_2|x_1\sim N(\mu_2+\rho(x_1-\mu_1),1-\rho^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simulation&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/gibbs.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chanllenges-of-monitoring-convergence-mixing-and-stationarity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chanllenges of monitoring convergence: mixing and stationarity&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/convergence.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;did-the-chain-mix-well&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Did the chain mix well?&lt;/h2&gt;
&lt;p&gt;We can use the ACF or a trace.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;autocorrelation function&lt;/strong&gt; (ACF) is a measure of the correlation between observations of a time series that are separated by &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; time units.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/ACF.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recent promising work by Gorham &amp;amp; Mackey using &lt;strong&gt;Stein discrepancy&lt;/strong&gt; can
provide a â€œYesâ€ (but itâ€™s expensive).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-hierarchical-normal-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: hierarchical normal model&lt;/h2&gt;
&lt;p&gt;The data &lt;span class=&#34;math inline&#34;&gt;\(y_{ij},i=1,\dots,n_j,j=1,\dots,J\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/data_D.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Under the hierarchical normal model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij}\sim N(\theta_j,\sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_j\sim N(\mu,\tau^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Uniform prior distribution: &lt;span class=&#34;math inline&#34;&gt;\((\mu,\log \sigma,\tau)\propto 1\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[(\mu,\log \sigma,\log \tau)\propto \tau\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior distribution&lt;/h2&gt;
&lt;p&gt;The joint posterior density of all the parameters is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\log \sigma,\tau|y) \propto \tau\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\prod_{j=1}^J\prod_{i=1}^{n_j}N(y_{ij}|\theta_j,\sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional posterior distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\theta_j|\mu,\sigma,\tau,y\sim N(\hat{\theta}_j,V_{\theta_j})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu|\theta,\sigma,\tau,y\sim N(\hat{\mu},\tau^2/J),\ \hat{\mu}=\frac 1J\sum_{j=1}^J\theta_j\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2|\theta,\mu,\tau,y=\sigma^2|\theta,y\sim \mathrm{Inv-}\chi^2(n,\hat{\sigma}^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\tau^2|\theta,\mu,\sigma,y=\tau^2|\theta,\mu,y\sim \mathrm{Inv-}\chi^2(J-1,\hat{\tau}^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\sigma}^2 = \frac 1 n\sum_{j=1}^J\sum_{i=1}^{n_j}(y_{ij}-\theta_j)^2,\ \hat{\tau}^2=\frac{1}{J-1}\sum_{j=1}^J(\theta_j-\mu)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/normrel.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Starting pionts: &lt;span class=&#34;math inline&#34;&gt;\(\theta_j^{(0)}\sim U\{y_{ij},i=1,\dots,n_j\},\mu^{(0)}=\frac 1 J\sum_{j=1}^T \theta_j^{(0)},(\tau^2)^{(0)}\sim \mathrm{Inv-}\chi^2(J-1,\hat{\tau}^2), (\sigma^2)^{(0)}\sim \mathrm{Inv-}\chi^2(n,\hat{\sigma}^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The potential scale reduction &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt;, which declines to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to\infty\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬2ç« </title>
      <link>/en/courses/bayes/chap2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap2/</guid>
      <description>&lt;h2 id=&#34;3-steps-in-bda&#34;&gt;3 steps in BDA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;set up the statistical model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;compute the &lt;code&gt;posterior distribution&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;model checking and model improvement&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;statistical-inference&#34;&gt;Statistical inference&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: draw conclusions about &lt;strong&gt;unobserved quantities&lt;/strong&gt; from the data (observed)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;potentially observable quantities, e.g., future observations of a process&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;not directly observable quantities, e.g., unobservable population parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notations-and-assumptions&#34;&gt;Notations and assumptions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;unobservable population parameters of interest: $\theta=(\theta_1,\dots,\theta_m)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the observed data: $y=(y_1,\dots,y_n)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;potentially observable quantities: $\tilde y$, e.g., $\tilde y=y_{n+1}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exchangeability: the $n$ values $y_i$ are exchangeable, e.g., iid samples conditonal on the population parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conditional independence of $y$ and $\tilde y$ given $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bayesian-inference&#34;&gt;Bayesian inference&lt;/h2&gt;
&lt;p&gt;To make inferences about the posterior distributions, such as $p(\theta|y)$ and $p(\tilde y|y)$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes&amp;rsquo; rule&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$p(\theta|y)=\frac{p(\theta,y)}{p(y)}=\frac{p(y|\theta)p(\theta)}{p(y)}$$&lt;/p&gt;
&lt;p&gt;$$p(\theta|y)\propto  p(y|\theta)p(\theta)$$&lt;/p&gt;
&lt;p&gt;The imiplied constant is
$$p(y)=\int p(y|\theta)p(\theta) d \theta.$$&lt;/p&gt;
&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;
&lt;p&gt;To make inferences about an unknown observable quantity&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;prior predictive distribution: $p(y)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;posterior predictive dsitribution: $p(\tilde y|y)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
p(\tilde y|y) = \int p(\tilde y,\theta|y)d\theta = \int p(\tilde y|\theta,y)p(\theta|y)d \theta = \int p(\tilde y|\theta)p(\theta|y)d \theta
$$&lt;/p&gt;
&lt;p&gt;Again, $y$ and $\tilde y$ are conditionally independent given $\theta$.&lt;/p&gt;
&lt;h2 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;$p(y|\theta)$ is called the &lt;strong&gt;likelihood function&lt;/strong&gt;, which is regarded as a function of $\theta$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;odds ratios&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$\frac{p(\theta_1|y)}{p(\theta_2|y)}=\frac{p(\theta_1)p(y|\theta_1)/p(y)}{p(\theta_2)p(y|\theta_2)/p(y)}=\frac{p(\theta_1)}{p(\theta_2)}\frac{p(y|\theta_1)}{p(y|\theta_2)}$$&lt;/p&gt;
&lt;p&gt;posterior odds = prior odds $\times$ likelihood ratio&lt;/p&gt;
&lt;h2 id=&#34;example-1-inference-about-a-genetic-status&#34;&gt;Example 1: inference about a genetic status&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;males: one X-chromosome + one Y-chromosome&lt;/li&gt;
&lt;li&gt;females: two X-chromosomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hemophilia is a disease that exhibits X-chromosome-linked recessive inheritance.
The disease is generally fatal for women who inherit two such genes.&lt;/p&gt;
&lt;p&gt;Consider a woman who has an affected brother and her father is not affected.
Let $\theta$ be the state of the woman: a carrier of the gene ($\theta=1$) or not ($\theta=0$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;: $P(\theta=1)=P(\theta=0)=0.5$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data and model&lt;/strong&gt;: She has two sons. Let $y_i=1$ or 0 denote the state of her sons. Now observe that  her sons are not affected. Given $\theta$, $y_1$ and $y_2$ are iid.&lt;/p&gt;
&lt;h2 id=&#34;example-1-inference-about-a-genetic-status-1&#34;&gt;Example 1: inference about a genetic status&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$P(y_1=0,y_2=0|\theta=1)=0.5\times 0.5=0.25$$&lt;/p&gt;
&lt;p&gt;$$P(y_1=0,y_2=0|\theta=0)=1\times 1=1$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$P(\theta=1|y) = \frac{p(y|\theta=1)p(\theta=1)}{p(y)}=0.2$$&lt;/p&gt;
&lt;h2 id=&#34;example-1-inference-about-a-genetic-status-2&#34;&gt;Example 1: inference about a genetic status&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Adding more data&lt;/strong&gt;: suppose that the woman has a third son, who is also unaffacted.&lt;/p&gt;
&lt;p&gt;$$P(\theta=1|y_1,y_2,y_3) = \frac{0.5\times 0.2}{0.5\times 0.2+1\times 0.8}=0.111$$&lt;/p&gt;
&lt;p&gt;A key aspect of Bayesian analysis is the ease with which sequential analyses can be performed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: What happen if we suppose that the third son is affected?&lt;/p&gt;
&lt;h2 id=&#34;example-2-spelling-correction&#34;&gt;Example 2: spelling correction&lt;/h2&gt;
&lt;p&gt;Classification of words is a problem of managing uncertainty. Suppose someone types &lt;strong&gt;radom&lt;/strong&gt;. How should that be read?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;random&lt;/li&gt;
&lt;li&gt;radon&lt;/li&gt;
&lt;li&gt;radom&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data and model&lt;/strong&gt;: Let $\theta$ be the word that the person was intending to type, and let $y$ as the data. Now $y=$&#39;radom&amp;rsquo; and $\theta\in${$\theta_1$=&#39;random&amp;rsquo;,$\theta_2$=&#39;radon&amp;rsquo;,$\theta_3$=&#39;radom&amp;rsquo;}. The posterior density is&lt;/p&gt;
&lt;p&gt;$$P(\theta|y=\text{&amp;lsquo;radom&amp;rsquo;})\propto p(\theta)P(y=\text{&amp;lsquo;radom&amp;rsquo;}|\theta).$$&lt;/p&gt;
&lt;h2 id=&#34;example-2-spelling-correction-1&#34;&gt;Example 2: spelling correction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;: Here are probabilities supplied by researchers at Google.
Goole Ngram Viewer: 
&lt;a href=&#34;https://books.google.com/ngrams&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://books.google.com/ngrams&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$\theta$&lt;/th&gt;
&lt;th&gt;$p(\theta)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;random&lt;/td&gt;
&lt;td&gt;$7.60\times 10^{-5}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;radon&lt;/td&gt;
&lt;td&gt;$6.05\times 10^{-6}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;radom&lt;/td&gt;
&lt;td&gt;$3.12\times 10^{-7}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;: Here are some conditional probabilities from Google&amp;rsquo;s model of spelling and typing errors:&lt;/p&gt;
&lt;p&gt;$\theta$ | $p(\text{&amp;lsquo;radom&amp;rsquo;}|\theta)$ |
-|-|
random | $0.00193$ |
radon  | $0.000143$ |
radom  | $0.975$ |&lt;/p&gt;
&lt;h2 id=&#34;example-2-spelling-correction-2&#34;&gt;Example 2: spelling correction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$\theta$ | $p(\theta)P(y=\text{&amp;lsquo;radom&amp;rsquo;}|\theta)$ | $P(\theta|y=\text{&amp;lsquo;radom&amp;rsquo;})$ |
-|-|-|
random | $1.47\times 10^{-7}$  | 0.325 |
radon  | $8.65\times 10^{-10}$ | 0.002 |
radom  | $3.04\times 10^{-7}$  | &lt;strong&gt;0.673&lt;/strong&gt; |&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model improvement&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;including contextual info in the prior probabilities, e.g., statistical book.&lt;/li&gt;
&lt;li&gt;let $x$ be the contextual information used by the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$p(\theta|x,y)\propto p(\theta|x)p(y|\theta,x)$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for simplicity, we may assume $p(y|\theta,x)=p(y|\theta)$.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬3ç« </title>
      <link>/en/courses/bayes/chap3/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap3/</guid>
      <description>


&lt;div id=&#34;binomial-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Binomial models&lt;/h1&gt;
&lt;div id=&#34;estimating-a-probability-from-binomial-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating a probability from binomial data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the proportion of successes in the population&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the data &lt;span class=&#34;math inline&#34;&gt;\((y_1,\dots,y_n)\in \{0,1\}^n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the total number of successes in the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; trials is denoted by &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the binomial model is
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = C_n^y\theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y) \propto p(\theta)p(y|\theta)\propto p(\theta)\theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: estimating the probability of a female birth&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-choose-a-proper-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to choose a proper prior?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A naive choice for &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is uniform on the interval &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;. Then
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y) \propto \theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;that is, &lt;span class=&#34;math inline&#34;&gt;\(\theta|y\sim Beta(y+1,n-y+1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/zh/courses/Bayes/chap3_files/figure-html/beta-1.png&#34; width=&#34;60%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(P(\theta\ge 0.5|y=241945,n=241945+251527)\approx 1.15\times 10^{-42}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt; be the result of a new trial
&lt;span class=&#34;math display&#34;&gt;\[P(\tilde y =1|y) = \int_0^1 P(\tilde y=1|\theta,y)p(\theta|y)d \theta=E[\theta|y]=\frac{y+1}{n+2}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Posterior as compromise between data and prior information&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prior mean is &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;sample mean is &lt;span class=&#34;math inline&#34;&gt;\(y/n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior mean is &lt;span class=&#34;math inline&#34;&gt;\((y+1)/(n+2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the compromise is controlled to a greater extent by the data as the sample size
increases.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-quantiles-and-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior quantiles and intervals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(T_1\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(\alpha/2\)&lt;/span&gt; quantile of the posterior distribution&lt;/li&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(T_2\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(1-\alpha/2\)&lt;/span&gt; quantile of the posterior distribution&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; posterior interval is &lt;span class=&#34;math inline&#34;&gt;\([T_1,T_2]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/binterval.png&#34; width=&#34;65%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compare with the usual confidence interval&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;informative-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Informative prior distributions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: assigning a prior distribution that reflects substantive info.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the likelihood is
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) \propto \theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;choose a prior as a &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt; distribution:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta&amp;gt;0\)&lt;/span&gt; of the prior distribution is called &lt;em&gt;hyperparameters&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto \theta^{\alpha+y-1}(1-\theta)^{n-y+\beta-1}=Beta(\alpha+y,\beta+n-y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;informative-prior-distributions-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Informative prior distributions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the posterior mean is
&lt;span class=&#34;math display&#34;&gt;\[E[\theta|y]=\frac{\alpha+y}{\alpha+\beta+n}\]&lt;/span&gt;
which lies between the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(y/n\)&lt;/span&gt; and the prior mean &lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior variance is
&lt;span class=&#34;math display&#34;&gt;\[Var[\theta|y]=\frac{(\alpha+y)(\beta+n-y)}{(\alpha+\beta+n)^2(\alpha+\beta+n+1)}=\frac{E[\theta|y](1-E[\theta|y])}{\alpha+\beta+n+1}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;as &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; become large with fixed &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[E[\theta|y]\approx \frac yn,\ Var[\theta|y]\approx \frac 1n\frac yn(1-\frac yn).\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conjugate-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conjugate prior distributions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; is a class of sampling distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt; is a class of prior distributions for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, then the class &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt; is &lt;em&gt;conjugate&lt;/em&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; if
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\in \mathcal{P} \text{ for all } p(\cdot|\theta)\in\mathcal{F} \text{ and }p(\cdot)\in\mathcal{P}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages of conjugate prior distributions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computational convenience&lt;/li&gt;
&lt;li&gt;can be interpreted as additional data&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-families&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential families&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: The class &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; is an &lt;em&gt;exponential family&lt;/em&gt; if all its members have the form
&lt;span class=&#34;math display&#34;&gt;\[p(y_i|\theta)=f(y_i)g(\theta)\exp[\phi(\theta)^\top u(y_i)].\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\ge 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\phi(\theta)\)&lt;/span&gt; is called the &lt;code&gt;natural parameter&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For iid samples, we have
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)=\left(\prod_{i=1}^n f(y_i)\right)g(\theta)^n\exp\left[\phi(\theta)^\top \sum_{i=1}^nu(y_i)\right]
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)\propto g(\theta)^n\exp[\phi(\theta)^\top t(y)]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t(y)=\sum_{i=1}^nu(y_i)\)&lt;/span&gt; (i.e., a &lt;em&gt;sufficient statistic&lt;/em&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conjugate-prior-distribution-for-exponential-families&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conjugate prior distribution for exponential families&lt;/h2&gt;
&lt;p&gt;If the prior distribution is specified as
&lt;span class=&#34;math display&#34;&gt;\[p(\theta)\propto g(\theta)^\eta \exp[\phi(\theta)^\top \nu],\]&lt;/span&gt;
then the posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto g(\theta)^{\eta+n} \exp[\phi(\theta)^\top (\nu+t(y))].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A list of exponential families&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;binomial distributions&lt;/li&gt;
&lt;li&gt;normal distributions&lt;/li&gt;
&lt;li&gt;exponential distributions&lt;/li&gt;
&lt;li&gt;possion distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-probability-of-a-girl-birth-given-placenta-previa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Probability of a girl birth given placenta previa&lt;/h2&gt;
&lt;p&gt;An early study concerning the sex of placenta previa births in Germany found that of a total of 980 births, 437 were female.&lt;/p&gt;
&lt;p&gt;How much evidence does this provide for the claim that the proportion of female births in the population of placenta previa births is less than &lt;strong&gt;0.485&lt;/strong&gt;, the proportion of female births in the general population?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;using a uniform prior: the posterior is &lt;span class=&#34;math inline&#34;&gt;\(Beta(438,544)\)&lt;/span&gt;. The central &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior interval is &lt;span class=&#34;math inline&#34;&gt;\([0.415,0.477]\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;using conjugate prior &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;using nonconjugate prior&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;different-conjugate-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different conjugate prior distributions&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha+\beta\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;posterior median&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior interval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;0.447&lt;/td&gt;
&lt;td&gt;[0.416, 0.478]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;0.450&lt;/td&gt;
&lt;td&gt;[0.420, 0.479]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;0.453&lt;/td&gt;
&lt;td&gt;[0.424, 0.481]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Posterior inferences based on a large sample are not sensitive to the prior distribution.&lt;/li&gt;
&lt;li&gt;All the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior intervals exclude the prior mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effect-of-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The effect of prior distributions&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/bprior.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-nonconjugate-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using a nonconjugate prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/nonconjugate.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior interval is [0.419, 0.480]&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Normal models&lt;/h1&gt;
&lt;div id=&#34;estimating-a-normal-mean-with-known-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating a normal mean with known variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = \prod_{i=1}^n \frac 1{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\theta)^2}{2\sigma^2}}\propto e^{-\frac{n\theta^2}{2\sigma^2}}e^{\frac{n\theta\bar y}{\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conjugate prior&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim N(\mu_0,\tau_0^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto e^{-\frac{n\theta^2}{2\sigma^2}}e^{\frac{n\theta\bar y}{\sigma^2}}e^{-\frac{\theta^2}{2\tau_0^2}}e^{\frac{\mu_0\theta}{\tau_0^2}}=N(\mu_n,\tau_n^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\mu_n=\frac{\frac 1{\tau_0^2}\mu_0+\frac n{\sigma^2}\bar y}{\frac 1{\tau_0^2}+\frac n{\sigma^2}},\ \frac1{\tau_n^2}=\frac{1}{\tau_0^2}+\frac n{\sigma^2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the inverse of the variance plays a prominet role and is called the &lt;em&gt;precision&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;posterior precision = prior precision + data precision&lt;/li&gt;
&lt;li&gt;the posterior mean is expressed as a weighted average of the prior mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt;, with weights proportional to the precisions.&lt;/li&gt;
&lt;li&gt;what happens if &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\tau_0^2\)&lt;/span&gt; fixed? data info. dominated!&lt;/li&gt;
&lt;li&gt;what happens if &lt;span class=&#34;math inline&#34;&gt;\(\tau_0\to \infty\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; fixed? This would result from assuming &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is proportional to a constant for &lt;span class=&#34;math inline&#34;&gt;\(\theta\in(-\infty,\infty)\)&lt;/span&gt;. (improper prior, serves as an noninformative prior)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution-with-known-mean-but-unknown-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal distribution with known mean but unknown variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\sigma^2)=\prod_{i=1}^n \frac 1{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}}\propto \sigma^{-n}\exp\left[-\frac{n}{2\sigma^2}\nu\right]\]&lt;/span&gt;
where the sufficient statistic is
&lt;span class=&#34;math display&#34;&gt;\[\nu=\frac 1n\sum_{i=1}^n(y_i-\mu)^2.\]&lt;/span&gt;
&lt;strong&gt;Conjugate prior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2)\propto (\sigma^2)^{-(\alpha+1)}e^{-\beta/\sigma^2},\]&lt;/span&gt;
where the hyperparameters is &lt;span class=&#34;math inline&#34;&gt;\((\alpha,\beta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We may take &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\sim \text{Inv-}\chi^2(\nu_0,\sigma^2_0)\)&lt;/span&gt; as a prior (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\stackrel d {=}\sigma_0^2\nu_0/\chi^2_{\nu_0}\)&lt;/span&gt;), whose PDF is given by
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2) =\frac{(\nu_0/2)^{\nu_0/2}}{\Gamma(\nu_0/2)}\sigma^{\nu_0}_0(\sigma^2)^{-(\nu_0/2+1)}e^{-\nu_0\sigma_0^2/(2\sigma^2)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution-with-known-mean-but-unknown-variance-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal distribution with known mean but unknown variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2)= \text{Inv-}\chi^2(\nu_0,\sigma^2_0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2|y)=\text{Inv-}\chi^2\left(\nu_0+n,\frac{\nu_0\sigma_0^2+n\nu}{\nu_0+n}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;degree of freedom = sum of the prior and data&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;scale = weighted average of the prior and data&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(\nu_0=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2|y)=\text{Inv-}\chi^2(n,\nu)\)&lt;/span&gt;, as effectively taking &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2)\propto 1/\sigma^2\)&lt;/span&gt; (improper prior, serves as an noninformative prior)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Poisson models&lt;/h1&gt;
&lt;div id=&#34;poisson-models-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;: The Possion distribution arises naturally in the study of data taking the form of counts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of customer on the queue over an unit time&lt;/li&gt;
&lt;li&gt;epidemiology â€“ the incidence of diseases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = \prod_{i=1}^n\frac{\theta^{y_i}e^{-\theta}}{y_i!}\propto \theta^{t(y)}e^{-n\theta}\propto e^{-n\theta}e^{t(y)\log \theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t(y)=\sum_{i=1}^n y_i\)&lt;/span&gt; is the sufficient statistic&lt;/li&gt;
&lt;li&gt;the natural parameter is &lt;span class=&#34;math inline&#34;&gt;\(\log \theta\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conjugate prior&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta)\propto e^{-\eta\theta}e^{\nu\log \theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we may choose &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\propto \theta^{\alpha-1}e^{-\beta\theta}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-models-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)=Gamma(\alpha+n\bar y,\beta+n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marginal density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y_i)=C_{\alpha+y_i-1}^{y_i} \left(\frac{\beta}{\beta+1}\right)^\alpha\left(\frac{1}{\beta+1}\right)^{y_i}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_i\sim \text{Neg-bin}(\alpha,\beta)\)&lt;/span&gt;, i.e., the negative binomial distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;possion-models-an-extension&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Possion models: an extension&lt;/h2&gt;
&lt;p&gt;In many applications, it is convenient to extend the Possion model for data pionts &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_n\)&lt;/span&gt; to the form
&lt;span class=&#34;math display&#34;&gt;\[y_i\sim Poission(x_i\theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the values &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are known positive values of an explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, called the &lt;em&gt;exposure&lt;/em&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th unit&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is unknown, called the &lt;em&gt;rate&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=Gamma(\alpha+\sum_{i=1}^ny_i,\beta+\sum_{i=1}^nx_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Bayesian inference for the cancer death rates (p.48)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exponential models&lt;/h1&gt;
&lt;div id=&#34;exponential-models-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;: The expoential distribution is commonly used to model â€˜waiting timesâ€™ and other continuous, poisitive, real-valued random variables. It has a â€˜memorylessâ€™ property that makes it a natural model for survival or lifetime data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = \prod_{i=1}^n\theta \exp(-y_i\theta)= \theta^{n}e^{-n\bar y \theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=Gamma(\alpha+n,\beta+n\bar y)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Population&lt;/th&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Conjugate prior&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Binomial&lt;/td&gt;
&lt;td&gt;probability of success&lt;/td&gt;
&lt;td&gt;Beta dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Possion&lt;/td&gt;
&lt;td&gt;mean&lt;/td&gt;
&lt;td&gt;Gamma dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Exponential&lt;/td&gt;
&lt;td&gt;inverse mean&lt;/td&gt;
&lt;td&gt;Gamma dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Normal (known variance)&lt;/td&gt;
&lt;td&gt;mean&lt;/td&gt;
&lt;td&gt;Normal dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Normal (known mean)&lt;/td&gt;
&lt;td&gt;variance&lt;/td&gt;
&lt;td&gt;Inv-Gamma dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;end-notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;End notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;two kinds of prior distributions: uniform (noninformative) and conjugate (informative)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;some other noninformative prior distributions: Jeffreysâ€™ prior etc. See pp.52-56&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;noninformative prior are often useful when it does not seem to be worth the effort to quantify oneâ€™s real prior knowledge as a probability distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when using conjugate prior, it remains to choose the hyperparameters; see Chapter 5 for hierarchical models&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬4ç« </title>
      <link>/en/courses/bayes/chap4/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap4/</guid>
      <description>


&lt;div id=&#34;nuisance-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nuisance parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;there are more than one unknown or unobservable parameters&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;conclusions will often be drawn about one, or only a few parameters at a time&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;there is no interest in making inferences about many of the unknown parameters â€“ &lt;em&gt;nuisance parameters&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;suppose &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\theta_1,\theta_2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interest centers only on &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt; is a â€˜nuisanceâ€™ parameter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1,\theta_2|y)\propto p(y|\theta_1,\theta_2)p(\theta_1,\theta_2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the marginal posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1|y)=\int p(\theta_1,\theta_2|y)d\theta_2=\int p(\theta_1|\theta_2,y)p(\theta_2|y)d\theta_2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-noninformative-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a noninformative prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\mu,\sigma^2)=\prod_{i=1}^n \frac 1{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\theta)^2}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Noninformative prior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\sigma^2)\propto 1\times (\sigma^2)^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\sigma^2|y)\propto \sigma^{-n-2}e^{-\frac{\sum_{i=1}^n(y_i-\theta)^2}{2\sigma^2}}=\sigma^{-n-2}e^{-\frac{(n-1)s^2+n(\bar y-\mu)^2}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac 1{n-1}\sum_{i=1}^n(y_i-\bar y)^2\)&lt;/span&gt; is the sample variance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-noninformative-prior-distribution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a noninformative prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Conditional posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu|\sigma^2,y)\sim N(\bar y,\sigma^2/n)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marginal posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2|y)\)&lt;/span&gt;&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2|y)\propto \int \sigma^{-n-2}e^{-\frac{(n-1)s^2+n(\bar y-\mu)^2}{2\sigma^2}} d\mu=(\sigma^2)^{-\frac{n+1}2}e^{-\frac{(n-1)s^2}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2|y\sim \text{Inv-}\chi^2(n-1,s^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-noninformative-prior-distribution-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a noninformative prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Marginal posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\mu|y)\)&lt;/span&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu|y)\propto \int_0^\infty \sigma^{-n-2}e^{-\frac{(n-1)s^2+n(\bar y-\mu)^2}{2\sigma^2}} d\sigma^2\propto \left[1+\frac{n(\mu-\bar y)^2}{(n-1)s^2}\right]^{-\frac n2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu|y\sim t_{n-1}(\bar y,s^2/n),\ \frac{\mu-\bar y}{s/\sqrt{n}}\Big|y\sim t_{n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior predictive distribution for a future observation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde y|y \sim t_{n-1}(\bar y,(1+1/n)s^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-estimating-the-speed-of-light&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Estimating the speed of light&lt;/h2&gt;
&lt;p&gt;Simon Newcomb set up an experiment in 1882 to measure the speed of light. Newcom measured the amount of time rquired for light to travel a distance of 7442 meters (66 measurements, from Stigler (1977), the data are recorded as deviations from 24800 nanoseconds).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=66,\ \bar y = 26.2,\ s = 10.8\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\mu-26.2)/(10.8/\sqrt{66})|y\sim t_{65}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; central posterior interval for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(26.2\pm 10.8t_{65,0.975}/\sqrt{66}=[23.6,28.8]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the speed of light is 299792458 m/s, so the true value for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(23.8\)&lt;/span&gt; nanoseconds&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-estimating-the-speed-of-light-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Estimating the speed of light&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figs/newcomb.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-conjugate-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a conjugate prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\mu|\sigma^2\sim N(\mu_0,\sigma^2/\kappa_0),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2\sim \text{Inv-}\chi^2(\nu_0,\sigma_0^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\sigma^2)\propto \sigma^{-1}(\sigma^2)^{-(\nu_0/2+1)}\exp\left(-\frac 1{2\sigma^2}[\nu_0\sigma^2+\kappa_0(\mu_0-\mu)^2]\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;denoted by &lt;span class=&#34;math inline&#34;&gt;\(\text{N-Inv-}\chi^2(\mu_0,\sigma^2_0/\kappa_0;\nu_0,\sigma_0^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-conjugate-prior-distribution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a conjugate prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu,\sigma^2|y\sim \text{N-Inv-}\chi^2(\mu_n,\sigma^2_n/\kappa_n;\nu_n,\sigma_n^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\mu_n &amp;amp;= \frac{\kappa_0}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar y\\
\kappa_n &amp;amp;= \kappa_0+n\\
\nu_n&amp;amp;=\nu_0+n\\
\nu_n\sigma_n^2 &amp;amp;= \nu_0\sigma_0^2+(n-1)s^2+\frac{\kappa_0n}{\kappa_0+n}(\bar y-\mu_0)^2
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu|\sigma^2,y\sim N(\mu_n,\sigma^2/\kappa_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2|y\sim \text{Inv-}\chi^2(\nu_n,\sigma_n^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu|y\sim t_{\nu_n}(\mu_n,\sigma_n^2/\kappa_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multinormal-model-for-categorical-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multinormal model for categorical data&lt;/h2&gt;
&lt;p&gt;The multinomial sampling distribution is used to describe data for which each observation is one of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; possible outcomes. If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the vector of counts of the number of observations of each outcome, then
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)\propto \prod_{j=1}^k\theta_j^{y_j},\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^k\theta_j=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conjugate prior&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\alpha)\propto \prod_{j=1}^k\theta_j^{\alpha_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dirichlet distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto \prod_{j=1}^k\theta_j^{y_j+\alpha_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-model-with-known-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate normal model with known variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y_1,\dots,y_n|\mu,\Sigma)\propto |\Sigma|^{-n/2}\exp\left(-\frac 12\sum_{i=1}^n(y_i-\mu)^\top\Sigma^{-1}(y_i-\mu)\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conjuate prior&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu\sim N(\mu_0,\Lambda_0)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu|y\sim N(\mu_n,\Lambda_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_n=(\Lambda_n^{-1}+n\Sigma^{-1})^{-1}(\Lambda_0^{-1}\mu_0+n\Sigma^{-1}\bar y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Lambda_n^{-1} = \Lambda_n^{-1}+n\Sigma^{-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-model-with-unknown-mean-and-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate normal model with unknown mean and variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;: the normal-inverse-Wishart &lt;span class=&#34;math inline&#34;&gt;\((\mu_0,\kappa_0;\nu_0,\Lambda_0)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Sigma\sim \text{Inv-Wishart}_{\nu_0}(\Lambda_0^{-1})\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu|\Sigma\sim N(\mu_0,\Sigma/\kappa_0)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\Sigma)\propto |\Sigma|^{-\frac{\nu_0+d}{2}-1}\exp\left(-\frac{1}{2}tr(\Lambda_0\Sigma^{-1})-\frac {\kappa_0}2(\mu-\mu_0)^\top\Sigma^{-1}(\mu-\mu_0)\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;: the normal-inverse-Wishart &lt;span class=&#34;math inline&#34;&gt;\((\mu_n,\kappa_n;\nu_0,\Lambda_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\mu_n &amp;amp;= \frac{\kappa_0}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar y\\
\kappa_n &amp;amp;= \kappa_0+n\\
\nu_n&amp;amp;=\nu_0+n\\
\Lambda_n &amp;amp;= \Lambda_0+\sum_{i=1}^n(y_i-\bar y)(y_i-\bar y)^\top+\frac{\kappa_0n}{\kappa_0+n}(\bar y-\mu_0)(\bar y-\mu_0)^\top
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-model-with-unknown-mean-and-variance-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate normal model with unknown mean and variance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Sigma|y\sim \text{Inv-Wishart}_{\nu_n}(\Lambda_n^{-1}),\ \mu|\Sigma,y\sim N(\mu_n,\Sigma/\kappa_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu|y \sim t_{\nu_n-d+1}(\mu_,\Lambda_n/(\kappa_n(\nu_n-d+1)))\)&lt;/span&gt; multivariate t distriution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\tilde y|y \sim t_{\nu_n-d+1}(\mu_,(k_n+1)\Lambda_n/(\kappa_n(\nu_n-d+1)))\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;wishart-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wishart distributions&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_i \stackrel {iid}\sim N_p(0, \Sigma)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\times p\)&lt;/span&gt; definite matrix, and &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\sum_{i=1}^n X_iX_i&amp;#39;\in R^{p\times p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is called the &lt;strong&gt;Wishart distribution&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; degree of freedom, denoted by &lt;span class=&#34;math inline&#34;&gt;\(\text{Wishart}_n(\Sigma)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(p=\Sigma = 1\)&lt;/span&gt;, then it is a chi-squared distribution with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; degrees of freedom.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(n \ge p\)&lt;/span&gt; the matrix &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is invertible with probability 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(X_i \stackrel {iid}\sim N_p(\mu, \Sigma)\)&lt;/span&gt;, then&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[(n-1)S^2 = \sum_{i=1}^n (X_i-\bar X)(X_i-\bar X)&amp;#39;\sim \text{Wishart}_{n-1}(\Sigma).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inverse-wishart-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inverse-Wishart distributions&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(W\sim \text{Wishart}_n(\Sigma)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\ge p\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(W^{-1}\sim \text{Inv-Wishart}_n(\Sigma^{-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬5ç« </title>
      <link>/en/courses/bayes/chap5/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap5/</guid>
      <description>


&lt;div id=&#34;large-sample-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Large-sample theory&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions and notations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;true distribution: &lt;span class=&#34;math inline&#34;&gt;\(y_i\stackrel {iid}{\sim} f(\cdot)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;model distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Kullback-Leibler divergence&lt;/em&gt;: a measure of â€˜discrepancyâ€™ between the model and the true distribution
&lt;span class=&#34;math display&#34;&gt;\[KL(\theta)= E_f\left[\log\left(\frac{f(y)}{p(y|\theta)}\right)\right]=\int \log\left(\frac{f(y)}{p(y|\theta)}\right)f(y)dy\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;: the &lt;strong&gt;unique minimizer&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(KL(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(f(y) = p(y|\theta)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;consistency-of-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Consistency of the posterior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Discrete parmeter space&lt;/strong&gt;: If the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is finite and &lt;span class=&#34;math inline&#34;&gt;\(P(\theta=\theta_0)&amp;gt;0\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[P(\theta=\theta_0|y)\to 1\text{ as }n\to \infty,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuous parmeter space&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is defined on a compace set &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a neighborhood of &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(P(\theta\in A)&amp;gt;0\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[P(\theta\in A|y)\to 1\text{ as }n\to \infty,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;See the proofs in Appendix B.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-approximations-to-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal approximations to the posterior distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;: the posterior mode&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Taylor series expansion of &lt;span class=&#34;math inline&#34;&gt;\(\log p(\theta|y)\)&lt;/span&gt; gives
&lt;span class=&#34;math display&#34;&gt;\[\log p(\theta|y) = \log p(\hat \theta|y)-\frac 12 (\theta-\hat\theta)^\top I(\hat \theta) (\theta-\hat\theta) + \cdots \]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt; is the &lt;em&gt;observed&lt;/em&gt; information
&lt;span class=&#34;math display&#34;&gt;\[I(\theta)=-\frac{d^2}{d\theta^2}\log p(\theta|y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normal approximation: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\approx N(\hat\theta,[I(\hat\theta)]^{-1})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fisher information&lt;/em&gt;:
&lt;span class=&#34;math display&#34;&gt;\[J(\theta)=-E_f\left[\frac{d^2}{d\theta^2}\log p(y_j|\theta)\right]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-of-the-posterior-distribution-to-normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence of the posterior distribution to normality&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under some regularity conditions (notably that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; not be on the boundary of &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;), as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;, the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; approaches normality with mean &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\([nJ(\theta_0)]^{-1}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; is the Fisher information.&lt;/p&gt;
&lt;p&gt;Oberved that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\to \theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(I(\hat\theta)=-\frac{d^2}{d\theta^2}\log p(\hat\theta)-\sum_{i=1}^n\frac{d^2}{d\theta^2}\log p(y_i|\hat\theta)\approx nJ(\theta_0)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(nJ(\theta_0)=\frac{d^2}{d\theta^2} KL(\theta_0)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NB: &lt;span class=&#34;math inline&#34;&gt;\(\frac{d^2}{d\theta^2} KL(\theta)= \frac{d^2}{d\theta^2} E_f\left[-\log p(y|\theta) \right]=-n\frac{d^2}{d\theta^2} E_f\left[\log p(y_i|\theta) \right]=-n E_f\left[\frac{d^2}{d\theta^2}\log p(y_i|\theta) \right]\)&lt;/span&gt; if the interchange of expectation and derivative is allowed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;counterexamples-to-the-theorems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counterexamples to the theorems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;underidentified models: &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is equal for a range of values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nonindentified parameters: for example, consider the model,
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{matrix}
u\\
v
\end{matrix}
\right)\sim N \left( \left(\begin{matrix}
0\\
0
\end{matrix}
\right),\left(\begin{matrix}
1&amp;amp;\rho\\
\rho &amp;amp; 1
\end{matrix}
\right)\right)\]&lt;/span&gt;
only one of &lt;span class=&#34;math inline&#34;&gt;\(u,v\)&lt;/span&gt; is observed from each pair &lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;number of parameters increasing with sample sizes: new latent parameters with each data point&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;point-estimation-consistency-and-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Point estimation, consistency, and efficiency&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;point estimations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior mode &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=\arg \max_{\theta\in\Theta} p(\theta|y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior mean &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=E[\theta|y]=\int \theta p(\theta|y)d \theta\)&lt;/span&gt; (the optimal one under the Bayesian decision rule)&lt;/li&gt;
&lt;li&gt;posterior median &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=F^{-1}_{\theta|y}(0.5)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;consistency&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)\to \theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;asymptotic unbiasedness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(E[\hat\theta|\theta_0]\to\theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;efficiency&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\text{eff}(\hat\theta)=\frac{\inf_T E[(T(y)-\theta_0)^2|\theta_0]}{E[(\hat\theta-\theta_0)^2|\theta_0]}\le 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;asymptotically efficient&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\text{eff}(\hat\theta)\to 1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ç¬¬6ç« </title>
      <link>/en/courses/bayes/chap6/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/en/courses/bayes/chap6/</guid>
      <description>


&lt;div id=&#34;introduction-to-hierarchial-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to hierarchial models&lt;/h2&gt;
&lt;p&gt;Many statistical applications involve multiple parameters (say, &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\dots,\theta_J\)&lt;/span&gt;) that can be regarded as related or connected in some way by the structure of the problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the group &lt;span class=&#34;math inline&#34;&gt;\(j\in 1{:}J\)&lt;/span&gt;, we have the observed data &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,n_j\)&lt;/span&gt; from the population distribution with unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we use a prior distribution in which the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;â€™s are viewed as a sample from a common &lt;em&gt;population distribution&lt;/em&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|\phi)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is known as &lt;em&gt;hyperparameters&lt;/em&gt;. Assume that &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are iid, i.e.,
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\phi)=\prod_{j=1}^Jp(\theta_j|\phi)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-for-rats-experiment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model for Rats experiment&lt;/h2&gt;
&lt;p&gt;The experiment is used to estimate the probability &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; of tumor in a population of female laboratory rats of type â€˜F344â€™ that receive a zero dose of the drug. The data show that 4 out of 14 rats developed a kind of tumor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assume a binomial model for the number of tumors&lt;/li&gt;
&lt;li&gt;select a prior from the conjugate family, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim Beta(\alpha,\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the posterior is therefore &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha+1,\beta+10)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question is how to determine the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\phi=(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;historical data are available on previous experiments on similar groups of rats: in the jth historical experiments, let the number of rats with tumors be &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and the total number of rats be &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt;, the parameters for the populations are &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(j=1,\dots,70\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;for current experiment, let &lt;span class=&#34;math inline&#34;&gt;\(y_{71},n_{71},\theta_{71}\)&lt;/span&gt; be the associated notations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;historical-data-for-the-70-historical-experiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Historical data for the 70 historical experiments&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  2
## [24]  2  2  2  2  2  2  2  2  1  5  2  5  3  2  7  7  3  3  2  9 10  4  4
## [47]  4  4  4  4  4 10  4  4  4  5 11 12  5  5  6  5  6  6  6  6 16 15 15
## [70]  9  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 20 20 20 20 20 20 20 19 19 19 19 18 18 17 20 20 20 20 19 19 18 18 25
## [24] 24 23 20 20 20 20 20 20 10 49 19 46 27 17 49 47 20 20 13 48 50 20 20
## [47] 20 20 20 20 20 48 19 19 19 22 46 49 20 20 23 19 22 20 20 20 52 46 47
## [70] 24 14&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;viewed-as-separate-models-using-uniform-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewed as separate models using uniform priors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/separate_model.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viewed-as-a-pooled-model-using-uniform-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewed as a pooled model using uniform prior&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/pool_model.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-historical-data-to-estimate-the-hyperparameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the historical data to estimate the hyperparameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the sample mean and standard deviation of the 70 values &lt;span class=&#34;math inline&#34;&gt;\(y_i/n_i\)&lt;/span&gt; are 0.136 and 0.103&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(E[\theta]=\frac{\alpha}{\alpha+\beta}=0.136\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var[\theta]=\frac{E[\theta](1-E[\theta])}{\alpha+\beta+1}=0.103\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\alpha}=1.4,\ \hat{\beta}=8.6\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the current exeriment, the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Beta(5.4,18.6)\)&lt;/span&gt;, posterior mean is &lt;span class=&#34;math inline&#34;&gt;\(0.223\)&lt;/span&gt;, standard deviation is 0.083.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several logical and practical problems with the approach of directly estimating a prior distribution from existing data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the data will be used twice for inference about the first 70 experiments â€“ overestimate our precision&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the point estimate for &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta\)&lt;/span&gt; seems arbitrary that necessarily ignores some posterior uncertainty&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;this is not the logic of Bayesian inference&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-full-bayesian-treatment-of-the-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The full Bayesian treatment of the hierarchical model&lt;/h2&gt;
&lt;p&gt;Suppose the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; has its own prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\phi)\)&lt;/span&gt;, which is called &lt;em&gt;hyperprior distribution&lt;/em&gt;. The appropriate Bayesian posterior distribution is of the vector &lt;span class=&#34;math inline&#34;&gt;\((\phi,\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the joint prior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta)=p(\phi)p(\theta|\phi)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta|y)\propto p(\phi,\theta)p(y|\phi,\theta)=p(\phi)p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previously, we assumed &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; was known, which is unrealistic; now we include the uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fully-bayesian-analysis-of-conjugate-hierarchical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fully Bayesian analysis of conjugate hierarchical models&lt;/h2&gt;
&lt;p&gt;Consider the setting in which &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|\phi)\)&lt;/span&gt; is conjugate to the likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;. For this case, it is easy to determine analytically &lt;span class=&#34;math display&#34;&gt;\[p(\theta|\phi,y)\propto p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the joint posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta|y)\propto p(\phi)p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the marginal posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\phi|y)\)&lt;/span&gt; can be computed via
&lt;span class=&#34;math display&#34;&gt;\[p(\phi|y)=\int p(\phi,\theta|y)d \theta\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{or }p(\phi|y)=\frac{p(\phi,\theta|y)}{p(\theta|\phi,y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-model-for-rat-tumors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to the model for rat tumors&lt;/h2&gt;
&lt;p&gt;The binomial model:
&lt;span class=&#34;math display&#34;&gt;\[y_j\sim Bin(n_j,\theta_j),\ j=1,\dots,J=71\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are assumed to be independent samples from a beta distribution:
&lt;span class=&#34;math display&#34;&gt;\[\theta_j\sim Beta(\alpha,\beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The joint posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\alpha,\beta|y)\propto p(\alpha,\beta)p(\theta|\alpha,\beta)p(y|\theta)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\propto p(\alpha,\beta)\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha-1}(1-\theta_j)^{\beta-1}\prod_{j=1}^J\theta_j^{y_j}(1-\theta_j)^{n_j-y_j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\alpha,\beta,y)=\prod_{j=1}^J\frac{\Gamma(\alpha+\beta+n_j)}{\Gamma(\alpha+y_j)\Gamma(\beta+n_j-y_j)}\theta_j^{\alpha+y_i-1}(1-\theta_j)^{\beta+n_j-y_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-model-for-rat-tumors-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to the model for rat tumors&lt;/h2&gt;
&lt;p&gt;The marginal posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha,\beta|y)\propto p(\alpha,\beta)\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\alpha+y_j)\Gamma(\beta+n_j-y_j)}{\Gamma(\alpha+\beta+n_j)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Choosing a noninformative hyperprior distribution:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha,\beta)\propto (\alpha+\beta)^{-5/2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies that &lt;span class=&#34;math inline&#34;&gt;\((\alpha/(\alpha+\beta),(\alpha+\beta)^{-1/2})\)&lt;/span&gt; is uniformly distributed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the prior mean is &lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the prior variance is approximately &lt;span class=&#34;math inline&#34;&gt;\((\alpha+\beta)^{-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-of-the-marginal-posterior-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot of the marginal posterior density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/alphabeta.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-the-separate-model-and-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare the separate model and hierarchical model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/hier_sep.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-based-on-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model based on normal distribution&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; independent experiments, with experiment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; form &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; independent distributed data points &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt;, each with known error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is
&lt;span class=&#34;math display&#34;&gt;\[y_{ij}|\theta_j\stackrel{iid}{\sim} N(\theta_j,\sigma^2), \text{ for }i=1,\dots,n_j;\ j=1,\dots,J\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;denote the sample mean of each group &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[\bar{y}_{\cdot j}=\frac 1{n_j}\sum_{i=1}^{n_j}y_{ij}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j=\sigma^2/n_j\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the likelihood for each &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\bar{y}_{\cdot j}|\theta_j\sim N(\theta_j,\sigma_j^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-based-on-normal-distribution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model based on normal distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the convenience of conjugacy, assume the paramerters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are drawn from a normal distribution with hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\mu,\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1,\dots,\theta_J|\mu,\tau)=\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;assign noninformative uniform hyperprior density to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau)=p(\mu|\tau)p(\tau)\propto p(\tau)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\tau)\propto 1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\tau|y)\propto p(\mu,\tau)p(\theta|\mu,\tau)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\tau|y)\propto p(\mu,\tau)\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\theta_j,\sigma_j^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-based-on-normal-distribution-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model based on normal distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the conditional posterior distirbution:
&lt;span class=&#34;math display&#34;&gt;\[\theta_j|\mu,\tau,y\sim N(\hat{\theta}_j,V_j)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\hat{\theta}_j=\frac{\frac 1{\sigma^2}\bar{y}_{\cdot j}+\frac 1{\tau^2}\mu}{\frac 1{\sigma^2}+\frac 1{\tau^2}},\ V_j=\frac{1}{\frac 1{\sigma^2}+\frac 1{\tau^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the marginal posterior density can be computed in a simple way
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau|y)\propto p(\mu,\tau)p(y|\mu,\tau)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{\cdot j}|\mu,\tau\sim N(\mu,\sigma_j^2+\tau^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau|y)\propto p(\mu,\tau)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\mu,\sigma_j^2+\tau^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-based-on-normal-distribution-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model based on normal distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu|\tau,y\sim N(\hat{\mu},V_{\mu})\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}=\frac{\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}\bar{y}_{\cdot j}}{\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}},\ V_{\mu}^{-1}=\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\tau|y)=\frac{p(\mu,\tau|y)}{p(\mu|\tau,y)}\propto \frac{p(\tau)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\mu,\sigma_j^2+\tau^2)}{N(\mu|\hat{\mu},V_{\mu})}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\tau|y)\propto p(\tau)V_{\mu}^{1/2}\prod_{j=1}^J(\sigma_j^2+\tau^2)^{-1/2}\exp\left(-\frac{(\bar{y}_{\cdot j}-\hat{\mu})^2}{2(\sigma_j^2+\tau^2)}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-parallel-experiments-in-eight-schools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: parallel experiments in eight schools&lt;/h2&gt;
&lt;p&gt;A study was performanced for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Seperate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Verbal).&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;School&lt;/th&gt;
&lt;th&gt;Estiamted treatment effect &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;Standard error of effect estimate &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;-3&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;D&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;H&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;comparisons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparisons&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/8schools.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-posterior-summaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the posterior summaries&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/media/img/8schools2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/en/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/en/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>è´å¶æ–¯è®¡ç®—åŠå…¶æŒ‘æˆ˜</title>
      <link>/en/talk/bayes_comp/</link>
      <pubDate>Wed, 02 Sep 2020 19:00:00 +0000</pubDate>
      <guid>/en/talk/bayes_comp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Convergence analysis of quasi-Monte Carlo sampling for quantile and expected shortfall</title>
      <link>/en/publication/2020mcom/</link>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/en/publication/2020mcom/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Integrated Quasi-Monte Carlo Method for Handling High Dimensional Problems with Discontinuities in Financial Engineering</title>
      <link>/en/publication/2020ec/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/en/publication/2020ec/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Error Rate of Conditional Quasi-Monte Carlo for Discontinuous Functions</title>
      <link>/en/publication/2019sinum/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/en/publication/2019sinum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Writing technical content in Academic</title>
      <link>/en/post/writing-technical-content/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/writing-technical-content/</guid>
      <description>&lt;p&gt;Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Highlight your code snippets, take notes on math classes, and draw diagrams from textual representation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Academic.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the &lt;code&gt;highlight&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;$...$&lt;/code&gt; or &lt;code&gt;$$...$$&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left |\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right |^2}$$&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;$\nabla F(\mathbf{x}_{n})$&lt;/code&gt; renders as $\nabla F(\mathbf{x}_{n})$.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the &lt;code&gt;\\\\&lt;/code&gt; math linebreak:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \\\\
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \\&lt;br&gt;
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$&lt;/p&gt;
&lt;h3 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the &lt;code&gt;diagram&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file or by adding &lt;code&gt;diagram: true&lt;/code&gt; to your page front matter.&lt;/p&gt;
&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;Gantt diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;class diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;state diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h3&gt;
&lt;p&gt;You can even write your todo lists in Academic too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;- [x] Write math example
- [x] Write diagram example
- [ ] Do something else
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write math example&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write diagram example&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Do something else&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tables&#34;&gt;Tables&lt;/h3&gt;
&lt;p&gt;Represent your data in tables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;| First Header  | Second Header |
| ------------- | ------------- |
| Content Cell  | Content Cell  |
| Content Cell  | Content Cell  |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;First Header&lt;/th&gt;
&lt;th&gt;Second Header&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;asides&#34;&gt;Asides&lt;/h3&gt;
&lt;p&gt;Academic supports a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#alerts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcode for asides&lt;/a&gt;, also referred to as &lt;em&gt;notices&lt;/em&gt;, &lt;em&gt;hints&lt;/em&gt;, or &lt;em&gt;alerts&lt;/em&gt;. By wrapping a paragraph in &lt;code&gt;{{% alert note %}} ... {{% /alert %}}&lt;/code&gt;, it will render as an aside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% alert note %}}
A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
{{% /alert %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;spoilers&#34;&gt;Spoilers&lt;/h3&gt;
&lt;p&gt;Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; spoiler text=&amp;quot;Click to view the spoiler&amp;quot; &amp;gt;}}
You found me!
{{&amp;lt; /spoiler &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-1&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-1&#34;&gt;
      Click to view the spoiler
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-1&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      You found me!
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;icons&#34;&gt;Icons&lt;/h3&gt;
&lt;p&gt;Academic enables you to use a wide range of 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/#icons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;icons from &lt;em&gt;Font Awesome&lt;/em&gt; and &lt;em&gt;Academicons&lt;/em&gt;&lt;/a&gt; in addition to 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#emojis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;emojis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some examples using the &lt;code&gt;icon&lt;/code&gt; shortcode to render icons:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; icon name=&amp;quot;terminal&amp;quot; pack=&amp;quot;fas&amp;quot; &amp;gt;}} Terminal  
{{&amp;lt; icon name=&amp;quot;python&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} Python  
{{&amp;lt; icon name=&amp;quot;r-project&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} R
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-terminal  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Terminal&lt;br&gt;

  &lt;i class=&#34;fab fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Python&lt;br&gt;

  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; R&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it ğŸ™Œ&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>An importance sampling-based smoothing approach for quasi-Monte Carlo simulation of discrete barrier options</title>
      <link>/en/publication/2019ejor/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/en/publication/2019ejor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Asymptotic normality of extensible grid sampling</title>
      <link>/en/publication/2019cs/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/en/publication/2019cs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/en/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/jupyter/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (
&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the metadata of your post, using the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/en/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/en/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/en/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/en/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/en/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/en/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quasi-Monte Carlo for Discontinuous Integrands with Singularities along the Boundary of the Unit Cube</title>
      <link>/en/publication/2018mcom/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/en/publication/2018mcom/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Computation of Option Prices and Greeks by Quasi-Monte Carlo Method with Smoothing and Dimension reduction</title>
      <link>/en/publication/2017sisc/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/en/publication/2017sisc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/en/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/en/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>/en/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/en/post/getting-started/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or 
&lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ‘‰ 
&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ“š 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ’¬ 
&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Ask a question&lt;/strong&gt; on the forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ‘¥ 
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ¦ Twitter: 
&lt;a href=&#34;https://twitter.com/source_themes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@source_themes&lt;/a&gt; 
&lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; 
&lt;a href=&#34;https://twitter.com/search?q=%23MadeWithAcademic&amp;amp;src=typd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithAcademic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ’¡ 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;â¬†ï¸ &lt;strong&gt;Updating?&lt;/strong&gt; View the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Guide&lt;/a&gt; and 
&lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;â¤ï¸ &lt;strong&gt;Support development&lt;/strong&gt; of Academic:
&lt;ul&gt;
&lt;li&gt;â˜•ï¸ 
&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Donate a coffee&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ’µ 
&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Become a backer on &lt;strong&gt;Patreon&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ–¼ï¸ 
&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decorate your laptop or journal with an Academic &lt;strong&gt;sticker&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ‘• 
&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wear the &lt;strong&gt;T-shirt&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ğŸ‘©â€ğŸ’» 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/contribute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Contribute&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


















&lt;figure id=&#34;figure-academic-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; data-caption=&#34;Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable 
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and 
&lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - 
&lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, 
&lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, ä¸­æ–‡, and PortuguÃªs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Academic comes with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can  choose their preferred mode - click the sun/moon icon in the top right of the 
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customizable&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Scripts&lt;/a&gt;:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;personalize and deploy your new site&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the Update Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; to help keep track of 
&lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;updates&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present 
&lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Auto-Realignment Method in Quasi-Monte Carlo for Pricing Financial Derivatives with Jump Structures</title>
      <link>/en/publication/2016ejor/</link>
      <pubDate>Sat, 26 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/en/publication/2016ejor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extensible Grids -- Uniform Sampling on a Space-Filling Curve</title>
      <link>/en/publication/2016jrssb/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/en/publication/2016jrssb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Convergence Rate of Randomized Quasi-Monte Carlo for Discontinuous Functions</title>
      <link>/en/publication/2015sinum/</link>
      <pubDate>Thu, 29 Oct 2015 00:00:00 +0000</pubDate>
      <guid>/en/publication/2015sinum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/en/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      <guid>/en/post/2015-07-23-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Good Path Generation Methods in Quasi-Monte Carlo for Pricing Financial Derivatives</title>
      <link>/en/publication/2014sisc/</link>
      <pubDate>Tue, 18 Mar 2014 00:00:00 +0000</pubDate>
      <guid>/en/publication/2014sisc/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
