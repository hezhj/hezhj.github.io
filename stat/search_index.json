[
["index.html", "数理统计讲义 前言 致谢 版权", " 数理统计讲义 何志坚 2020-02-27 前言 本讲义为《数理统计》课程配套材料，大部分为课堂讲解内容。第1章介绍了数理统计基本概念。第2章介绍了点估计方法，包括矩法估计、极大似然估计，还介绍了区间估计方法。第3章介绍了假设检验基本概念以及似然比检验方法。第4章介绍了一元线性回归和多元线性回归模型，以及最小二乘估计，相应的区间估计和显著性检验。 致谢 献给我的妻子和儿子。 版权 本讲义仅供选修《数理统计》课程的同学学习使用，如有其它用途请联系作者：hezhijian@scut.edu.cn "],
["author.html", "作者简介", " 作者简介 何志坚，男，华南理工大学数学学院副教授、硕士生导师。清华大学统计学博士，华南理工大学数学与应用数学本科。研究兴趣为统计计算与建模、随机模拟方法及其应用、金融工程。相关研究发表在统计学和计算科学领域顶级期刊，如统计学四大期刊Journal of the Royal Statistical Society: Series B，计算科学顶级期刊SIAM Journal on Numerical Analysis，SIAM Journal on Scientific Computing和Mathematics of Computation。博士论文获得新世界数学奖银奖。曾获第十四届金融系统工程与工程管理国际年会(FSERM2016)优秀论文奖。 根据https://scimeter.org/clouds/提供的词云分析，我的研究关键字如下： 发表论文 Z. He and X. Wang. Good Path Generation Methods in Quasi-Monte Carlo for Pricing Financial Derivatives, SIAM Journal on Scientific Computing, 36 (2), B171-B197, 2014. Z. He and X. Wang. On the Convergence Rate of Randomized Quasi-Monte Carlo for Discontinuous Functions, SIAM Journal on Numerical Analysis, 53 (5), 2488-2503, 2015. Z. He and A. B. Owen. Extensible Grids: Uniform Sampling on a Space-Filling Curve,Journal of the Royal Statistical Society: Series B, 78 (4), 917-931, 2016. C. Weng, X. Wang, and Z. He. An Auto-Realignment Method in Quasi-Monte Carlo for Pricing Financial Derivatives with Jump Structures,European Journal of Operational Research, 254 (1), 304-311, 2016. C. Weng, X. Wang, and Z. He. Efficient Computation of Option Prices and Greeks by Quasi-Monte Carlo Method with Smoothing and Dimension reduction, SIAM Journal on Scientific Computing, 39 (2), B298-B322, 2017. Z. He and A. B. Owen. Discussion of: ‘Sequential Quasi-Monte Carlo’ by M. Gerber and N. Chopin, Journal of the Royal Statistical Society: Series B, 77 (3), 563-564, 2015. C. Schretter, Z. He, M. Gerber, N. Chopin, and H. Niederreiter. Van der Corput and Golden Ratio Sequences Along the Hilbert Space-Filling Curve,Proceedings of the MCQMC 2014 conference, R. Cools and D. Nuyens (Eds.), 531-544, 2016. Z. He. Quasi-Monte Carlo for Discontinuous Integrands with Singularities along the Boundary of the Unit Cube. Mathematics of Computation, 87 (314), 2857-2870, 2018. Z. He and L. Zhu. Asymptotic Normality of Extensible Grid Sampling.Statistics and Computing, 29 (1), 53-65, 2019. F. Xie, Z. He, and X. Wang. An Importance Sampling-Based Smoothing Approach for Quasi-Monte Carlo Simulation of Barrier Options. European Journal of Operational Research, 274 (2), 759-772, 2019. Z. He. On the Error Rate of Conditional Quasi-Monte Carlo for Discontinuous Functions. SIAM Journal on Numerical Analysis, 57(2), 854-874, 2019. "],
["intro.html", "第 1 章 绪论 1.1 学科介绍 1.2 基本概念 1.3 概率分布族 1.4 统计量与估计量 1.5 充分统计量 1.6 抽样分布 1.7 分位数 1.8 本章习题", " 第 1 章 绪论 1.1 学科介绍 数理统计学是探讨随机现象统计规律的一门学科。它使用概率论和其它数学方法，研究怎样收集带有随机误差的数据，并在设定的统计模型下，对这种随机数据进行统计分析，以对所研究的问题作出统计推断。 由于所收集的统计数据只能反映问题的局部特征，数理统计的任务就在于从统计资料所反映的局部特征以概率论作为理论基础去推断事物的整体特征。一言以蔽之，由局部推断整体，如下图所示。 图 1.1: 统计推断示意图 数据是什么？本课程所涉及的数据是指带有随机性质的数据。随机性来源于数据产生机制的不确定性。在数据没有观测之前是不可以预知的。例如，测量数据是带有随机误差的，每次测量的结果不是固定的、可预测的数据。股票价格同样是不可以预知的。事实上，随机数据可以分成两种类型，一种是现实生活中收集到的真实数据，另一种是通过计算机模拟出来的数据。在一般情况下，我们并不完全知道真实数据是如何产生的，因此我们要通过统计的方法来研究产生该数据的机制，即背后的统计模型。模拟数据则是通过具体的统计模型来产生的数据，也就是蒙特卡罗方法抽样。这种方法常用于随机模拟计算，检验和比较统计方法的稳健性和优越性。 模型是什么？这里的模型是指产生随机数据的机制。一个很关键的问题是我们对这种机制的认知有多少？一无所知抑或了如指掌？实际上，我们不可能对这种机制百分之百地了解，否则就没有必要对数据进行分析。相反，由于我们对实际问题有或多或少的客观或者主观认识，所以我们对这种机制是有一定认识的。什么是客观认知？比如，我们得到了一枚硬币抛10次的数据（1代表正面朝上，0表示反面朝上）：1001001001。假如你的问题是研究这枚硬币的均匀性。此时，你会毫不犹豫假设这个数据来自一个二项分布\\(B(1,p)\\)模型，其中\\(p\\)是硬币正面朝上的概率，也就是所关心的均匀性。这里的客观性体现在此问题的分布类型是明确的——伯努利分布，你永远不会用正态分布或者其他连续型分布来刻画这种数据。尽管如此，我们对这种数据产生机制还不是100%确定的，未知部分在于\\(p\\)的取值。所以，对于一些问题，我们可以利用常见的概率分布客观地刻画随机数据的产生机制。然而，对于大部分实际问题，现有的分布并不能完美地刻画随机数据的产生机制。我们往往需要根据问题的特征进行统计建模，选择恰当的模型，此时就不可避免一些主观因素。统计分析是允许这种不完美性的。正如著名统计学家乔治·博克斯（1919—2013）所言，“本质上，所有模型都是错的，但有一些是有用的。” 作为一门实用的学科，我们尤其关注统计方法的实用性。 Essentially, all models are wrong, but some are useful. —— George Box 什么样的推断？由数据到整体的推理称为统计推断，有两种基本形式： 参数估计 假设模型可以表示成参数形式\\(M(\\theta)\\)，其中\\(M\\)的形式已知，可以为简单的一类分布（如二项分布、正态分布），也可以为复杂的模型（如线性回归模型），但\\(\\theta\\)是未知参数。现有来自该模型的数据\\(X_1,X_2,\\dots,X_n\\)。一个基本问题是如何通过这些数据估计未知参数\\(\\theta\\)。更一般地，我们希望估计形如\\(g(\\theta)\\)的未知量，其中\\(g(\\cdot)\\)为给定的函数。例如，考虑估计一个与模型相关事件\\(A\\)的概率\\(P_\\theta(A)\\)，显然该概率可表示为参数\\(\\theta\\)的函数\\(g(\\theta)\\)。无论是估计模型中未知参数还是与模型参数相关的未知量都统称为参数估计问题。参数估计问题分为两种类型：点估计和区间估计，相应统计方法见第2章。 例 1.1 某品牌灯泡的平均寿命是多少？使用超过1年的可能性是多少？ 图 1.2: 灯泡寿命估计 假设检验 有时候我们不一定迫切想知道未知参数的具体值，而是想得到一个“真”或“假”的答案。比如，我们关心一个硬币是否均匀，这对“用抛硬币来做决定”的公平性至关重要。假设\\(\\theta\\)表示这枚硬币正面朝上的概率，该问题也就变成判断\\(\\theta=0.5\\)是否成立，而不是关心\\(\\theta\\)的具体值是多少。诸如此类的问题数不胜数，比如，一种新药对某疾病是否凑效？这种新药是不是比传统药物更能缓解病情？在第4章的线性回归分析中，我们经常要判断回归方程是否显著，回归系数是否显著？这些判断命题真假的问题称为假设检验，严格的数学描述和检验方法见第3章。 例 1.2 OPPO手机真的能做到充电五分钟通话两小时吗？ 图 1.3: 知乎上有关OPPO手机通话时间的问题 1.1.1 统计学的发展简史 统计学作为一门学科已有三百多年的历史。按统计方法及历史的演变顺序，通常可以将统计学的发展史分为三个阶段，分别是古典统计学时期、近代统计学时期和现代统计学时期。 第一个时期（古典统计学时期） 古典统计学的萌芽最早可以追溯到17世纪中叶，此时的欧洲正处于封建社会解体和资本主义兴起的阶段，工业、手工业快速增长，社会经历着重大变革。政治改革家们急需辅助国家经营和管理的数据证据以适应经济发展需要，此时一系列统计学的奠基工作在欧洲各国相继展开。这个阶段以描述性统计为主。代表性人物：高斯(C. F. Gauss, 1777-1855), 皮尔逊(K. Pearson, 1857-1936)等。 第二个时期（近代统计学时期） 20世纪初至第二次世界大战为近代统计学发展时期。科学技术开始进入全面繁荣时期，天文、气象、社会人口等领域的数据资料达到一定规模的积累，对统计的需求已从国家层面扩展至社会科学各个领域。对事物现象静态性的描述也已不能满足社会需求。一些重要的统计概念也在这一时期提出，误差测定、正态分布曲线、最小二乘法、大数定律等理论方法的大量运用为社会、经济、人口、法律等领域的研究提供了大量宝贵的指导。代表性人物：费希尔(R. A. Fisher, 1890-1962), 奈曼(J. Neyman, 1894-1981), 小皮尔逊(E. S. Pearson, 1895-1980), 许宝騄(1910-1970)等。 第三个时期（现代统计学时期） 二战后至今，得益于计算机的发展，统计方法渗透许多学科。这一阶段，统计在毒理学、分子生物学、临床试验等生物医学领域获得了大量应用，这些领域的发展又带动统计方法不断创新，主成分估计、非参数估计等方法应运而生。得益于高性能计算，贝叶斯学派蓬勃发展。 20世纪80年代开始，随着现代生物医学的发展，计算机技术的进步，人类对健康的管理和疾病的治疗已进入基因领域，对基因数据分析产生了大量需求。高维海量的基因数据具有全新的数据特征，变量维度远远大于样本数，传统的统计方法失效了，因此一系列面向高维数据的统计分析方法相继产生，比如著名的Lasso方法。 20世纪90年代以来，随着互联网的发展，数据库中积累了海量的数据，进入大数据时代。如何从海量的数据中挖掘有用的信息就变得越来越重要了，数据挖掘也就应运而生了。与数据挖掘比较接近的名词是机器学习, 机器学习被看作是人工智能的一个分支，主要是研究一些让计算机可以自动“学习”的算法，是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为机器学习算法中涉及了很多的统计学理论，与统计学的关系密切，也被称为统计学习。 1.1.2 频率学派与贝叶斯学派 在统计学领域，存在两大学派，频率学派和贝叶斯学派。他们的分歧在于未知参数是否随机变量。这种分歧衍生出两种截然不同的统计思想。 频率学派（传统学派） 频率学派认为样本信息来自总体，仅通过研究样本信息可以对总体信息做出合理的推断和估计，并且样本越多，就越准确。 代表性人物：费希尔 (R. A. Fisher, 1890-1962) 贝叶斯学派 起源于英国学者贝叶斯(T. Bayes, 1702-1761)在1763年发表的著名论文《论有关机遇问题的求解》 最基本观点：任何一个未知量都可以看作是随机的，应该用一个概率分布去描述未知参数，而不是频率派认为的固定值。这种信息称为先验信息，是主观信息。 Good (1973)评价道： “主观主义者直抒他们的判断，而客观主义者以假设来掩盖其判断，并以此享受科学客观性的荣耀。” 图 1.4: 生活大爆炸中的贝叶斯公式 贝叶斯统计的发展 自然语言处理：计算机翻译语言、识别语音、认识文字和海量文献的检索 南京市长江大桥欢迎您! 人工智能、无人驾驶 垃圾短信、垃圾邮件识别 如何在一个陌生的地方找餐馆吃饭？ 1.1.3 统计学专业 统计学的应用涉及金融、经济、社会学、工程学、环境等多个领域，从而形成的相应的研究分支。其特点是多学科交叉、实用为主。 统计学专业包含理论统计和应用统计两方面 理论统计：模型选择，非参统计方法，贝叶斯统计，时间序列与生存分析，高维数据分析与机器学习，数据挖掘等等。 应用统计：目前发展最为突出的是生物统计，金融统计等等。 统计学经过漫长的发展，尤其是计算机的大量应用，目前包括但不限于下面这些分支（或者交叉领域）。目前最火热的学科都是跟计算机结合比较紧密的。 统计理论研究：大样本性质、各种渐近理论分析等 高维（超高维）统计推断：变量选择、大规模假设检验 统计计算方法：蒙特卡罗模拟、卡尔曼滤波算法、近似贝叶斯算法、自助法 生物统计：纵向分析、空间分析 统计学习：数据挖掘、人工智能 1.2 基本概念 本节介绍数理统计中一些基本概念：总体、样本、随机抽样。 1.2.1 总体 我们把研究对象的全体（包括有形的和潜在的）称作总体，其中每个成员称为个体。常用随机变量\\(X\\)来刻画一个总体（或者总体的特征值）。 例 1.3 考虑以下三个总体： 网上购物居民占全市居民的比例 过去一年内网购居民的购物次数 某品牌灯泡的寿命 总体\\(X\\)的分布函数\\(F(x)\\)未知或者部分未知，统计学的核心任务就是要对总体进行观测，并对所得数据推断总体的分布信息。 1.2.2 样本 研究总体可分为普查和抽样这两种方法。 普查（全数检查） 对总体中的每个个体进行观察，如我国每十年一次的人口普查 缺点：费用高、时间长、不适合破坏性试验 抽样 从总体中抽取若干个体进行观察，用所获得数据对总体进行统计推断 优点：费用低、时间短 抽取的部分组成的集合\\((X_1,\\dots,X_n)\\)称为样本，\\(X_i\\)称为样品 样品个数\\(n\\)称为样本量或者样本容量 1.2.3 简单随机抽样 简单随机抽样满足以下两个特征： 随机性：每个个体都有相同的机会选中（有放回随机抽取/独立重复观测），即\\(X_i\\)与总体\\(X\\)同分布 独立性：每个样本的选取是独立的 这种方式得到的样本也称为简单随机样本。本课程所研究的均为简单随机样本，简称样本。常用记号\\(X_i\\stackrel{iid}{\\sim} F\\)表示独立同分布\\(F\\)，其中“iid”为independent and identically distributed 的缩写。 样本具有两重性 抽取之前无法预知它们的数值，故\\((X_1,\\dots,X_n)\\)为\\(n\\)维随机向量 抽取后样本为具体的数，用小写字母\\((x_1,\\dots,x_n)\\)表示，称为样本观测值 注：所有的统计分析都是基于随机变量，统计推断结论基于样本观测值（数据）。 1.2.4 案例 2018年高考全国II卷作文（适用地区: 内蒙古、黑龙江、辽宁、吉林、重庆、陕西、甘肃、宁夏、青海、新疆、西藏、海南） “二战”期间，为了加强对战机的防护，英美军方调查了作战后幸存飞机上弹痕的分布，决定哪里弹痕多就加强哪里，然而统计学家瓦尔德(Abrahom Wald, 1902–1950)力排众议，指出更应该注意弹痕少的部位，因为这些部位受到重创的战机，很难有机会返航，而这部分数据被忽略了。事实证明沃德是正确的。 要求: 综合材料内容及含义，选好角度，确定立意，明确文体，自拟标题; 不要套作，不得抄袭; 不少于800字。 图 1.5: 幸存者偏见 在这个案例中，我们关心的总体是飞机的弹痕分布。那么，哪些是样本？是顺利返航飞机的弹痕？还是应该包含失事飞机的弹痕数据？显然后者不可忽略，否则推断总体就会出现很大偏差。在心理学上，这就是著名的“幸存者偏见”。 1.3 概率分布族 一般情况下，由于一些主观和客观的认识，我们都会对统计模型（总体）做出一些假定。通常地，假设总体\\(X\\)分布\\(F(x)\\)属于某个分布族\\(\\mathcal{F}\\). 分为以下三类： 参数族 \\(\\mathcal{F}\\)中的分布的一般数学形式已知，但包含若干未知参数\\(\\theta=(\\theta_1,\\dots,\\theta_m)\\) \\(\\mathcal{F}:=\\{F_\\theta,\\theta\\in\\Theta\\}\\), 其中\\(\\Theta\\subset \\mathbb{R}^m\\)称为参数空间 该模型为参数统计问题，\\(m\\)为模型的维数 \\(m=1\\)为单参数统计问题，\\(m&gt;1\\)为多参数统计问题 非参数族 当\\(\\mathcal{F}\\)中的分布不能通过有限个未知参数来刻画 该模型为非参数统计问题 半参数族 \\(\\mathcal{F}\\)中的分布有一部分可以用参数刻画，一部分则不可以。 为什么要引进分布族的概念？原因是我们不知道总体确切的分布，但基于部分信息，我们可以把考虑的范围缩小到一个明确的集合里面进行分析。在给定的集合里面，我们就可以讨论最优性，比较不同统计方法的优越等等。而缩小的幅度取决于对总体的了解程度。但值得注意的是，虽然总体有未知参数，但该总体对应的分布族应是明确的。对于参数分布族而言，分布类型是确定的，参数空间也是给定的范围。 1.3.1 常用的参数族 离散型 二项分布族\\(\\{b(n,p);0&lt;p&lt;1\\}\\) 几何分布族\\(\\{Ge(p);0&lt;p&lt;1\\}\\) 泊松分布族\\(\\{P(\\lambda);\\lambda&gt;0\\}\\) 连续型 正态分布族\\(\\{N(\\mu,\\sigma^2);-\\infty&lt;\\mu&lt;\\infty,\\sigma&gt;0\\}\\) 均匀分布族\\(\\{U(a,b);-\\infty&lt;a&lt;b&lt;\\infty\\}\\) 指数分布族\\(\\{Exp(\\lambda);\\lambda&gt;0\\}\\) 1.3.2 伽玛分布族 伽玛分布的密度函数为 \\[f(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x}1\\{x&gt; 0\\},\\] 其中\\(\\alpha&gt;0\\)称为形状参数，\\(\\lambda&gt;0\\)称为速率，\\(1/\\lambda\\)称为尺度参数，\\(\\Gamma(\\alpha)\\)为伽马函数，定义及性质如下： \\(\\Gamma(\\alpha)=\\int_0^{+\\infty} x^{\\alpha-1}e^{-x}dx\\) \\(\\Gamma(1)=1,\\Gamma(1/2)=\\sqrt{\\pi}\\) \\(\\Gamma(\\alpha+1)=\\alpha\\Gamma(\\alpha)\\) 当\\(\\alpha\\)为整数\\(n\\)时，\\(\\Gamma(n+1)=n!\\) 伽玛分布记为\\(Ga(\\alpha,\\lambda)\\)，期望为\\(\\frac \\alpha \\lambda\\)，方差为\\(\\frac \\alpha {\\lambda^2}\\)。伽玛分布有两个特例： \\(\\alpha=1\\)时伽玛分布为指数分布，即\\(Ga(\\alpha,\\lambda)=Exp(\\lambda)\\) \\(\\alpha=n/2,\\lambda=1/2\\)时伽玛分布为自由度为\\(n\\)的卡方分布（后文提及），即\\(Ga(n/2,1/2)=\\chi^2(n)\\)。 图 1.6: 伽马分布的密度函数示意图 性质1（可加性）：设\\(X_1\\sim Ga(\\alpha_1,\\lambda),\\ X_2\\sim Ga(\\alpha_2,\\lambda)\\)。如果\\(X_1\\)与\\(X_2\\)独立，则 \\[X_1+X_2\\sim Ga(\\alpha_1+\\alpha_2,\\lambda).\\] 性质2（可乘性）：设\\(X\\sim Ga(\\alpha,\\lambda)\\),则\\(kX\\sim Ga(\\alpha,\\lambda/k)\\), 其中\\(k&gt;0\\). 证明提示：\\(Ga(\\alpha,\\lambda)\\)分布的特征函数为 \\[\\phi(t)=E[e^{itX}]=\\left(1-\\frac{it}\\lambda\\right)^{-\\alpha}.\\] 性质2很好地解释了为什么称\\(1/\\lambda\\)为尺度参数。 1.3.3 贝塔分布族 贝塔分布的密度函数为 \\[f(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1\\{0&lt;x&lt;1\\},\\] 其中\\(\\alpha&gt;0,\\beta&gt;0\\)为形状参数族。记为\\(Beta(\\alpha,\\beta)\\)，贝塔分布期望为\\(\\frac{\\alpha}{\\alpha+\\beta}\\)，方差：\\(\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)。 特例：当\\(\\alpha=\\beta=1\\)时，\\(Beta(1,1)=U(0,1)\\). 图 1.7: 贝塔分布密度函数示意图 贝塔分布常用于以下场合： 不合格率 市场占有率 命中率 1.3.4 指数型分布族 下面介绍一个更一般的分布族，包括了常见的分布。 定义 1.1 指数型分布族\\(\\mathcal{F}=\\{f_\\theta(x);\\theta\\in\\Theta\\}\\)中的分布\\(f_\\theta(x)\\)（分布列或者密度函数）都可以表示成如下形式： \\[f_\\theta(x)=c(\\theta)\\exp\\{\\sum_{j=1}^kc_j(\\theta)T_j(x)\\}h(x),\\] 其中， \\(k\\)为正整数，称为该指数分布族的“维数”。 \\(c(\\theta)&gt;0,c_j(\\theta)\\)为参数空间\\(\\Theta\\)上的函数 \\(h(x)\\ge 0\\), \\(T_1(x),\\dots,T_k(x)\\)线性无关 令\\(\\eta_j=c_j(\\theta),\\eta=(\\eta_1,\\dots,\\eta_k)^\\top\\)，则分布函数可以变成关于新参数\\(\\eta\\)的函数 \\[f_\\eta(x)=\\tilde c(\\eta)\\exp\\{\\sum_{j=1}^k\\eta_j T_j(x)\\}h(x),\\] 其中 \\[\\tilde c(\\eta) = 1/\\int \\exp\\{\\sum_{j=1}^k\\eta_j T_j(x)\\}h(x)d x.\\] 称\\(\\eta_i\\)为自然参数。 常见的指数型分布族 正态分布族是指数型分布族 \\[f(x,\\mu,\\sigma)=\\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\mu^2/(2\\sigma^2)}\\exp\\{\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2\\}\\] 二项分布族是指数型分布族 \\[P(X=x) = C_n^x p^x(1-p)^{n-x}=(1-p)^n\\exp\\{\\ln[p/(1-p)]x \\}C_n^x\\] 伽玛/贝塔分布族是指数型分布族（请自行验证） 注意到指数型分布的支撑与参数\\(\\theta\\)无关。因此排除了均匀分布族。指数型分布族的优点是能够把常见的分布纳入同一框架，有利于统计分析。 命题 1.1 如果\\(X_1,\\dots,X_n\\)是来自某指数型分布族中某分布的样本，则样本的联合分布还是指数型分布。 证明. 这是因为： \\[f_\\theta(x_1,\\dots,x_n)=\\prod_{i=1}^np_\\theta(x_i)=c(\\theta)^n\\exp\\{\\sum_{j=1}^kc_j(\\theta)\\sum_{i=1}^nT_j(x_i)\\}\\prod_{i=1}^nh(x_i).\\] 1.4 统计量与估计量 样本是总体的反映，但样本所含信息不能直接用于解决我们所要研究的问题，而需要把样本所含的信息进行数学上的加工使其浓缩起来，从而解决我们的问题。为此，数理统计学往往构造一个合适的依赖于样本的函数，我们称之为统计量。统计推断大多是基于（一个或者多个）统计量。 定义 1.2 如果\\((X_1,\\dots,X_n)\\)为来自总体\\(X\\)的样本，若样本函数 \\[T=T(X_1,\\dots,X_n):\\mathbb{R}^n\\to \\mathbb{R}\\] 是不含有任何未知参数的（可测）函数，则称\\(T\\)为统计量。统计量的分布称为抽样分布。 定义 1.3 用于估计未知参数的统计量称为点估计量，或者简称估计量。 用于检验检验的统计量称为检验统计量。 注：这里的未知参数常指以下几种： 分布中所含的未知参数 分布的数字特征：期望、方差、标准差、分位数等 某事件的概率 例 1.4 设\\(X_1,\\dots,X_n\\)为来自\\(X\\sim N(\\mu,\\sigma^2)\\)的样本, 若\\(\\mu\\)已知，\\(\\sigma\\)未知，判断\\(T_1,T_2\\)是否为统计量? \\[T_1 = \\frac{\\sqrt{n}(\\sum_{i=1}^n X_i-\\mu)}{\\sigma},\\] \\[T_2 = \\min(X_1,\\dots,X_n).\\] 常见的统计量如下： 样本均值 \\[\\bar{X}=\\bar X_n=\\frac{1}{n}\\sum_{i=1}^n X_i\\] 样本方差 \\[S_n^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] 修正样本方差 \\[S_n^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2=\\frac{n}{n-1}S_n^2\\] 样本标准差 \\[S_n=\\sqrt{S_n^2}\\] 样本\\(k\\)阶原点矩 \\[\\overline{X^k}=\\frac{1}{n}\\sum_{i=1}^n X_i^k\\] 样本\\(k\\)阶中心矩 \\[\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^k\\] 顺序统计量 \\[X_{(1)}\\le X_{(2)}\\le \\dots\\le X_{(n)}\\] 其中\\(X_{(1)}=\\min\\{X_1,\\dots,X_n\\}\\), \\(X_{(n)}=\\max\\{X_1,\\dots,X_n\\}\\), \\(X_{(k)}\\)为\\({X_1,\\dots,X_n}\\)的递增排序的第\\(k\\)位。\\(X_{(n)}-X_{(1)}\\)样本极差。 样本中位数 \\[ \\tilde{X}= \\begin{cases} X_{(\\frac{n+1}{2})},\\ &amp;\\text{$n$为奇数}\\\\ (X_{(\\frac{n}{2})}+X_{(\\frac{n}{2}+1)})/2,\\ &amp;\\text{$n$为偶数} \\end{cases} \\] 注意：样本方差和修正样本方差相差一个常数倍，即\\(S_n^2 = \\frac{n-1}{n}S_n^{*2}\\). 后面会看到为什么要引入修正样本方差，因为它是总体方差的无偏估计。 1.5 充分统计量 统计量各式各样，有些并不能反映有用的信息。但有一类统计量比较特殊，称为充分统计量，它能够 简化数据 不损失样本信息 定义 1.4 设有一个分布族\\(\\mathcal{F}\\), \\(X_1,\\dots,X_n\\)是从某分布\\(F\\in\\mathcal{F}\\)中抽取的一个样本。\\(T=T(X_1,\\dots,X_n)\\)是一个（向量）统计量。样本的条件分布\\((X_1,\\dots,X_n)|T\\)与总体分布\\(F\\)无关，则称\\(T\\)为此分布族\\(\\mathcal{F}\\)的充分统计量。如果\\(\\mathcal{F}=\\{F_\\theta;\\theta\\in\\Theta\\}\\)是参数分布族，样本的条件分布\\((X_1,\\dots,X_n)|T\\)与参数\\(\\theta\\)无关，则称\\(T\\)为参数\\(\\theta\\)的充分统计量。 注：上面是充分统计量的数学定义，该定义如何体现“不损失样本信息”这一特征呢？假设现在只知道充分统计量\\(T\\)的观测值为\\(t\\)，是否可以由此“恢复”样本数据？注意到在给定\\(T=t\\)条件下，样本\\((X_1,\\dots,X_n)\\)的条件分布与参数\\(\\theta\\)无关，我们可以通过这个条件分布随机生成样本\\((X&#39;_1,\\dots,X&#39;_n)\\)。（为什么？）容易证明，\\((X&#39;_1,\\dots,X&#39;_n)\\)的分布与\\((X_1,\\dots,X_n)\\)一样。从这个角度上讲，充分统计量不损失样本信息。 例 1.5 总体\\(X\\sim b(1,p),0&lt;p&lt;1\\). 判断以下两个统计量是否是充分统计量 \\(T_1=\\sum_{i=1}^nX_i\\) \\(T_2=X_1+X_2\\) 解. (1) \\[\\begin{align*} P(X_1=x_1,\\dots,X_n=x_n|T_1=t) &amp;= \\frac{P(X_1=x_1,\\dots,X_n=x_n,T_1=t)}{P(T_1=t)}\\\\ &amp;=\\begin{cases} \\frac{P(X_1=x_1,\\dots,X_n=x_n)}{P(T_1=t)},\\ &amp;\\sum_{i=1}^n x_i = t\\\\ 0, &amp; else \\end{cases} \\\\ &amp;=\\begin{cases} \\frac{p^{\\sum_{i=1}^n x_i}(1-p)^{n-\\sum_{i=1}^n x_i}}{C_n^t p^t(1-p)^{n-t}},\\ &amp;\\sum_{i=1}^n x_i = t\\\\ 0, &amp; else \\end{cases} \\\\&amp;=\\begin{cases} \\frac 1{C_n^t},\\ &amp;\\sum_{i=1}^n x_i = t\\\\ 0, &amp; else \\end{cases} \\end{align*}\\] 该分布与\\(p\\)无关，所以\\(T_1\\)是\\(p\\)的充分统计量。这意味着，如果知道\\(T_1=t\\)，则\\((X_1,\\dots,X_n)\\)的取值只有\\(C_n^t\\)种，每种情况发生的可能性一样。此时，只需要在这\\(C_n^t\\)种情况中等可能抽样就可以“恢复数据”。 \\[\\begin{align*} P(X_1=x_1,\\dots,X_n=x_n|T_2=t) &amp;= \\frac{P(X_1=x_1,\\dots,X_n=x_n,T_2=t)}{P(T_2=t)} \\\\&amp;=\\frac{P(X_1=x_1,X_2=t-x_1,\\dots,X_n=x_n)}{P(T_2=t)} \\\\&amp;=\\frac{p^{t+\\sum_{i=3}^n x_i}(1-p)^{n-t-\\sum_{i=3}^n x_i}}{C_2^t p^t(1-p)^{2-t}} \\\\&amp;=\\frac 1{C_2^t}p^{\\sum_{i=3}^n x_i}(1-p)^{n-2-\\sum_{i=3}^n x_i} \\end{align*}\\] 该分布与\\(p\\)相关，所以\\(T_2\\)不是\\(p\\)的充分统计量。 例 1.6 总体\\(X\\sim Ge(p),0&lt;p&lt;1\\), 证明\\(T=\\sum_{i=1}^nX_i\\)是参数\\(p\\)的充分统计量。 解. 考虑\\(\\sum_{i=1}^n x_i = t\\), 则有 \\[\\begin{align*} P(X_1=x_1,\\dots,X_n=x_n|T=t) &amp;=\\frac{P(X_1=x_1,\\dots,X_n=x_n,T=t)}{P(T=t)} \\\\&amp;=\\frac{\\prod_{i=1}^n[p(1-p)^{x_i-1}]}{C_{t-1}^{n-1} p^n (1-p)^{t-n}}=\\frac{1}{C_{t-1}^{n-1}} \\end{align*}\\] 该分布与\\(p\\)无关，所以\\(T\\)是\\(p\\)的充分统计量。 例 1.7 总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其中\\(\\sigma\\)已知。证明\\(T=\\sum_{i=1}^nX_i\\)是参数\\(\\mu\\)的充分统计量。 引理 1.1 设总体的密度为\\(f_\\theta(x)\\). 则在给定\\(T=t\\)下，样本的条件密度函数为 \\[ f_\\theta(x_1,\\dots,x_n|T=t)=\\frac{\\prod_{i=1}^nf_\\theta(x_i) 1\\{T(x_1,\\dots,x_n)=t\\}}{f^T_\\theta(t)}, \\] 其中\\(f^T_\\theta(t)\\)为\\(T\\)的密度函数。 证明. 由条件密度定义知， \\[\\begin{align*} f_\\theta(x_1,\\dots,x_n|T(x_{1:n})=t)&amp;=\\frac{f_\\theta(x_1,\\dots,x_n,t)}{f^T_\\theta(t)} \\\\&amp;=\\frac{f_\\theta(t|x_1,\\dots,x_n)f_\\theta(x_1,\\dots,x_n)}{f^T_\\theta(t)} \\\\&amp;= \\frac{\\prod_{i=1}^nf_\\theta(x_i)1\\{T(x_1,\\dots,x_n)=t\\}}{f^T_\\theta(t)}. \\end{align*}\\] 思考题: 顺序统计量\\(X_{(1)},\\dots,X_{(n)}\\)是否充分统计量？ 1.5.1 因子分解定理 前面几个例题是通过充分统计量的定义判断，需要求条件分布，比较繁琐。下面介绍因子分解定理，由J. Neyman和P. R. Halmos在20世纪40年代提出，能够很方便地判断/求解充分统计量。 定理 1.1 (因子分解定理) 设样本的分布为\\(f_\\theta(x_1,\\dots,x_n)\\)（在离散总体情况下表示样本的分布列，在连续总体情况下表示样本的密度函数）。则在统计量\\(T\\)是充分的当且仅当存在两个函数满足 \\(h(x_1,\\dots,x_n)\\)非负 在统计量\\(T\\)取值空间上的函数\\(g_\\theta(t)\\), 使得 \\[\\begin{equation} f_\\theta(x_1,\\dots,x_n) = g_\\theta(T(x_1,\\dots,x_n))h(x_1,\\dots,x_n),\\ \\forall\\theta\\in\\Theta. \\tag{1.1} \\end{equation}\\] 证明. 由于数学工具的限制，仅考虑离散情形。以下设\\(t\\)为\\(T\\)任意一种观测结果。 （1）必要性 由于\\(T\\)是充分的，不妨设\\(P(X_{1:n}=x_{1:n}|T=t) = h(t,x_{1:n})\\). 考虑到 \\[\\{X_{1:n}=x_{1:n}\\}\\subset \\{T(X_{1:n})=T(x_{1:n})\\}.\\] 因此, \\[\\begin{align*} P(X_{1:n}=x_{1:n})&amp;= P(X_{1:n}=x_{1:n}, T(X_{1:n})=T(x_{1:n})) \\\\&amp;=P(X_{1:n}=x_{1:n}|T(X_{1:n})=T(x_{1:n}))P(T(X_{1:n})=T(x_{1:n})) \\\\&amp;=h(T(x_{1:n}),x_{1:n})g_\\theta(T(x_{1:n})), \\end{align*}\\] 其中\\(g_\\theta\\)为\\(T\\)的分布函数。 （2）充分性 设样本\\(X_{1{:}n}\\)的取值空间为\\(\\Omega\\)，令\\(A(t) = \\{x_{1:n}\\in \\Omega|T(x_{1:n})=t\\}\\). 不难看出集合\\(A(t)\\)有至多可列个元素。 \\[\\begin{align*} P(X_{1:n}=x_{1:n}|T(X_{1:n})=t)) &amp;= \\frac{P(X_{1:n}=x_{1:n},X_{1:n}\\in A(t))}{P(X_{1:n}\\in A(t))}\\\\ &amp;= \\frac{P(X_{1:n}=x_{1:n})1\\{x_{1:n}\\in A(t)\\}}{\\sum_{y_{1:n}\\in A(t)}P(X_{1:n}=y_{1:n})}\\\\ &amp;= \\frac{g_\\theta(T(x_{1:n}))h(x_{1:n})1\\{x_{1:n}\\in A(t)\\}}{\\sum_{y_{1:n}\\in A(t)}g_\\theta(T(y_{1:n}))h(y_{1:n})}\\\\ &amp;= \\frac{g_\\theta(t)h(x_{1:n})1\\{x_{1:n}\\in A(t)\\}}{\\sum_{y_{1:n}\\in A(t)}g_\\theta(t)h(y_{1:n})}\\\\ &amp;= \\frac{h(x_{1:n})1\\{x_{1:n}\\in A(t)\\}}{\\sum_{y_{1:n}\\in A(t)}h(y_{1:n})} \\end{align*}\\] 与\\(\\theta\\)无关，所以是充分的。 1.5.2 因子分解定理的应用 例 1.8 总体分布为\\(U(0,\\theta)\\), 求参数\\(\\theta\\)的充分统计量。 解. 样本联合密度函数为 \\[f_\\theta(x_1,\\dots,x_n) =\\prod_{i=1}^n \\frac 1\\theta 1\\{0&lt;x_i&lt;\\theta\\}=\\frac 1{\\theta^n}1\\{x_{(1)}&gt;0\\}1\\{x_{(n)}&lt;\\theta\\}.\\] 由因子分解定理知，\\(X_{(n)}\\)为\\(\\theta\\)的充分统计量。 例 1.9 总体分布为\\(N(\\mu,\\sigma^2)\\)，求 参数\\((\\mu,\\sigma^2)\\)的充分统计量 当\\(\\sigma^2\\)已知时，\\(\\mu\\)的充分统计量 当\\(\\mu\\)已知时，\\(\\sigma^2\\)的充分统计量 解. 样本联合密度函数为 \\[f_\\theta(x_1,\\dots,x_n) = \\frac{1}{(2\\pi)^{n/2}}e^{-\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{2\\sigma^2}}=\\frac{1}{(2\\pi)^{n/2}}e^{-\\frac{\\sum_{i=1}^n x_i^2-2\\mu\\sum_{i=1}^n x_i +n\\mu^2}{2\\sigma^2}}.\\] 由因子分解定理，参数\\((\\mu,\\sigma^2)\\)的充分统计量为\\((\\sum_{i=1}^n x_i,\\sum_{i=1}^n x_i^2)\\). 当\\(\\sigma^2\\)已知时，\\(\\mu\\)的充分统计量为\\(\\bar X\\); 当\\(\\mu\\)已知时，\\(\\sigma^2\\)的充分统计量为\\(\\sum_{i=1}^n(x_i-\\mu)^2\\)。 指数型分布族的充分统计量 指数型分布族下的样本分布为 \\[f_\\theta(x_1,\\dots,x_n)=\\prod_{i=1}^np_\\theta(x_i)=c(\\theta)^n\\exp\\{\\sum_{j=1}^kc_j(\\theta)\\sum_{i=1}^nT_j(x_i)\\}\\prod_{i=1}^nh(x_i)\\] 由因子分解定理知，参数\\(\\theta\\)的一个充分统计量为 \\[\\left(\\sum_{i=1}^nT_1(x_i),\\dots,\\sum_{i=1}^nT_k(x_i)\\right).\\] 注意到\\(k\\)是一个和样本量\\(n\\)无关的量。当\\(n\\)远远大于\\(k\\)的时候，该充分统计量可以大大简化数据，减小数据的“维数”。此外， 不难发现，充分统计量有无穷多个。 定理 1.2 如果\\(T\\)是\\(n_1\\)维充分统计量，\\(S\\)是\\(n_2\\)维统计量且\\(T=\\psi(S)\\), 其中\\(\\psi:\\mathbb{R}^{n_2}\\to \\mathbb{R}^{n_1}\\)是可测函数，则\\(S\\)也是充分的。 例 1.10 总体分布为\\(N(\\mu,\\sigma^2)\\)，以下哪些统计量为参数\\((\\mu,\\sigma^2)\\)的充分统计量 \\((\\bar X, S_n)\\) \\((\\bar X, S_n^2)\\) \\((\\bar X, S_n^*)\\) \\((\\bar X, S_n^{*2})\\) \\((\\sum_{i=1}^n X_i,\\sum_{i=1}^n X_i^2)\\) \\((\\sum_{i=1}^n X_i,\\sum_{i=1}^n |X_i|)\\) \\((\\sum_{i=1}^n X_i,\\sum_{i=1}^n |X_i|,\\sum_{i=1}^n X_i^2)\\) 思考：哪种降维效果最好？这需要引进最小充分统计量的概念。相关内容可以参考Jun Shao的专著《Mathematical Statistics》。 1.6 抽样分布 统计量的概率分布称为抽样分布，分为如下三类： 精确抽样分布 当总体\\(X\\)的分布已知时，对任意自然数\\(n\\)都能导出统计量\\(T(X_1,\\dots,X_n)\\)的分布的显示表达式 对样本量\\(n\\)较小的统计推断问题（小样本问题）特别有用 精确抽样分布多数是在正态总体下得到 渐近抽样分布 寻求在样本量\\(n\\)无限大时统计量\\(T(X_1,\\dots,X_n)\\)的极限分布 适用于对样本量\\(n\\)较大的统计推断问题（大样本问题） 常用的方法是中心极限定理 近似抽样分布 寻找一种分布来近似统计量\\(T(X_1,\\dots,X_n)\\)的分布 1.6.1 样本均值的抽样分布 定理 1.3 设\\(X_1,\\dots,X_n\\)为来自总体\\(X\\)的样本，\\(\\bar X\\)为其样本均值。 如果\\(X\\sim N(\\mu,\\sigma^2)\\)，则\\(\\bar X\\)的精确分布为\\(N(\\mu,\\sigma^2/n)\\). 如果总体不是正态分布，但\\(E[X]=\\mu,Var[X]=\\sigma^2\\)存在，则\\(\\bar X\\)的渐近分布为 \\(N(\\mu,\\sigma^2/n)\\)，记为\\(\\bar X\\stackrel{\\cdot}\\sim N(\\mu,\\sigma^2/n)\\). 证明. 第一种情况利用独立正态分布的可加性即可。应用中心极限定理可得第二种情况的结论。 下面比较不同样本量下均匀分布总体均值（标准化后）的分布，从中发现当\\(n\\)增大时，标准化后的分布与标准正态分布接近。 图 1.8: 均匀分布总体均值 1.6.2 卡方分布 定义 1.5 设\\(X_i\\stackrel{iid}\\sim N(0,1),i=1,\\dots,n\\)，则称随机变量 \\[X = X_1^2+\\dots+X_n^2\\] 的分布为自由度为\\(n\\)的卡方分布，记为\\(\\chi^2(n)\\)。其密度函数为 \\[f_n(x)=\\frac{1}{2^{\\frac n2}\\Gamma(n/2)}x^{\\frac n2-1}e^{-\\frac x2} 1\\{x&gt;0\\}.\\] 对比伽马分布密度函数可知，\\(Ga(n/2,1/2)=\\chi^2(n)\\)。卡方分布的密度函数曲线如下图所示： 图 1.9: 卡方分布密度函数示意图 可加性：如果\\(X\\sim \\chi^2(n)\\), \\(Y\\sim \\chi^2(m)\\)且它们独立，则 \\[X+Y\\sim \\chi^2(n+m).\\] 容易计算卡方分布的期望和方差：\\(E[X]=n,\\ Var[X]=2n\\)。由中心极限定理可得： \\[\\frac{X-n}{\\sqrt{2n}}\\stackrel{d}\\to N(0,1),\\] 其中\\(\\stackrel{d}\\to\\)表示依分布收敛。 1.6.3 正态总体抽样分布定理 正态分布是最常见的一种分布，本节研究正态总体下样本均值\\(\\bar X\\)和样本方差\\(S_n^2\\)的联合分布以及边际分布。以下定理为数理统计学中最基本的定理，后面的章节中反复用到。 定理 1.4 设\\((X_1,\\dots,X_n)\\)为来自总体\\(X\\sim N(\\mu,\\sigma^2)\\)的样本，则 \\(\\bar{X}\\sim N(\\mu,\\sigma^2/n)\\), \\[\\frac{nS_n^2}{\\sigma^2}=\\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{\\sigma^2}\\sim \\chi^2(n-1),\\] 样本均值\\(\\bar{X}\\)与样本方差\\(S_n^2\\)相互独立。 定理1.4实际上给出了样本均值\\(\\bar X\\)和样本方差\\(S_n^2\\)的联合分布。 为证明这个定理，我们需要用到多元正态分布的性质。 引理 1.2 假设\\(X_{1:n}=(X_1,\\dots,X_n)^\\top \\sim N(\\mu,\\Sigma)\\)，其中\\(\\mu=(\\mu_1,\\dots,\\mu_n)^\\top\\)为均值向量, \\(\\Sigma\\)为（非奇异）协方差矩阵。对任意可逆矩阵\\(A\\in \\mathbb{R}^{n\\times n}\\)，有\\[AX_{1:n}\\sim N(A\\mu,A\\Sigma A^\\top).\\] 证明. 令\\(Y_{1:n}=(Y_1,\\dots,Y_n)^\\top=AX_{1:n}\\)，则其CDF为： \\[\\begin{align*} F_Y(y) &amp;= P(Y_{1:n}\\le y) = P(AX_{1:n}\\le y) \\\\&amp;=\\int_{Ax\\le y}\\frac 1{(2\\pi)^{n/2}|\\Sigma|^{1/2}}e^{-(1/2)(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)}d x \\\\&amp;=\\int_{z\\le y}|A^{-1}|\\frac 1{(2\\pi)^{n/2}|\\Sigma|^{1/2}}e^{-(1/2)(A^{-1}z-\\mu)^\\top \\Sigma^{-1}(A^{-1}z-\\mu)}d z \\\\&amp;=\\int_{z\\le y}\\frac 1{(2\\pi)^{n/2}|A\\Sigma A^\\top|^{1/2}}e^{-(1/2)(z-A\\mu)^\\top (A\\Sigma A^\\top)^{-1}(z-A\\mu)}d z. \\end{align*}\\] 这表明\\(Y_{1:n}\\)的pdf为： \\[f_Y(y) = \\frac 1{(2\\pi)^{n/2}|A\\Sigma A^\\top|^{1/2}}e^{-(1/2)(y-A\\mu)^\\top (A\\Sigma A^\\top)^{-1}(y-A\\mu)},\\] 即\\(Y_{1:n}\\sim N(A\\mu,A\\Sigma A^\\top)\\). 注：该引理中\\(A\\)为可逆矩阵的条件可以放松为\\(A\\in \\mathbb{R}^{m\\times n}\\)为行满秩矩阵，即\\(\\mathrm{rank}(A) = m\\le n\\)。（请读者自行证明） 定理1.4的证明： 易知\\(X_{1:n}\\sim N((\\mu,\\dots,\\mu)^\\top,\\sigma^2 I_n)\\). 假设\\(A\\)为如下正交矩阵： \\[ A=\\left[ \\begin{matrix} \\frac 1{\\sqrt{n}} &amp; \\frac 1{\\sqrt{n}} &amp; \\frac 1{\\sqrt{n}} &amp; \\cdots &amp; \\frac1{\\sqrt{n}}\\\\ \\frac 1{\\sqrt{2\\times 1}} &amp; -\\frac 1{\\sqrt{2\\times1}} &amp; 0 &amp; \\cdots &amp; 0\\\\ \\frac 1{\\sqrt{3\\times 2}} &amp; \\frac 1{\\sqrt{3\\times2}} &amp; -\\frac 2{\\sqrt{3\\times2}} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac 1{\\sqrt{n\\times (n-1)}} &amp; \\frac 1{\\sqrt{n\\times (n-1)}} &amp; \\frac {1}{\\sqrt{n\\times (n-1)}} &amp; \\cdots &amp; -\\frac {n-1}{\\sqrt{n\\times (n-1)}}\\\\ \\end{matrix} \\right]. \\] 令\\(Y_{1:n}=AX_{1:n}\\). 由上面引理得，\\(Y_{1:n}\\sim N((\\sqrt{n} \\mu,0,\\dots,0)^\\top,\\sigma^2 I_n)\\). 注意到，\\(Y_1 = \\sqrt{n}\\bar X\\), 所以\\(\\bar X=Y_1/\\sqrt{n}\\sim N(\\mu,\\sigma^2/n)\\). 又\\[\\sum_{i=1}^n Y_i^2 = Y_{1:n}^\\top Y_{1:n} = (A X_{1:n})^\\top A X_{1:n}=X_{1:n}^\\top X_{1:n}=\\sum_{i=1}^n X_i^2.\\] 所以， \\[\\begin{align*} \\frac{nS_n^2}{\\sigma^2}&amp;=\\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{\\sigma^2}=\\frac{\\sum_{i=1}^nX_i^2-n(\\bar X)^2}{\\sigma^2} \\\\&amp;=\\frac{\\sum_{i=1}^nY_i^2-Y_1^2}{\\sigma^2}=\\sum_{i=2}^n(Y_i/\\sigma)^2\\sim \\chi^2(n-1), \\end{align*}\\] 这是因为\\(Y_i\\sim N(0,\\sigma^2),i=2,\\dots,n\\). 由于\\(Y_i\\)相互独立，\\(\\bar X\\)可以用\\(Y_1\\)表示，\\(S_n^2\\)可以用\\(Y_2,\\dots,Y_n\\)表示，所以它们独立。证毕。 研究发现，只有正态总体才有“样本均值与方差独立”这一性质。 1.6.4 t分布 定义 1.6 设\\(X\\sim N(0,1), Y\\sim \\chi^2(n)\\), 且它们独立，则称随机变量 \\[T = \\frac{X}{\\sqrt{Y/n}}\\] 的分布为自由度为\\(n\\)的学生氏t分布（简称t分布），记为\\(T\\sim t(n)\\)。其密度函数为： \\[ f_n(x)= \\frac{\\Gamma\\left(\\frac{n+1}2\\right)}{\\sqrt{n\\pi }\\Gamma\\left(\\frac n2\\right)}\\left(1+\\frac{x^2}n\\right)^{-\\frac{n+1}{2}}. \\] 图 1.10: t分布密度函数示意图 t分布的两种特例： 当\\(n=1\\)时，t分布成为柯西分布。 可以证明：\\(\\lim_{n\\to\\infty}f(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\). 当\\(n\\ge 25\\)时,可以认为t分布与\\(N(0,1)\\)接近。 由上图中可以看出当自由度\\(n\\)不断增大时，t分布越来越接近标准正态分布。 t分布的起源 1.6.5 样本均值与标准差之比的抽样分布 定理 1.5 设\\((X_1,\\dots,X_n)\\)为来自总体\\(X\\sim N(\\mu,\\sigma^2)\\)的样本，则 \\[\\frac{\\bar{X}-\\mu}{S_n/\\sqrt{n-1}}=\\frac{\\bar{X}-\\mu}{S_n^*/\\sqrt{n}}\\sim t(n-1).\\] 比较： \\[\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\] 标准正态分布与t分布尾部概率\\(P(|X|&gt;c)\\)的比较: 分布 \\(c=2\\) \\(c=2.5\\) \\(c=3\\) \\(c=3.5\\) \\(X\\sim N(0,1)\\) 0.0455 0.0124 0.0027 0.000465 \\(X\\sim t(4)\\) 0.1161 0.0668 0.0399 0.0249 从中可以看出，t分布是厚尾的，发生极端事件的概率比标准正态分布大。这点也可以通过比较两者的密度函数得知。t分布是密度函数是多项式衰减的，而标准正态分布是指数阶衰减的，远远比t分布衰减快。 1.6.6 F分布 定义 1.7 设\\(X\\sim \\chi^2(m), Y\\sim \\chi^2(n)\\), 且\\(X,Y\\)相互独立，则称随机变量 \\[Z=\\frac{X/m}{Y/n}\\] 的分布为第一自由度为\\(m\\)、第二自由度为\\(n\\)的F分布，记\\(Z\\sim F(m,n)\\)。其密度函数为 \\[f_{mn}(x)= \\frac{\\Gamma((m+n)/2)}{\\Gamma(m/2)\\Gamma(n/2)}\\left(\\frac{m}{n}\\right)^{m/2}x^{\\frac m2-1}(1+mx/n)^{-(m+n)/2} 1\\{x&gt;0\\}. \\] F分布的性质： \\(Z\\sim F(m,n)\\), 则\\(1/Z\\sim F(n,m)\\). 如果\\(T\\sim t(n)\\), 则\\(T^2\\sim F(1,n)\\). 图 1.11: F分布密度函数示意图 1.6.7 两个独立正态总体的抽样分布 定理 1.6 设两独立总体\\(X\\sim N(\\mu_1,\\sigma_1^2)\\),\\(Y\\sim N(\\mu_2,\\sigma_2^2)\\)的样本分别为\\((X_1,\\dots,X_m),(Y_1,\\dots,Y_n)\\). 样本方差分别为\\(S_{1m}^2,S_{2n}^2\\). 则 \\[\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{\\sqrt{\\sigma_1^2/m+\\sigma_2^2/n}}\\sim N(0,1).\\] \\[\\frac{mS_{1m}^2/\\sigma_1^2/(m-1)}{nS_{2n}^2/\\sigma_2^2/(n-1)}=\\frac{S_{1m}^{*2}\\sigma_2^2}{S_{2n}^{*2}\\sigma_1^2}\\sim F(m-1,n-1). \\] 如果\\(\\sigma_1=\\sigma_2=\\sigma\\), \\[\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{S_w\\sqrt{1/m+1/n}}\\sim t(m+n-2),\\] 其中\\(S_w =\\sqrt{(mS_{1m}^2+nS_{2n}^2)/(m+n-2)}\\). 证明. 根据F分布、t分布的构造方式即可证明。 注：\\(S_w^2\\)称为合并的样本方差。 1.6.8 顺序统计量 定理 1.7 若\\(X_1,\\dots,X_n\\)独立同分布，分布函数和密度函数分别为\\(F(x),f(x)\\). 则\\(X_{(1)}=\\min(X_1,\\dots,X_n)\\)的分布函数和密度函数分别 \\[\\begin{cases} F_{X_{(1)}}(x) = 1-(1-F(x))^n\\\\ f_{X_{(1)}}(x) = n(1-F(x))^{n-1}f(x). \\end{cases} \\] \\(X_{(n)}=\\max(X_1,\\dots,X_n)\\)的分布函数和密度函数分别 \\[\\begin{cases} F_{X_{(n)}}(x) = F(x)^n\\\\ f_{X_{(n)}}(x) = nF(x)^{n-1}f(x). \\end{cases} \\] 更一般地，对任意\\(k\\in \\{1,\\dots,n\\}\\)有 \\[f_{X_{(k)}}(x) = \\frac{n!}{(n-k)!(k-1)!}F(x)^{k-1}[1-F(x)]^{n-k}f(x).\\] 定理 1.8 顺序统计量\\((X_{(i)},X_{(j)})(i&lt;j)\\)的联合密度函数为 \\[\\begin{align*} f_{X_{(i)},X_{(j)}}(x,y) &amp;= \\frac{n!}{(i-1)!(j-i-1)!(n-j)!}F(x)^{i-1} [F(y)-F(x)]^{j-i-1}\\\\ &amp;\\times [1-F(y)]^{n-j}f(x)f(y) 1\\{x\\le y\\}. \\end{align*}\\] 定理 1.9 顺序统计量\\((X_{(1)},\\dots,X_{(n)})\\)的联合密度函数为 \\[ f(y_1,\\dots,y_n)= \\begin{cases} n!\\prod_{i=1}^nf(y_i),&amp;y_1&lt;y_2&lt;\\dots&lt;y_n\\\\ 0,&amp;else. \\end{cases} \\] 1.7 分位数 定义 1.8 设\\(X\\)的分布函数为\\(F(x)\\). 对于任意\\(\\alpha\\in(0,1)\\), \\(\\alpha\\)分位数定义为 \\[x_\\alpha=F^{-1}(\\alpha)=\\inf\\{t\\in\\mathbb{R}|F(t)\\ge \\alpha\\}.\\] 图 1.12: CDF的逆 由于分布函数是右连续函数，求逆时可能出现上图展示的三种情况。 常见的四种分位数符号： 标准正态分布分位数记为\\(u_{\\alpha}\\) \\(t\\)分布分位数记为\\(t_{\\alpha}(n)\\) \\(\\chi^2\\)分布分位数记为\\(\\chi^2_{\\alpha}(n)\\) \\(F\\)分布分位数记为\\(F_{\\alpha}(m,n)\\) 一些说明： 在分位点表中对于标准正态分布、\\(t\\)分布和F分布只能查到\\(\\alpha&gt;1/2\\)的分位数，需利用以下对称性间接查\\(\\alpha&lt;1/2\\)的分位数： \\[u_\\alpha=-u_{1-\\alpha},\\ t_\\alpha(n)=-t_{1-\\alpha}(n),\\ F_{\\alpha}(m,n)=\\frac{1}{F_{1-\\alpha}(n,m)}.\\] 对于\\(t(n)\\)分布，由于当\\(n\\to \\infty\\)时，其极限分布为\\(N(0,1)\\), 所以自由度\\(n\\)比较大时，\\(t_{\\alpha}(n)\\approx u_{\\alpha}\\). 若\\(X\\sim \\chi^2(n)\\)分布，由于当\\(n\\to \\infty\\)时，\\((X-n)/\\sqrt{2n}\\stackrel{d}\\to N(0,1)\\), 所以自由度\\(n\\)比较大时，\\(\\chi^2_{\\alpha}(n)\\approx u_{\\alpha}\\sqrt{2n}+n\\). 图 1.13: 分位数示意图 1.8 本章习题 习题 1.1 韦布尔分布(Weibull distribution)族 \\[p(x)=\\frac k\\lambda\\left(\\frac{x}{\\lambda}\\right)^{k-1}e^{-(x/\\lambda)^k}1\\{x\\ge 0\\},k&gt;0,\\lambda&gt;0\\] 是不是指数型分布族？ ( ) A. 是 B. 不是 习题 1.2 从均值为\\(\\mu\\), 方差为\\(\\sigma^2\\)的总体中随机抽取样本量为\\(n\\)的样本\\(x_1,\\dots,x_n\\), 其中\\(\\mu,\\sigma^2\\)均未知，指出下列样本函数中哪些为统计量? ( ) A. \\(T_1=x_1+x_2-2\\mu\\) B. \\(T_2=(x_1-\\mu)/\\sigma\\) C. \\(T_3=(\\bar x-10)/5\\) D. \\(T_4=\\frac 1 n\\sum_{i=1}^n(x_i-S_n)^2\\) 习题 1.3 设\\(\\bar x_n,s_n^2\\)表示样本\\(x_1,\\dots,x_n\\)的样本均值与样本方差。已知 \\[n=15,\\bar x_{n}=168, s_n=11.43, x_{n+1}=170.\\] 求\\(\\bar x_{n+1},s_{n+1}^2\\)，以及修正样本方差\\(s_{n+1}^{*2}\\). 习题 1.4 设\\(X_1\\sim Ga(\\alpha_1,\\lambda)\\), \\(X_2\\sim Ga(\\alpha_2,\\lambda)\\), 且\\(X_1\\)与\\(X_2\\)独立。证明 \\(Y_1=X_1+X_2\\sim Ga(\\alpha_1+\\alpha_2,\\lambda)\\) \\(Y_2=X_1/(X_1+X_2)\\sim Beta(\\alpha_1,\\alpha_2)\\) \\(Y_1\\)与\\(Y_2\\)独立 习题 1.5 设\\(X_1,\\dots,X_n\\)是来自某连续总体的一个样本，总体的分布函数\\(F(x)\\)是连续严增函数，证明：统计量\\(T=-2\\sum_{i=1}^n \\ln F(X_i)\\sim \\chi^2(2n)\\). 习题 1.6 从正态总体\\(N(52,6.3^2)\\)中随机抽取容量为36的样本。 求样本均值\\(\\bar X\\)的分布； 求\\(\\bar X\\)落在区间\\((50.8,53.8)\\)内的概率； 若要以\\(99\\%\\)的概率保证\\(|\\bar X-52|&lt;2\\), 试问样本量至少应取多少？ 习题 1.7 设随机变量\\(X\\sim N(0,1)\\), 对给定的\\(\\alpha\\in(0,1)\\), 数\\(u_{\\alpha}\\) 满足\\(P(X&gt;u_\\alpha)=\\alpha\\). 若\\(P(|X|&lt;x)=\\alpha\\), 则\\(x\\)等于( )。 A. \\(u_{\\alpha/2}\\) B. \\(u_{1-\\alpha/2}\\) C. \\(u_{(1-\\alpha)/2}\\) D. \\(u_{1-\\alpha}\\) 习题 1.8 设\\(X_1,\\dots,X_n\\)为总体\\(N(1,2^2)\\)的样本，下面正确的是( )。 A. \\(\\frac{\\bar X-1}{2/\\sqrt{n}}\\sim t(n)\\) B. \\(\\frac{1}{4}\\sum_{i=1}^n(X_i-1)^2\\sim F(n,1)\\) C. \\(\\frac{\\bar X-1}{\\sqrt{2}/\\sqrt{n}}\\sim N(0,1)\\) D. \\(\\frac{1}{4}\\sum_{i=1}^n(X_i-1)^2\\sim \\chi^2(n)\\) 习题 1.9 设\\(X_1,\\dots,X_{15}\\)为总体\\(N(0,2^2)\\)的样本，则统计量 \\[Y=\\frac{X_1^2+\\dots+X_{10}^2}{2(X_{11}^2+\\dots+X_{15}^2)}\\] 的分布为( )。 A. \\(F(10,5)\\) B. \\(F(11,4)\\) C. \\(\\chi^2(10)\\) D. 以上都不是 习题 1.10 设\\(X_1,\\dots,X_n\\)是来自双参数指数分布 \\[p(x;\\mu,\\theta)=\\frac 1\\theta \\exp\\{-(x-\\mu)/\\theta\\}, x&gt;\\mu,\\theta&gt;0\\] 的一个样本，证明\\((\\bar X,X_{(1)})\\)是该分布的充分统计量。 习题 1.11 设\\(X_1,\\dots,X_n\\)是来自密度函数 \\[p_\\theta(x)=\\theta/x^2,\\ 0&lt;\\theta&lt;x&lt;\\infty\\] 的一个样本，求参数\\(\\theta\\)的充分统计量。 习题 1.12 Let \\(X_1,X_2,\\dots,X_6\\) be a simple random sample taken from \\(N(0,2^2)\\). Denote \\[Y = (X_1+X_2)^2+(X_3+X_4)^2+(X_5+X_6)^2.\\] If \\(kY\\sim \\chi^2(3)\\), then \\(k=\\)? "],
["est.html", "第 2 章 估计 2.1 参数估计 2.2 估计的优良性标准 2.3 区间估计 2.4 分布的估计 2.5 本章习题", " 第 2 章 估计 2.1 参数估计 在实际问题中，对于一个总体\\(X\\)往往是仅知其分布的类型\\(F_\\theta\\)，而参数\\(\\theta=(\\theta_1,\\dots,\\theta_m)\\in \\Theta \\subset \\mathbb{R}^m\\)是未知的。对任给的实值函数 \\[g:\\ \\mathbb{R}^m\\to \\mathbb{R},\\] 如何根据\\(X\\)的样本\\(x_1,\\dots,x_n\\)估计\\(g( \\theta)\\)的值呢？这就是统计推断中的“参数估计”问题。 点估计：寻找一个统计量\\(\\hat{\\theta} = T(X_1,\\dots,X_n)\\)作为\\(\\theta\\)的点估计 区间估计：寻找两个统计量\\(\\hat{ \\theta}_1 = T_1(X_1,\\dots,X_n)\\), \\(\\hat{ \\theta}_2 = T_2(X_1,\\dots,X_n)\\)，所构成的区间\\([\\hat{\\theta}_1,\\hat{\\theta}_2]\\)作为 \\(\\theta\\)的区间估计 定义 2.1 假设总体\\(X\\)的分布是\\(F_\\theta\\). 如果存在\\(\\theta_1\\neq\\theta_2\\)使得\\(F_{\\theta_1}=F_{\\theta_2}\\)，则称\\(\\theta\\)对于\\(X\\)是不可识别的(unidentifiable)。 例 2.1 设\\(X\\sim N(\\mu_1+\\mu_2,\\sigma^2)\\), 则不难发现\\(\\theta=(\\mu_1,\\mu_2,\\sigma^2)\\)是不可识别的。估计参数\\(\\theta\\)是没有统计意义的。 考虑一个测量问题（如温度、血压、距离等），记测量对象真实值为\\(\\theta\\)，\\(n\\)次测量数据为\\(x_1,x_2,\\dots,x_n\\)。相信大部分人都会用样本均值 \\[\\bar x = \\frac{x_1+\\dots+x_n}{n}\\] 来估计\\(\\theta\\)。但是，为什么用样本均值估计是合理的呢？这是最好的估计方式吗？这些是本章重点讨论的问题。 2.1.1 矩估计法 矩估计的想法来源于大数定理。如果总体\\(X\\)存在\\(k\\)阶矩，对任意\\(\\epsilon&gt;0\\), \\[ \\lim_{n\\to \\infty} P(|\\frac 1 n\\sum_{i=1}^n X_i^k-E[X^k]|\\ge \\epsilon )=0. \\] 这说明，当样本容量\\(n\\)较大时，样本\\(k\\)阶矩与总体\\(k\\)阶矩差别很小。矩法估计就是用样本\\(k\\)阶矩代替总体的\\(k\\)阶矩。通常用\\(\\hat{\\theta}_M\\)表示。一般步骤如下： 列出估计式\\(E[X^k]=g_k(\\theta_1,\\dots,\\theta_m),\\ k=1,\\dots,m.\\) 求解关于估计量的方程组\\(\\theta_k = \\theta_k(E[X^1],\\dots,E[X^m])\\) 用\\(M_k=\\frac 1 n\\sum_{i=1}^n X_i^k\\)替代\\(E[X^k]\\)得到矩估计\\(\\hat\\theta_k = \\theta_k(M_1,\\dots,M_m)\\) 例 2.2 求总体\\(X\\)的期望\\(\\mu=E[X]\\)与方差\\(\\sigma^2=Var[X]\\)的矩估计。 解. (1)列出估计式 \\[ \\begin{cases} E[X] &amp;= \\mu\\\\ E[X^2] &amp;= \\mu^2+\\sigma^2 \\end{cases} \\] (2)求解关于估计量的方程组 \\[ \\begin{cases} \\mu &amp;= E[X]\\\\ \\sigma^2 &amp;= E[X^2]-(E[X])^2 \\end{cases} \\] 所以，\\(\\hat{\\mu}_M = \\bar X\\), \\(\\hat{\\sigma}^2_M = \\frac{1}{n}\\sum_{i=1}^n X_i^2-(\\bar X)^2 = S_n^2.\\) 注：不难证明，总体的各阶中心矩的矩估计就是样本各阶中心矩。 例 2.3 设总体\\(X\\sim U[a,b]\\), 求\\(a,b\\)的矩估计。 解. 易知，\\(E[X]=(a+b)/2,\\ Var[X]= (b-a)^2/12\\). 所以， \\[ \\begin{cases} a &amp;= E[X]-\\sqrt{3Var[X]}\\\\ b &amp;= E[X]+\\sqrt{3Var[X]} \\end{cases} \\] \\[ \\begin{cases} \\hat a_M &amp;= \\bar{X}-\\sqrt{3}S_n\\\\ \\hat b_M &amp;= \\bar{X}+\\sqrt{3}S_n \\end{cases} \\] 例 2.4 设总体\\(X\\)的分布密度为 \\[ f(x)=\\frac{\\theta}{2}e^{-\\theta|x|},\\ x\\in\\mathbb{R}, \\theta&gt;0. \\] 求\\(\\theta\\)的矩估计。 解. \\[ E[X]= 0,\\ E[X^2]=\\int_{-\\infty}^{\\infty}x^2\\frac{\\theta}{2}e^{-\\theta|x|}d x=\\theta\\int_{0}^{\\infty}x^2e^{-\\theta x}d x=\\frac{2}{\\theta^2} \\] \\[\\hat{\\theta}_M=\\sqrt{\\frac{2n}{\\sum_{i=1}^n X_i^2}}.\\] 除外，还可以由\\(E[|X|]=1/\\theta\\)得到另一种矩估计。 2.1.2 最大似然估计法 最大似然估计法最早由高斯(C.F.Gauss)提出，后来被 Fisher完善。最大似然估计这一名称也是Fisher给的。这是一个目前仍得到广泛应用的方法。它是建立在最大似然原理基础上的一个统计方法。 最大似然原理：最先出现的是概率最大的 例 2.5 设有外形完全相同的两个箱子，甲箱中有99个白球和1个黑球，乙箱中有99个黑球和1个白球，今抽取一箱并从中随机抽取一球，结果取得白球，问这球是从哪个箱子中取出？ 定义 2.2 假设总体\\(X\\)为离散随机变量，其分布函数记为\\(P(X=x)=f(x;\\theta)\\)，与参数\\(\\theta\\)相关。设\\(X_1,\\dots,X_n\\)为其样本，\\(x_1,\\dots,x_n\\)为该样本的观测值。样本的似然函数(likelihood function)定义为观测到样本\\(x_1,\\dots,x_n\\)的概率 \\[L(x_1,\\dots,x_n;\\theta)=P(X_1=x_1,\\dots,X_n=x_n)=\\prod_{i=1}^{n}f(x_i;\\theta). \\] 固定参数\\(\\theta\\)，似然函数\\(L(x_1,\\dots,x_n;\\theta)\\)为样本的概率质量函数(Probability Mass Function, PMF)。另一方面，给定样本观测值\\(x_1,\\dots,x_n\\)，似然函数\\(L(x_1,\\dots,x_n;\\theta)\\)是一个关于\\(\\theta\\)的函数，其中\\(\\theta\\in\\Theta\\)，有时简记为\\(L(\\theta)\\)。 例 2.6 设总体\\(X\\sim B(1,p)\\), 从中抽取样本的观测值为\\(1,1,0,0,1\\). 不难计算似然函数为 \\[L(p)=p^3(1-p)^2,\\ p\\in (0,1).\\] 图像如下 对于该数据，不同\\(p\\)的值得到不同的概率。现在要估计\\(p\\)的值，一种合理的方式是找出使得该概率最大对应\\(p\\)的值作为估计值。这就是最大似然估计的核心思想。通过简单的计算，可以发现\\(L(p)\\)的最大值点发生在\\(p=0.6\\). 因此，\\(0.6\\)可以作为\\(p\\)的估计值。 更一般地，给定样本观测值\\((x_1,\\dots,x_n)\\), 记\\(L(x_1,\\dots,x_n;\\theta)\\)的最大值点为\\(\\theta=T(x_1,\\dots,x_n)\\). 则\\(\\theta\\)的最大似然估计量(MLE, maximum likelihood estimator)为 \\[\\hat{\\theta}_L=T(X_1,\\dots,X_n).\\] 对于上述例子，如果观测数据是\\(x_1,\\dots,x_n\\)，似然函数则为 \\[ L(x_1,\\dots,x_n;p)=\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}=p^{\\sum_{i=1}^nx_i}(1-p)^{n-\\sum_{i=1}^nx_i}.\\] 令\\(y=\\sum_{i=1}^nx_i\\)。为了便于计算，对似然函数取对数变换，得到对数似然函数为： \\[\\ln L = y \\ln p + (n-y)\\ln (1-p).\\] 对数似然函数的极大值点与似然函数的极大值点一致。故求其求导可得，对数似然方程为： \\[\\frac{d \\ln L}{d p} = y/p - (n-y)/(1-p)=0.\\] 解得\\(p= y/n=\\frac{1}{n}\\sum_{i=1}^nx_i\\). 因为\\(\\frac{d^2\\ln L}{d p^2}&lt;0\\), 所以\\(p= y/n\\)是极大值。最大似然估计量为\\(\\hat{p}_L = \\bar X.\\) 如果\\(X\\)是连续的总体，似然函数该如何定义？此时，若沿用上述定义，由于连续型随机变量在某点发生的概率为零，则有\\(P(X_1=x_1,\\dots,X_n=x_n)=0\\). 然而，在该点一个领域的概率不为零。不妨考虑样本落在观测值点一个小邻域的概率。记\\(O(x,\\delta) = (x-\\delta,x+\\delta)\\)为\\(x\\)的\\(\\delta\\)邻域，则当\\(\\delta\\)比较小时， \\[P(X\\in O(x,\\delta)) = F(x+\\delta;\\theta)-F(x-\\delta;\\theta) \\approx f(x;\\theta)\\delta,\\] 其中\\(F\\)和\\(f\\)分别为X的分布函数和密度函数。现考虑样本落在\\(x_1,\\dots,x_n\\)的附近的概率， \\[P(X_1\\in O(x_1,\\delta_1),\\dots,X_n\\in O(x_n,\\delta_n))\\approx (\\prod_{i=1}^n\\delta_i)\\prod_{i=1}^n f(x_i;\\theta),\\] 其中\\(\\delta_i\\)为比较小的常数。从中可以看出，该概率的大小与\\(\\prod_{i=1}^n f(x_i;\\theta)\\)相关。我们把它定义成连续总体下样本的似然函数，即 \\[L(x_1,\\dots,x_n;\\theta)=\\prod_{i=1}^n f(x_i;\\theta).\\] 此时，样本的似然函数为样本的联合密度函数。 更一般的情形，样本不一定是独立同分布，此时似然函数同样可以定义为该样本的联合密度函数。本质上，似然函数是刻画样本在给定观测值处的“可能性”。PMF/PDF用来衡量这种“可能性”。 最大似然估计的一般步骤归纳如下： 第一步：写出似然函数\\(L(x_1,\\dots,x_n;\\theta)\\) 第二步：若似然函数\\(L\\)是\\(\\theta\\)的可微函数，则最大值必然满足似然方程 \\[\\frac{d L}{d \\theta}=0\\] 解出\\(\\theta\\), 并验证其是否是极大值：\\[\\frac{d^2 L}{d \\theta^2}&lt;0.\\] 注1：为方便求导，一般求对数似然函数\\(\\ln L(x_1,\\dots,x_n;\\theta)\\)求极大值点 注2：若有多个参数\\(\\theta_1,\\dots,\\theta_m\\)，对每个变量求偏导，联立\\(m\\)个方程求解 注3：（对数）似然方程的解称为“驻点”(stationary point)，可能为（局部）极大或者极小值点，也可能为鞍点(saddle point)，为了进一步区分需要求 Hessian矩阵并分析其正定性。 图 2.1: 三种类型的驻点 例 2.7 设总体\\(X\\sim N(\\mu,\\sigma^2)\\), 从中抽取样本\\(X_1,\\dots,X_n\\)的观测值为\\(x_1,\\dots,x_n\\). 求参数\\(\\mu,\\sigma^2\\)的最大似然估计。 解. 似然函数为 \\[ L(x_1,\\dots,x_n;\\mu,\\sigma^2)=\\prod_{i=1}^{n}f(x_i)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x_i-\\mu)^2/(2\\sigma^2)}\\] 令\\(\\theta_1=\\mu,\\theta_2=\\sigma^2\\), 对数似然函数为： \\[\\ln L = -(n/2)\\ln (2\\pi)-(n/2)\\ln\\theta_2-\\frac{\\sum_{i=1}^n(x_i-\\theta_1)^2}{2\\theta_2}\\] 对数似然方程组为： \\[ \\begin{cases} \\frac{\\partial \\ln L}{\\partial \\theta_1} &amp;=\\frac{\\sum_{i=1}^n(x_i-\\theta_1)}{\\theta_2}=0\\\\ \\frac{\\partial \\ln L}{\\partial \\theta_2} &amp;=-\\frac{n}{2\\theta_2}+\\frac{\\sum_{i=1}^n(x_i-\\theta_1)^2}{2\\theta_2^2}=0 \\end{cases} \\] 解得\\(\\hat{\\mu}_L=\\bar X,\\ \\hat{\\sigma}^2_L = S_n^2\\). (可以验证二阶导函数非正定，即取得极大值。) 例 2.8 设总体\\(X\\sim U[a,b]\\), 从中抽取样本\\(X_1,\\dots,X_n\\)的观测值为\\(x_1,\\dots,x_n\\). 求参数\\(a,b\\)的最大似然估计。 解. 似然函数为 \\[ L(x_1,\\dots,x_n;a,b)=\\frac{1}{(b-a)^n}\\prod_{i=1}^{n} 1\\{a\\le x_i\\le b\\}\\] 注意到\\(L\\)关于\\(a,b\\)不可微。容易观察到，当\\(a=\\min_{i=1,\\dots,n}\\{x_i\\},\\ b=\\max_{i=1,\\dots,n}\\{x_i\\}\\)时\\(L\\)取得最大值。故 \\[\\hat{a}_L = X_{(1)},\\ \\hat{b}_L = X_{(n)}.\\] 关于最大似然估计的一些说明: 最大似然估计的不变性：如果\\(\\hat{\\theta}\\)是\\(\\theta\\)的最大似然估计，则对任一函数\\(g(\\theta)\\), 其最大似然估计为\\(g(\\hat{\\theta})\\). 当分布中有多余的参数或者数据为截尾或缺失时，似然函数的求极大值比较困难。针对这种问题，文献 Dempster, A.P.; Laird, N.M.; Rubin, D.B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B. 39 (1): 1–38. (cited by 54539, 2018/8/18) 提出了一种有效的Expectation–Maximization (EM)算法。 2.1.3 矩估计与最大似然估计的对比 矩估计法（也称数字特征法） 直观意义比较明显，但要求总体\\(k\\)阶矩存在。 缺点是不唯一，此时尽量使用样本低阶矩。 观测值受异常值影响较大，不够稳健，实际中避免使用样本高阶矩。 估计值可能不落在参数空间 极大似然估计法 具有一些理论上的优点（不变性、渐近正态性） 缺点是如果似然函数不可微，没有一般的求解法则。 分布名称 记号 期望 方差 矩估计 极大似然估计 0-1分布 \\(B(1,p)\\) \\(p\\) \\(pq\\) \\(\\hat p_M=\\bar{X}\\) \\(\\hat p_L=\\bar{X}\\) 泊松分布 \\(Pois(\\lambda)\\) \\(\\lambda\\) \\(\\lambda\\) \\(\\hat{\\lambda}_M=\\bar{X}\\) \\(\\hat{\\lambda}_L=\\bar{X}\\) 几何分布 \\(Geo(p)\\) \\(1/p\\) \\(q/p^2\\) \\(\\hat p_M=1/\\bar{X}\\) \\(\\hat p_L=1/\\bar{X}\\) 均匀分布 \\(\\mathbb{U}[a,b]\\) \\((a+b)/2\\) \\((b-a)^2/12\\) \\(\\hat{a}_M=\\bar X-\\sqrt{3}S_n\\) \\(\\hat{a}_L=X_{(1)}\\) \\(\\hat{b}_M=\\bar X+\\sqrt{3}S_n\\) \\(\\hat{b}_L=X_{(n)}\\) 指数分布 \\(Exp(\\lambda)\\) \\(1/\\lambda\\) \\(1/\\lambda^2\\) \\(\\hat{\\lambda}_M=1/\\bar{X}\\) \\(\\hat{\\lambda}_L=1/\\bar{X}\\) 正态分布 \\(N(\\mu,\\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) \\(\\hat{\\mu}_M=\\bar X\\) \\(\\hat{\\mu}_L=\\bar X\\) \\(\\hat{\\sigma}^2_M = S_n^2\\) \\(\\hat{\\sigma}^2_L = S_n^2\\) 2.1.4 混合正态分布的参数估计 假设总体\\(X\\)的分布为：以概率\\(\\lambda\\)服从\\(N(\\mu_1,\\sigma_1^2)\\), 以概率\\(1-\\lambda\\)服从\\(N(\\mu_2,\\sigma_2^2)\\)。该混合分布的密度函数为 \\[f(x;\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2)=\\frac{\\lambda}{\\sqrt{2\\pi}\\sigma_1}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}}+\\frac{1-\\lambda}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}}.\\] 样本似然函数为： \\[L(\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2) = \\prod_{i=1}^n \\left[\\frac{\\lambda}{\\sqrt{2\\pi}\\sigma_1}e^{-\\frac{(x_i-\\mu_1)^2}{2\\sigma_1^2}}+\\frac{1-\\lambda}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{(x_i-\\mu_2)^2}{2\\sigma_2^2}}\\right].\\] 不难发现，似然函数是无界的（见习题），所以最大似然估计不存在。然而，如果我们有先验的信息：\\(\\sigma_1=k\\sigma_2\\)，其中\\(k\\)是已知的数（比如1）。似然函数则可表示为： \\[L(\\lambda,\\mu_1,\\mu_2,\\sigma_2^2) =\\prod_{i=1}^n\\left[ \\frac{\\lambda}{\\sqrt{2\\pi}k\\sigma_2}e^{-\\frac{(x_i-\\mu_1)^2}{2k^2\\sigma_2^2}}+\\frac{1-\\lambda}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{(x_i-\\mu_2)^2}{2\\sigma_2^2}}\\right]. \\] 此时，似然函数是有界的，所以最大似然估计存在。 设\\(Y_i=1\\)表示\\(X_i\\)来自\\(N(\\mu_1,\\sigma_1^2)\\)分布，\\(Y_i=2\\)表示\\(X_i\\)来自\\(N(\\mu_2,\\sigma_2^2)\\)分布。假设我们可以观测\\(Y_i\\)的值，基于样本\\((X_i,Y_i),i=1,\\dots,n\\)，我们可以得到\\(\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\)的最大似然估计。 令\\(I = \\{i=1,\\dots,n|y_i=1\\}\\), 则似然函数为 \\[\\begin{align*} L(\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2)&amp;=\\prod_{i\\in I} \\frac{\\lambda}{\\sqrt{2\\pi}\\sigma_1}e^{-\\frac{(x_i-\\mu_1)^2}{2\\sigma_1^2}} \\prod_{i\\notin I}\\frac{1-\\lambda}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{(x_i-\\mu_2)^2}{2\\sigma_2^2}}\\\\ &amp;=\\lambda^{|I|}(1-\\lambda)^{n-|I|}\\prod_{i\\in I} \\frac{1}{\\sqrt{2\\pi}\\sigma_1}e^{-\\frac{(x_i-\\mu_1)^2}{2\\sigma_1^2}} \\prod_{i\\notin I}\\frac{1}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{(x_i-\\mu_2)^2}{2\\sigma_2^2}}. \\end{align*}\\] 则只需分别求出上式中三部分的最大值点即可。对于第一部分，不难发现\\(\\hat\\lambda = \\frac 1 n\\sum_{i=1}^n1\\{Y_i=1\\}\\). 后两部分等价于求样本为\\(\\{X_i,i\\in I\\}\\)和\\(\\{X_i,i\\notin I\\}\\)时，对应正态总体的最大似然估计，所以最大似然估计分别为 \\[\\hat{\\mu}_1=\\frac{1}{|I|}\\sum_{i\\in I} X_i=\\frac{\\sum_{i=1}^nX_i\\cdot1\\{Y_i=1\\}}{\\sum_{i=1}^n1\\{Y_i=1\\}},\\] \\[\\hat{\\sigma_1^2} = \\frac{1}{|I|} \\sum_{i\\in I}(X_i-\\hat{\\mu}_1)^2=\\frac{\\sum_{i=1}^n(X_i-\\hat\\mu_1)^2\\cdot1\\{Y_i=1\\}}{\\sum_{i=1}^n1\\{Y_i=1\\}},\\] \\[\\hat{\\mu}_2=\\frac{\\sum_{i=1}^nX_i\\cdot1\\{Y_i=2\\}}{\\sum_{i=1}^n1\\{Y_i=2\\}},\\ \\hat{\\sigma_2^2} =\\frac{\\sum_{i=1}^n(X_i-\\hat\\mu_2)^2\\cdot1\\{Y_i=2\\}}{\\sum_{i=1}^n1\\{Y_i=2\\}}.\\] 上述三式要求分母，否则相应部分的估计量可以为任意常数。 上述问题不难推广到\\(K\\ge 3\\)个不同正态分布的混合的情形。然而，大部分问题，\\(Y_i\\)是不可观测的（即无标签）。Kiefer (1978) 证明了样本\\(X_i\\)的似然方程的某个解（对应局部极大值）也是有效的估计量，同样会收敛到真值。这说明了，似然方程的极大值点也可以作为一个有效的估计量。如何找到似然方程的极大值点？我们可以利用EM算法找出似然函数局部极大值点。参考： Andrew Ng’s lecture notes 1 Andrew Ng’s lecture notes 2 另一方面，对于混合正态分布，我们可以通过矩法得到五个参数的估计量。Cohen (1967)给出了矩法估计的一般公式，转化成求一个9次多项式方程的负根。 2.1.5 EM算法 Expectation-Maximization (EM) 算法是由Dempster et al. (1977)提出。该算法的推导用到Jensen不等式。 定理 2.1 (Jensen不等式) 设\\(A\\)为\\(\\mathbb{R}^k\\)中凸集，\\(f(x)\\)为\\(A\\)上凸函数，即对任意\\(\\lambda\\in[0,1], x,y\\in A\\)，恒有 \\[ f(\\lambda x+(1-\\lambda)y)\\le \\lambda f(x)+(1-\\lambda)f(y). \\] 如果\\(k\\)维随机向量\\(X\\)满足\\(P(X\\in A)=1\\)，则有 \\[f(E[X])\\le E[f(X)].\\] 进而，如果\\(f\\)为严格凸函数，\\(f(E[X])=E[f(X)]\\)当且仅当\\(P(X=E[X])=1\\)，即\\(X\\)为常数向量。 换言之，如果\\(f\\)为严格凸函数且\\(X\\)不为常数向量，则\\(f(E[X])&lt;E[f(X)]\\). 如果\\(f\\)是凹函数，则定理中的不等式变号。注意到\\(\\ln x\\)是\\((0,\\infty)\\)上的严格凹函数，应用Jensen不等式得到下面一个结果。 例 2.9 如果\\(X\\)为一个取值为正的非常数随机变量，则有\\(E[\\ln X]&lt;\\ln (E[X])\\). EM算法用于求解不完备数据的极大似然估计。 设完备数据为\\(x=(y,z)\\)，其中\\(y\\)为观测数据（向量），\\(z\\)不可观测，称为潜变量。直接对观测数据求最大似然估计比较困难。 注意到观测数据的似然函数为 \\[\\begin{align*} L(\\theta|y) &amp;= \\int p(y,z|\\theta)dz=\\int q(z|\\eta)\\frac{p(y,z|\\theta)}{q(z|\\eta)}dz\\\\ &amp;=E_{q(z|\\eta)}\\left[\\frac{p(y,z|\\theta)}{q(z|\\eta)}\\right], \\end{align*}\\] 其中\\(p(y,z|\\theta)\\)为完全数据的密度函数, \\(q(z|\\eta)\\)为密度函数且满足：如果\\(q(z|\\eta)=0\\)，则\\(p(y,z|\\theta)=0\\)。对数似然函数 \\[\\ell(\\theta|y)= \\ln \\left(E_{q(z|\\eta)}\\left[\\frac{p(y,z|\\theta)}{q(z|\\eta)}\\right]\\right)\\ge E_{q(z|\\eta)}\\left[\\ln \\left(\\frac{p(y,z|\\theta)}{q(z|\\eta)}\\right)\\right]=:Q(q(z|\\eta),\\theta).\\] 上式用到Jensen不等式，当且仅当\\(p(y,z|\\theta)/q(z|\\eta)\\)为常数（即不依赖\\(z\\)）时，上式等号成立，由于\\(\\int q(z|\\eta)dz =1\\)，不难得到 \\[q(z|\\eta)=\\frac{p(y,z|\\theta)}{p(y|\\theta)}=p(z|y,\\theta)=:q^*(z|\\theta).\\] 此时，\\(\\ell(\\theta|y) = Q(q^*(z|\\theta),\\theta).\\) 否则\\(\\ell(\\theta|y)&gt;Q(q(z|\\eta),\\theta)\\). 假设第\\(t\\)步的估计值为\\(\\theta_t\\)，构造如下迭代算法 \\[\\theta_{t+1} = \\arg\\max_{\\theta\\in\\Theta} Q(q^*(z|\\theta_t),\\theta).\\] 注意到，\\(\\ell(\\theta_{t+1}|y)\\ge Q(q^*(z|\\theta_t),\\theta_{t+1})\\ge Q(q^*(z|\\theta_t),\\theta_{t})=\\ell(\\theta_t|y)\\). 如果\\(p(y,z|\\theta_{t+1})/p(y,z|\\theta_t)\\)与\\(z\\)相关（即不为常数向量），则有\\(\\ell(\\theta_{t+1}|y)&gt;\\ell(\\theta_t|y)\\). 所以，该迭代算法使得似然函数单调递增。如果\\(\\theta_t\\)收敛到\\(\\theta^*\\)，那么在满足一定条件下，\\(\\theta^*\\)为似然函数的驻点。 注意到 \\[Q(q^*(z|\\theta_t),\\theta)=E_{p(z|y,\\theta_t)}\\left[\\ln \\left(p(y,z|\\theta)\\right)\\right]-E_{p(z|y,\\theta_t)}\\left[\\ln \\left(p(z|y,\\theta_t)\\right)\\right].\\] 上式等式最后一项与\\(\\theta\\)无关，所以优化问题等价于 \\[\\theta_{t+1} = \\arg\\max_{\\theta\\in\\Theta} E_{p(z|y,\\theta_t)}\\left[\\ln \\left(p(y,z|\\theta)\\right)\\right].\\] EM算法包含两步： 第一步求期望\\(E_{p(z|y,\\theta_t)}\\left[\\ln \\left(p(y,z|\\theta)\\right)\\right]\\), 该期望称为预期的对数似然函数。 第二步则求预期的对数似然函数最大值。 以下定理保证该迭代算法收敛到似然函数的驻点。 定理 2.2 如果\\(E_{p(z|y,\\theta_t)}\\left[\\ln \\left(p(y,z|\\theta)\\right)\\right]\\)关于\\(\\theta\\)和\\(\\theta_t\\)连续，则\\(\\theta_{t}\\)收敛到似然函数\\(L(\\theta|y)\\)的某一驻点（局部极大值点或者鞍点）。 证明. 证明见Lehmann and Casella的《点估计理论》P460。 例 2.10 假设完备数据\\((y,z)\\)服从指数型分布族 \\[p(y,z|\\theta) = c(\\theta)\\exp\\left(\\sum_{i=1}^k T_i(y,z)c_i(\\theta)\\right)h(y,z).\\] 则对数似然函数为： \\[ \\ln p(y,z|\\theta) = \\ln c(\\theta)+\\sum_{i=1}^k T_i(y,z)c_i(\\theta)+\\ln h(y,z). \\] 则 \\[\\begin{align*} \\theta_{t+1} &amp;= \\arg\\max_{\\theta\\in\\Theta} E_{p(z|y,\\theta_t)}\\left[\\ln c(\\theta)+\\sum_{i=1}^k T_i(y,z)c_i(\\theta)+\\ln h(y,z)\\right]\\\\ &amp;=\\arg\\max_{\\theta\\in\\Theta} \\left\\lbrace\\ln c(\\theta)+\\sum_{i=1}^k c_i(\\theta)E_{p(z|y,\\theta_t)}[T_i(y,z)]\\right\\rbrace. \\end{align*}\\] 要求解该优化问题，只须求\\(E_{p(z|y,\\theta_t)}[T_i(y,z)]=E_{\\theta_t}[T_i(y,z)|y]\\)的表达式即可。 例 2.11 再次考虑混合正态分布。设完全数据\\(x=((y_i,z_i),i=1,\\dots,n)=(y,z)\\)，其中\\(z_i=1\\)或者\\(2\\)， 当\\(z_i=1\\)是表示\\(Y_i\\sim N(\\mu_1,\\sigma_1^2)\\)，当\\(z_i=2\\)是表示\\(Y_i\\sim N(\\mu_2,\\sigma_2^2)\\). 这里分类变量\\(z_i\\)是不可以观测。注意到直接对观测数据\\(y\\)构建似然函数，然后求极大值是非常困难的。下面我们通过EM算法求解。记\\(\\theta=(\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2,\\lambda)\\). 完全数据的对数似然函数为 \\[ \\ln p(y,z|\\theta)=\\sum_{i=1}^n \\ln p(y_i,z_i|\\theta) = \\sum_{i=1}^n \\ln p(y_i|z_i,\\theta)+\\ln p(z_i|\\theta). \\] 首先计算 \\[\\begin{align*} &amp;E_{p(z|y,\\theta_t)}[\\ln p(y,z|\\theta)]=E_{p(z|y,\\theta_t)}\\left[\\sum_{i=1}^n \\ln p(y_i|z_i,\\theta)+\\ln p(z_i|\\theta)\\right]\\\\ &amp;=\\sum_{i=1}^n \\sum_{j=1}^2 P(z_i=j|y_i,\\theta_t)[-\\frac 1 2\\ln (2\\pi)-\\ln\\sigma_j-\\frac{(x_i-\\mu_j)^2}{2\\sigma_j^2}+\\ln p(z_i=j|\\theta)]\\\\ &amp;=-\\frac 12\\sum_{i=1}^n \\sum_{j=1}^2 w_{ij}[\\ln \\sigma_j^2+(y_i-\\mu_j)^2/\\sigma_j^2]+\\sum_{i=1}^n [w_{i1}\\ln\\lambda+w_{i2}\\ln(1-\\lambda)]\\\\ &amp;=s_1(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)+s_2(\\lambda). \\end{align*}\\] 其中， \\[\\begin{align*} w_{ij}&amp;= P(z_i=j|y_i,\\theta_t)=\\frac{p(y_i|z_i=j,\\theta_t)p(z_i=j|\\theta_t)}{\\sum_{j=1}^2p(y_i|z_i=j,\\theta_t)p(z_i=j|\\theta_t)}\\\\ &amp;=\\frac{\\phi(y_i;\\mu_{j,t},\\sigma_{j,t}^2)(\\lambda_{t}1\\{j=1\\}+(1-\\lambda_{t})1\\{j=2\\})}{\\lambda_{t}\\phi(y_i;\\mu_{1,t},\\sigma_{1,t}^2)+(1-\\lambda_{t})\\phi(y_i;\\mu_{2,t},\\sigma_{2,t}^2)}, \\end{align*}\\] \\[s_1(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)=-\\frac 12\\sum_{i=1}^n \\sum_{j=1}^2 w_{ij}[\\ln \\sigma_j^2+(y_i-\\mu_j)^2/\\sigma_j^2],\\] \\[s_2(\\lambda)=\\sum_{i=1}^n [w_{i1}\\ln\\lambda+w_{i2}\\ln(1-\\lambda)],\\] \\(\\phi(x;\\mu,\\sigma^2)\\)为\\(N(\\mu,\\sigma^2)\\)的密度函数。 注意到\\(w_{ij}\\)与\\(\\theta\\)无关，可视为常数。对\\(E_{p(z|y,\\theta_t)}[\\ln p(y,z|\\theta)]\\)求最大值，等价于分别对\\(s_1,s_2\\)两部分求最大值。先考虑对第一部分\\(s_1(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)\\)求最大值. 对\\(\\mu_j\\)求偏导得到 \\[\\frac{\\partial s_1(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)}{\\partial \\mu_j}=\\sum_{i=1}^n w_{ij}[(y_i-\\mu_j)/\\sigma_j^2]=0.\\] 于是有 \\[\\mu_{j,t+1}=\\frac{\\sum_{i=1}^nw_{ij}y_{i}}{\\sum_{i=1}^nw_{ij}}, j=1,2.\\] 对求\\(\\sigma^2_j\\)偏导得到 \\[\\frac{\\partial s_1(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)}{\\partial \\sigma^2_j}=-\\frac 12\\sum_{i=1}^n w_{ij}\\left[\\frac 1{\\sigma_j^2}-\\frac{(y_i-\\mu_j)^2}{\\sigma_j^4}\\right]=0.\\] 于是有 \\[\\sigma_{j,t+1}^2=\\frac{\\sum_{i=1}^nw_{ij}(y_i-\\mu_{j,t+1})^2}{\\sum_{i=1}^nw_{ij}}.\\] 考虑第二部分\\(s_2(\\lambda)\\)求最大值. 对\\(\\lambda\\)求导得， \\[s_2&#39;(\\lambda)=\\sum_{i=1}^n\\left(\\frac{w_{i1}}{\\lambda}-\\frac{w_{i2}}{1-\\lambda}\\right)=0.\\] 于是有 \\[\\lambda_{t+1}=\\frac{\\sum_{i=1}^nw_{i1}}{\\sum_{i=1}^n\\sum_{j=1}^2w_{ij}}=\\frac{1}{n}\\sum_{i=1}^nw_{i1}.\\] 图 2.2: EM算法求解混合正态分布，红色为真实值，n=10000 2.2 估计的优良性标准 前面介绍了点估计中两种经典的方法，对于同一个问题，两种方法得到的估计量可能不一样。自然要问，哪种最好？ 本节讨论估计量的优良性质。 2.2.1 无偏性 定义 2.3 设总体\\(X\\sim F(x;\\theta),\\theta\\in \\Theta\\), \\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量。 无偏估计量： \\[E[T(X_1,\\dots,X_n)]=g(\\theta), \\forall \\theta\\in \\Theta\\] 渐近无偏估计量： \\[\\lim_{n\\to \\infty}E[T(X_1,\\dots,X_n)]=g(\\theta), \\forall \\theta\\in \\Theta\\] 无偏性意味着：虽然估计量\\(T\\)由于随机可能偏离真值\\(g(\\theta)\\), 但取其平均值（期望）却等于\\(g(\\theta)\\). 即没有系统偏差。 样本均值是总体的均值的无偏估计，即\\(E[\\bar X]=E[X]\\) 样本方差是总体方差的渐近无偏估计，即\\(\\lim_{n\\to \\infty}E[S_n^{2}]=Var[X]\\) 修正样本方差是总体方差的无偏估计，即\\(E[S_n^{*2}]=Var[X]\\) 如果\\(g(\\theta)\\)存在无偏估计量，则称\\(g(\\theta)\\)是可估的。但注意不是所有的参数估计都存在一个无偏估计量。 例 2.12 假设总体\\(X\\sim B(k,\\theta)\\), 其中\\(k\\ge 1\\)已知，\\(\\theta\\in (0,1)\\)未知。 证明\\(1/\\theta\\)不存在无偏估计量。 证明. 假设\\(T(X_1,\\dots,X_n)\\)为\\(1/\\theta\\)无偏估计量。不妨考虑\\(n=1\\)情形， \\[E[T] = \\sum_{i=0}^k C_k^i \\theta^i(1-\\theta)^{k-i} T(i)=:h(\\theta).\\] 显然当\\(\\theta\\to 0\\), \\(h(\\theta)\\to T(0)\\), 但\\(\\frac{1}{\\theta}\\to \\infty\\). 所以，\\(E[T]\\neq 1/\\theta\\). 此外，还应当注意一点是：无偏估计量不一定比有偏的估计量好（如下图所示）。下节给出一个评判标准，并通过一个具体的例子说明有时候有偏估计量更好。 图 2.3: 无偏VS有偏 2.2.2 均方误差 定义 2.4 设\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量，其均方误差 (mean squared error, MSE)为 \\[M_{\\theta}(T):=E_\\theta[(T(X_1,\\dots,X_n)-g(\\theta))^2].\\] 均方根误差 (root mean squared error, RMSE)为 \\[R_{\\theta}(T):=\\sqrt{E_\\theta[(T(X_1,\\dots,X_n)-g(\\theta))^2]}.\\] 注意到： \\[M_{\\theta}(T)=(E[T]-g(\\theta))^2+Var(T)=\\text{偏差}^2+\\text{方差}.\\] 注：如果\\(T\\)是\\(g(\\theta)\\)的无偏估计，则\\(M_{\\theta}(T)=Var(T)\\) 比较两个估计量的优劣 定义 2.5 若\\(T_1(X_1,\\dots,X_n)\\)和\\(T_2(X_1,\\dots,X_n)\\)都为\\(g(\\theta)\\)的估计量， 如果\\(M_{\\theta}(T_1)\\le M_{\\theta}(T_2),\\forall \\theta\\in \\Theta\\), 则称\\(T_1\\)不次于\\(T_2\\)。 在此基础上，如果存在一个\\(\\theta_0\\in\\Theta\\)使得\\(M_{\\theta_0}(T_1)&lt; M_{\\theta_0}(T_2)\\), 则称\\(T_1\\)比\\(T_2\\)有效。 例 2.13 设总体\\(X\\)的期望\\(\\mu\\)方差为\\(\\sigma^2\\), \\(X_1,\\dots,X_n\\)为其样本(\\(n&gt;1\\))，证明下列估计量\\(\\hat{\\mu} = \\sum_{i=1} C_iX_i\\)为\\(\\mu\\)的无偏估计的充要条件是\\(\\sum_{i=1}^nC_i = 1.\\) 在满足该条件前提下，\\(C_i\\)取何值时，\\(\\hat{\\mu}\\)的最有效。 解. \\(E[\\hat{\\mu}]=\\mu\\Leftrightarrow \\sum_{i=1}^nC_i = 1\\) \\[Var[\\hat{\\mu}]=\\sigma^2\\sum_{i=1}^nC_i^2\\ge \\sigma^2\\frac{(C_1+\\dots+C_n)^2}{n}=\\frac{\\sigma^2}{n}.\\] 而且唯一的最小值在\\(C_i=1/n,i=1,\\dots,n\\)处取得。 例 2.14 设\\(X_1,\\dots,X_n\\)为\\(N(\\mu,\\sigma^2)\\)分布的样本，参数\\(\\mu,\\sigma^2\\)未知。样本方差\\(S_n^2\\)与修正样本方差\\(S_n^{*2}\\)作为\\(\\sigma^2\\)的两种估计量，哪个更有效？ 解. 由于\\(S_n^{*2}\\)是无偏的，所以均方误差 \\[M(S_n^{*2}) = Var[S_n^{*2}]=\\frac{2\\sigma^4}{n-1}.\\] 对\\(S_n^{2}\\), 其均方误差为 \\[\\begin{align*} M(S_n) &amp;= Var[S_n^{2}]+(E[S_n^2]-\\sigma^2)^2\\\\ &amp;=\\frac{2(n-1)\\sigma^4}{n^2}+(\\frac{(n-1)\\sigma^2}{n}-\\sigma^2)^2 \\\\&amp;=\\frac{(2n-1)\\sigma^4}{n^2}. \\end{align*}\\] 又 \\[\\frac{M(S_n^{*2})}{M(S_n^{2})}=\\frac{2n^2}{(n-1)(2n-1)}&gt;1\\] 所以，\\(S_n^{2}\\)比\\(S_n^{*2}\\)有效。 启发：无偏估计量不一定是最有效的。 思考：对于上题，考虑估计量\\(T_k=k\\sum_{i=1}^n(X_i-\\bar X)^2\\)，其中\\(k\\)为给定常数。特别地，当\\(k=1/n\\)时，\\(T_k=S_n^2\\)；当\\(k=1/(n-1)\\)时，\\(T_k=S_n^{*2}\\)。样本方差\\(S_n^{2}\\)是不是比其它的\\(T_k\\)更有效？如果不是，那么最优的\\(k\\)是多少？ 2.2.3 一致最小方差无偏估计 定义 2.6 如果\\(T_0(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的无偏估计，如果对于\\(g(\\theta)\\)的任意无偏估计量\\(T(X_1,\\dots,X_n)\\)都有 \\[Var[T_0]\\le Var[T],\\ \\forall\\theta\\in\\Theta,\\] 则称\\(T_0\\)为\\(g(\\theta)\\)的一致最小方差无偏估计量 (uniformly minimum-variance unbiased estimator, UMVUE)。 如果\\(Var[T_0]\\le Var[T]\\)在\\(\\theta=\\theta_0\\)处成立，则称\\(T_0\\)为\\(g(\\theta)\\)的局部最小方差无偏估计量 (locally minimum-variance unbiased estimator, LMVUE)。 例 2.15 令\\(X\\)为取值\\(-1,0,1,\\dots\\)的离散型随机变量，分布为 \\[P(X=-1)=p,\\ P(X=k)=q^2p^k,\\ k=0,1,\\dots,\\] 其中\\(0&lt;p&lt;1,\\ q=1-p\\)。现考虑用\\(X\\)来估计\\(p\\)和\\(q^2\\). 对于估计\\(p\\)，一个简单的无偏估计为 \\[T_1 = 1\\{X=-1\\}.\\] 对于估计\\(q^2\\)，一个简单的无偏估计为 \\[T_2 = 1\\{X=0\\}.\\] 为求解最小方差估计量，现介绍以下引理。 引理 2.1 设\\(T_0\\)为\\(g(\\theta)\\)的任一无偏估计量，则所有的无偏估计量都可以表示为\\(T=T_0-U\\)，其中\\(U\\)为“零”的无偏估计，即\\(E[U]=0\\). 对于上例，考虑“零”的无偏估计\\(U=U(X)\\)。注意到， \\[\\begin{align*} E[U]&amp;=\\sum_{i=-1}^\\infty U(i)P(X=i)\\\\ &amp;=pU(-1)+q^2\\sum_{i=0}^\\infty U(i)p^i=0,\\ \\forall p\\in(0,1). \\end{align*}\\] 令\\(p\\to 0\\), 可得\\(U(0)=0\\). 于是， \\[\\sum_{k=1}^\\infty U(k)p^{k-1} + \\frac{U(-1)}{(1-p)^2}=0.\\] 注意到\\(1/(1-p)^2=\\sum_{k=1}^\\infty kp^{k-1}\\). 则有 \\[\\sum_{k=1}^\\infty [U(k)+kU(-1)]p^{k-1} =0.\\] 由于上式对于任意\\(p\\in (0,1)\\)，则所有系数应该为零，即\\(U(k)=-kU(-1),k=1,\\dots,\\infty\\). 这意味着，\\(U\\)为“零”的无偏估计量当且仅当\\(U(k)=ak\\)对于所有的\\(k=-1,0,1,\\dots\\)和某个\\(a\\)。要使方差最小等价于最小化二阶矩： \\[E[(T_i-U)^2]=\\sum_{k=-1}^\\infty P(X=k)[T_i(k)-ak]^2,\\ i=1,2.\\] 两种情况下，最小值点分别为 \\[a_1^* = -\\frac{p}{p+q^2\\sum_{k=1}^\\infty k^2p^k},\\ a_2^*=0.\\] 由于\\(a_2^*\\)不依赖\\(p\\)，所以\\(T_2^*=T_2-a_2^*X=T_2\\)是UMUVE. 但\\(a_1^*\\)依赖\\(p\\)，所以\\(T_1^*=T_1-a_1^*X\\)是LMUVE，对于估计\\(p\\)，不存在一致最小方差无偏估计量。 通过这个例子，我们发现对于同一总体，有些参数估计问题存在UMVUE，有些则不存在。那么自然要问，什么情况下所有可估的参数一定存在UMVUE？Blackwell, Rao, Lehmann, Scheffe等统计学家获得了一系列寻求UMVUE的理论和方法。为解决这个问题，下面先引入完全统计量的概念。 定义 2.7 设\\(T(X_1,\\dots,X_n)\\)为统计量。如果对任何(Borel可测)函数\\(u(\\cdot)\\), 只要\\(E[u(T)]=0\\)(对一切\\(\\theta\\))就可以推出\\(P(u(T)=0)=1\\), 则称统计量\\(T\\)为参数\\(\\theta\\)的完全的统计量。 如何理解统计量的完全性？假设非常数统计量\\(T\\)的分布与参数\\(\\theta\\)无关，则对于所有的函数\\(u(x)\\), \\(E[u(T)]\\)的值为常数\\(c\\)，与\\(\\theta\\)无关。这表明\\(E[u(T)-c]=0\\)对任意的\\(\\theta\\)成立，但由于\\(u\\)的任意性，\\(u(T)-c\\)可以不为零（比如取\\(u(T)=T\\)），故\\(T\\)不可能为完全统计量。如果统计量\\(T\\)的分布与参数\\(\\theta\\)无关，我们称此类统计量为辅助统计量。这类统计量不包含参数信息，故认为是“辅助的”。完全性排除了非常数辅助统计量。 例 2.16 设\\(X_1,\\dots,X_n\\)是来自\\(N(\\mu,1)\\)的样本。显然\\(T_1=X_1-X_2\\), \\(T_2=X_1-\\bar X\\)的分布与参数\\(\\mu\\)无关，故它们不是完全统计量。 例 2.17 设\\(X_1,\\dots,X_n\\)是来自伯努利分布\\(B(1,p)\\)的样本。证明\\(T=\\sum_{i=1}^n X_i\\)是参数\\(p\\)的完全统计量。 证明. 注意到\\(T\\sim B(n,p)\\)。假设对一切\\(p\\in(0,1)\\)都有\\(E[u(T)]=0\\)，则 \\[E[u(T)]=\\sum_{i=0}^{n}u(i)C_n^ip^i(1-p)^{n-i}= 0.\\] 令\\(y=p/1-p\\)，则对任意\\(y\\in \\mathbb{R}\\)都有\\(\\sum_{i=0}^n C_n^iu(i)y^i=0\\)。等式左边为\\(y\\)的多项式，所以该多项式的所有系数均为0，即\\(u(i)=0,\\ i=0,\\dots,n\\). 这意味着\\(u(T)=0\\). 这就说明\\(T\\)为完全统计量。 定理 2.3 考虑指数型分布族\\(\\mathcal{F}=\\{f_\\theta(x);\\theta\\in\\Theta\\}\\)中的分布\\(f_\\theta(x)\\)（分布列或者密度函数）都可以表示成如下形式： \\[f_\\theta(x)=c(\\theta)\\exp\\{\\sum_{j=1}^kc_j(\\theta)T_j(x)\\}h(x).\\] 如果\\(\\Theta\\)有内点，则该分布族的充分统计量 \\[\\left(\\sum_{i=1}^nT_1(x_i),\\dots,\\sum_{i=1}^nT_k(x_i)\\right)\\] 是完全的。 证明. 证明略。 定理 2.4 (Black-Lehmann-Scheffe定理) 考虑参数分布族\\(\\mathcal{F}=\\{f_\\theta(x);\\theta\\in\\Theta\\}\\)，设\\(T(X_1,\\dots,X_n)\\)为其完全的充分统计量。则所有的可估参数\\(g(\\theta)\\)均存在最小方差无偏估计且可表示为\\(T\\)的一个函数\\(\\psi(T)\\)（该表示在概率意义是唯一的）。 证明. 设\\(T_1\\)为\\(g(\\theta)\\)的任意无偏估计量，记\\(\\psi_1(t) = E[T_1|T=t]\\). 由于\\(T\\)是充分统计量，所以\\(\\psi_1(t)\\)与参数\\(\\theta\\)无关，即\\(\\psi_1(T)=E[T_1|T]\\)为一统计量。由全期望公式知，\\(E[\\psi_1(T)]=E[E[T_1|T]]=E[T_1]=g(\\theta)\\). 所以\\(\\psi_1(T)\\)为\\(g(\\theta)\\)的无偏估计量。 由全方差公式知， \\[Var[T_1] = E[Var[T_1|T]]+Var[E[T_1|T]]\\ge Var[\\psi_1(T)].\\] 同理，设\\(T_2\\)为\\(g(\\theta)\\)的另一无偏估计量，\\(\\psi_2(T)=E[T_2|T]\\)同样是无偏的，且\\(Var[\\psi_2(T)]\\le Var[T_2].\\) 令\\(u(x) = \\psi_2(x)-\\psi_1(x)\\). 则\\(E[u(T)]=E[\\psi_2(T)]-E[\\psi_1(T)]=0\\). 由于\\(T\\)是完全统计量，所以\\(P(\\psi_2(T)=\\psi_1(T))=1\\), 这表明\\(\\psi_2(T)\\)和\\(\\psi_1(T)\\)在概率意义上是相等的，为UMVUE. 该定理表明如果存在完全充分统计量\\(T\\)，UMVU估计量必然可以表示为该统计量的函数\\(\\psi(T)\\)。根据这个性质可以求解UMVU估计量，即求解方程 \\[E[\\psi(T)]=g(\\theta), \\forall \\theta\\in\\Theta.\\] 例 2.18 设\\(X\\sim B(1,p)\\)，其中\\(p\\in(0,1)\\). 求\\(g(p)=p(1-p)\\)的UMVUE. 解. 由定理2.3知，\\(T=\\sum_{i=1}^n X_i\\)为完全充分统计量。所以，对任意的\\(p\\in(0,1)\\)恒有 \\[E[\\psi(T)]=\\sum_{i=0}^n \\psi(i)C_n^i p^i(1-p)^{n-i} = p(1-p).\\] 令\\(\\rho = p/(1-p)\\), 则\\(p = \\rho/(1+\\rho)\\). 上式等价于 \\[\\sum_{i=0}^n \\psi(i)C_n^i \\rho^i = \\rho(1+\\rho)^{n-2}=\\sum_{i=1}^{n-1}C_{n-2}^{i-1}\\rho^i.\\] 比较系数可得 \\[\\psi(x)=\\frac{(n-x)x}{n(n-1)}.\\] 所以，\\(p(1-p)\\)的UMUV估计量为\\(n(1-\\bar X)\\bar X/(n-1)\\). 注意到\\(X_i\\)取值为\\(0,1\\), 所以\\(X_i=X_i^2\\). 于是， \\[S_n^2=\\frac 1n\\sum_{i=1}^n X_i^2-(\\bar X)^2=\\frac 1n\\sum_{i=1}^n X_i-(\\bar X)^2=\\bar X(1-\\bar X).\\] 因此，\\(S_n^{*2}=nS_n^2/(n-1)=n(1-\\bar X)\\bar X/(n-1)\\)为上述推导的UMUVE. 另外还可以通过求条件期望的方式得到UMVU估计量。假设\\(T_1\\)是\\(g(\\theta)\\)的任意无偏估计量，\\(T\\)为完全充分统计量。则\\(E[T_1|T]\\)为UMVU估计量。这种方法的难点在于计算条件期望。 对于正态分布总体\\(X\\sim N(\\mu,\\sigma^2)\\)，我们知道\\((\\bar X,S_n^2)\\)为完全充分统计量。则\\(\\bar X\\)为\\(\\mu\\)的UMVUE, \\(S_n^{*2}\\)为\\(\\sigma^2\\)的UMVUE. 下面考虑其它参数的UMVUE. 例 2.19 设总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其CDF记为\\(F(x)\\)。 给定\\(\\alpha\\in (0,1)\\)，求分位数\\(F^{-1}(\\alpha)\\)的UMVUE. 假设\\(\\sigma=1\\), 给定\\(u\\in \\mathbb{R}\\)，求\\(F(u)\\)和\\(F&#39;(u)\\)的UMVUE. 解. （1）注意到\\(F^{-1}(\\alpha)=\\mu+u_\\alpha \\sigma\\). 又\\(\\bar X\\)是\\(\\mu\\)的无偏估计。现在先求\\(\\sigma\\)的无偏估计。由抽样分布定理知， \\[Y:=\\frac{nS_n^2}{\\sigma^2}\\sim \\chi^2(n-1).\\] 对任意的\\(r&gt;1-n\\), 有 \\[\\begin{align*} E[(S_n/\\sigma)^r]&amp;=E[n^{-r/2}Y^{r/2}]=n^{-r/2}\\int_0^\\infty x^{r/2}\\frac{x^{\\frac {n-1}2-1}e^{-\\frac x2}}{2^{\\frac {n-1}2}\\Gamma((n-1)/2)}dx\\\\ &amp;=\\frac{2^{\\frac {r+n-1}2}\\Gamma((r+n-1)/2)}{n^{r/2}2^{\\frac {n-1}2}\\Gamma((n-1)/2)}\\int_0^\\infty \\frac{x^{(r+n-1)/2-1}e^{-x/2}}{2^{\\frac {r+n-1}2}\\Gamma((r+n-1)/2)}dx\\\\ &amp;=\\frac{2^{\\frac {r+n-1}2}\\Gamma((r+n-1)/2)}{n^{r/2}2^{\\frac {n-1}2}\\Gamma((n-1)/2)}\\\\ &amp;=\\frac{2^{r/2}\\Gamma((r+n-1)/2)}{n^{r/2}\\Gamma((n-1)/2)}=:K_{n,r}. \\end{align*}\\] 所以， \\[E[S_n^r]=\\left(\\frac 2n\\right)^{r/2}\\frac{\\Gamma((r+n-1)/2)}{\\Gamma((n-1)/2)}\\sigma^r=K_{n,r}\\sigma^r.\\] 特别地，取\\(r=1\\)，则\\(E[S_n]=K_{n,1}\\sigma\\). 所以\\(S_n/K_{n,1}\\)是\\(\\sigma\\)的无偏估计。由此可得，\\(F^{-1}(\\alpha)\\)的一个无偏估计量为 \\[\\bar X+u_\\alpha S_n/K_{n,1}.\\] 该统计量为完全充分统计量\\((\\bar X,S_n^2)\\)的函数，所以是UMVUE. （2）当\\(\\sigma\\)已知时，\\(\\bar X\\)是\\(\\mu\\)的完全充分统计量。令\\(T_1=1\\{X_1\\le u\\}\\)，则有\\(E(T_1)=P(X_1\\le u)=F(u)\\)。所以，\\(T_1\\)为\\(F(u)\\)的无偏估计。考虑 \\[\\begin{align*} E[T_1|\\bar X=\\bar x]&amp;=P(X_1\\le u|\\bar X=\\bar x)=P(X_1-\\bar X\\le u-\\bar x|\\bar X=\\bar x)\\\\ &amp;=P(X_1-\\bar X\\le u-\\bar x)=\\Phi\\left[\\sqrt{\\frac{n}{n-1}}(u-\\bar x)\\right]. \\end{align*}\\] 其中用到\\(\\bar X\\)与\\(X_1-\\bar X\\)独立，且\\(X_1-\\bar X\\sim N(0,(n-1)/n)\\). 因此，\\(\\Phi\\left[\\sqrt{\\frac{n}{n-1}}(u-\\bar X)\\right]\\)为\\(F(u)\\)的UMVUE. 不难发现， \\[\\frac{\\partial }{\\partial u}\\Phi\\left[\\sqrt{\\frac{n}{n-1}}(u-\\bar X)\\right]=\\sqrt{\\frac{n}{n-1}}\\phi\\left[\\sqrt{\\frac{n}{n-1}}(u-\\bar X)\\right]\\] 是\\(F&#39;(u)\\)的无偏估计量，从而为UMVUE. 这里\\(\\Phi\\)和\\(\\phi\\)分别为标准正态分布的CDF和PDF. 上题第二问中，如果\\(\\sigma\\)未知，经过柯尔莫哥洛夫(1950)研究，UMVUE同样存在，但推导过程比较繁琐，这里省略。结果见陈家鼎等教材P26-27. UMVUE考虑无偏估计中最好的一种。注意到在正态总体下，总体方差的UMVUE为修正样本方差，修正样本方差的均方误差却大于样本方差的均方误差。这表明如果考虑所有类型的估计量，UMVUE不一定是最好的。那为什么不在所有的估计量中研究所谓最好的估计量呢？是否可以相应地定义“一致最小均方误差估计量”？然而，这样的估计量是不存在的。不妨\\(g(\\theta)\\)不恒为一个常数。假设存在它的估计量\\(T_0\\)，对于所有的估计量\\(T\\)满足\\(M_\\theta (T_0)\\le M_\\theta(T),\\forall\\theta\\in\\Theta\\)，即\\(T_0\\)为一致最小均方误差估计量。那么，对任意\\(\\theta_0\\in\\Theta\\)，取\\(T=g(\\theta_0)\\)，则\\(M_{\\theta_0} (T_0)\\le M_{\\theta_0}(T)=0\\), 从而有\\(P(T_0=g(\\theta_0))=1\\). 由于\\(\\theta_0\\)的任意性以及\\(g(\\theta)\\)不恒为一个常数，这样的\\(T_0\\)是不存在的。这也就是为什么我们在无偏估计量中考虑最优性的原因。 定理 2.5 (Cramer-Rao不等式) 设总体\\(X\\)的密度为\\(f(x;\\theta)\\), 参数\\(\\theta\\in (a,b)\\). \\(X_1,\\dots,X_n\\)为\\(X\\)的样本，\\(\\psi(X_1,\\dots,X_n)\\)是\\(g(\\theta)\\)的一个无偏估计，且满足下列正则性条件： \\(X\\)的支撑与\\(\\theta\\)无关； \\(g&#39;(\\theta)\\)和\\(\\frac{df(x;\\theta)}{d\\theta}\\)都存在且对一切\\(\\theta\\)有 \\[\\begin{align*} \\int_{-\\infty}^\\infty \\frac{df(x;\\theta)}{d\\theta} d x &amp;= 0,\\\\ \\int_{-\\infty}^\\infty\\frac d{d\\theta} L(\\vec x;\\theta) d \\vec x&amp;=0,\\\\ \\frac d{d\\theta}\\int_{-\\infty}^\\infty \\psi(\\vec x) L(\\vec x;\\theta) d \\vec x&amp;=\\int_{-\\infty}^\\infty \\psi(\\vec x) \\frac d{d\\theta}L(\\vec x;\\theta) d \\vec x, \\end{align*}\\] \\(I(\\theta):=E[(\\frac {d\\ln f(X;\\theta)}{d\\theta})^2]&gt;0\\), 则有 \\[Var_\\theta[\\psi(X_1,\\dots,X_n)]\\ge \\frac{[g&#39;(\\theta)]^2}{nI(\\theta)}.\\] 证明. \\[\\begin{align*} g&#39;(\\theta) &amp;= dE[\\psi(X_1,\\dots,X_n)]/d\\theta \\\\ &amp;=\\frac{d}{d\\theta}\\int \\psi(\\vec x)L(\\vec x;\\theta)d \\vec x\\\\ &amp;=\\int \\psi(\\vec x)\\frac{d}{d\\theta}L(\\vec x;\\theta)d \\vec x\\\\ &amp;=\\int [\\psi(\\vec x)-g(\\theta)]\\frac{d}{d\\theta}L(\\vec x;\\theta)d \\vec x\\\\ &amp;=\\int [\\psi(\\vec x)-g(\\theta)]\\frac{d \\ln L(\\vec x;\\theta)}{d\\theta} L(\\vec x;\\theta)d \\vec x\\\\ &amp;=E\\left[[\\psi(\\vec X)-g(\\theta)]\\frac{d \\ln L(\\vec X;\\theta)}{d\\theta}\\right]. \\end{align*}\\] 由Cauchy–Schwarz不等式得， \\[\\begin{align*} [g&#39;(\\theta)]^2 &amp;\\le E[[\\psi(\\vec X)-g(\\theta)]^2] E[(\\frac{d}{d\\theta}\\ln L(\\vec X;\\theta))^2]\\\\ &amp;=Var[\\psi(\\vec X)]E[(\\sum_{i=1}^n \\frac{d\\ln f(X_i;\\theta)}{d\\theta})^2]\\\\ &amp;=Var[\\psi(\\vec X)] n I(\\theta). \\end{align*}\\] 其中用到 \\[\\begin{align*} E\\left[\\frac{d\\ln f(X_i;\\theta)}{d\\theta}\\right]&amp;=\\int \\frac{\\frac{df(x;\\theta)}{d\\theta}}{f(x;\\theta)}f(x;\\theta)dx\\\\&amp;=\\int \\frac{df(x;\\theta)}{d\\theta}dx=0. \\end{align*}\\] C-R不等式给出无偏估计量方差的下界，如果某个无偏估计量达到这个下界且定理2.5中的条件对所有的无偏估计成立，则可以说明是该无偏估计量是一致最小方差无偏的。然而，有些情况下，C-R不等式的下界不一定达到，见陈家鼎等编著的教材例2.9, p30. \\(I(\\theta)\\)叫做Fisher信息量。离散情形有类似的结论。 有时候定理2.5中的条件并不满足，但C-R不等式的下界还是可以用来刻画模型参数的“可估能力”。该下界越小越容易得到精度更高的估计。 例 2.20 设\\(X\\sim N(\\mu,\\sigma^2)\\), 其中\\(\\mu\\)未知，\\(\\sigma\\)已知。 Fisher信息量为 \\[I(\\mu) = E\\left[(\\frac {d\\ln f(X;\\mu)}{d\\mu})^2\\right]=\\frac 1{\\sigma^4}E[(X-\\mu)^2]=\\frac 1{\\sigma^2}.\\] \\[Var[\\bar X] = \\frac{\\sigma^2}{n}=\\frac{1}{nI(\\mu)}\\] 样本均值\\(\\bar X\\)的方差达到了C-R不等式的下界。 试证明：若\\(\\mu\\)已知，则\\(\\sigma^2\\)的估计量\\(\\frac 1n\\sum_{i=1}^n(X_i-\\mu)^2\\)的方差达到了C-R不等式的下界。 2.2.4 统计量的大样本性质 统计量的相合性(consistency) （弱）相合估计：称\\(T_n(X_1,\\dots,X_n)\\)是\\(g(\\theta)\\)的相合估计，如果对任何 \\(\\epsilon&gt;0\\), 有\\[\\lim_{n\\to\\infty}P(|T_n-g(\\theta)|\\ge \\epsilon)=0.\\] 也称\\(T_n\\)依概率收敛到\\(g(\\theta)\\)，记为\\(T_n\\stackrel p\\to g(\\theta)\\). 强相合估计：称\\(T_n(X_1,\\dots,X_n)\\)是\\(g(\\theta)\\)的强相合估计，如果 \\[P(\\lim_{n\\to\\infty}T_n=g(\\theta))=1.\\] 也称\\(T_n\\)以概率1收到\\(g(\\theta)\\)，记为敛\\(T_n\\stackrel {w.p.1}\\to g(\\theta)\\). 说明 由强大数定理知，矩估计一般是强估计的 最大似然估计在十分广泛的条件下也是有相合性（见下一节） 引理 2.2 如果\\(T_n\\stackrel p\\to a\\)，且函数\\(f(x)\\)在\\(x=a\\)处连续，则\\(f(T_n)\\stackrel p\\to f(a)\\). 证明. 因为\\(f(x)\\)在\\(x=a\\)处连续，所有对任意\\(\\epsilon&gt;0\\)存在\\(\\delta&gt;0\\)使得任意\\(x\\)满足\\(|x-a|&lt;\\delta\\)，均有\\(|f(x)-f(a)|&lt;\\epsilon\\). 注意到\\(\\{|f(T_n)-f(a)|\\ge \\epsilon\\}\\subset \\{|T_n-a|\\ge \\delta\\}\\). 所以 \\[0\\le \\lim_{n\\to \\infty} P(|f(T_n)-f(a)|\\ge \\epsilon)\\le \\lim_{n\\to \\infty} P(|T_n-a|\\ge \\delta)=0.\\] 这表明\\(f(T_n)\\stackrel p\\to f(a)\\). 该引理容易推广到多元连续的情形。 引理 2.3 如果\\(T_n^{(i)}\\stackrel p\\to a_i\\), \\(i=1,\\dots,k\\)，且\\(f(x_1,\\dots,x_k)\\)在\\((a_1,\\dots,a_k)\\)点连续，则\\(f(T_n^{(1)},\\dots,T_n^{(k)})\\stackrel p\\to f(a_1,\\dots,a_k)\\). 例 2.21 设总体\\(X\\)的期望\\(\\mu\\)方差为\\(\\sigma^2\\), \\(X_1,\\dots,X_n\\)为其样本，证明 样本均值\\(\\bar X\\)是\\(\\mu\\)的相合估计量； 样本\\(k\\)阶原点矩\\(M_k\\)是总体\\(k\\)阶原点矩\\(E[X^k]\\)的相合估计量； 样本方差\\(S_n^2\\)和修正样本方差\\(S_n^{2*}\\)都是\\(\\sigma^2\\)的相合估计量。 证明. 由辛钦大数定律知，\\(\\bar X\\stackrel p\\to \\mu\\), \\(M_k\\stackrel p\\to E[X^k]\\). 由引理2.3得 \\[S_n^2 = \\frac{1}{n}\\sum_{i=1}^nX_i^2-\\bar X^2\\stackrel p\\to E[X^2]-E[X]^2=\\sigma^2.\\] 同理，\\(S_n^{2*}=\\frac{n-1}{n}S_n^2\\stackrel p\\to \\sigma^2\\). 下面给出判断相合估计的一个常用的充分条件。 定理 2.6 设\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量。如果 \\[\\lim_{n\\to \\infty}E[T(X_1,\\dots,X_n)] = g(\\theta),\\ \\lim_{n\\to \\infty}Var[T(X_1,\\dots,X_n)] =0,\\] 则\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的相合估计量。 证明. 令\\(T_n=T(X_1,\\dots,X_n)\\). 注意到 \\[\\{|T_n-g(\\theta)|\\ge \\epsilon\\}\\subset \\{|T_n-E[T_n]|\\ge \\epsilon/2\\}\\cup \\{|E[T_n]-g(\\theta)|\\ge \\epsilon/2\\}.\\] 对任意\\(\\epsilon&gt;0\\), 存在\\(N\\), 当\\(n\\ge N\\)时，\\(|E[T_n]-g(\\theta)|&lt; \\epsilon/2\\). 此时 \\[\\{|T_n-g(\\theta)|\\ge \\epsilon \\}\\subset \\{|T_n-E[T_n]|\\ge \\epsilon/2\\}.\\] 所以， \\[P(|T_n-g(\\theta)|\\ge \\epsilon)\\le P(|T_n-E[T_n]|\\ge \\epsilon/2)\\le \\frac{4 Var[T_n]}{\\epsilon^2}\\to 0.\\] 此外还可以用Markov不等式（如果\\(X\\)为非负随机变量且\\(a&gt;0\\)，则\\(P(X\\ge a)\\le E[X]/a\\)）证明。 所以， \\[\\begin{align*} P(|T_n-g(\\theta)|\\ge \\epsilon)&amp;=P((T_n-g(\\theta))^2\\ge \\epsilon^2)\\le \\frac{E[(T_n-g(\\theta))^2]}{\\epsilon^2}\\\\ &amp;=\\frac{Var[T_n]+(E[T_n]-g(\\theta))^2}{\\epsilon^2}\\to 0. \\end{align*}\\] 统计量的渐近正态性(asymptotic normality) 定义 2.8 设\\(T(X_1,\\dots,X_n)\\)为\\(\\theta\\)的估计量。如果存在一个趋于零的正数列\\(\\sigma_n(\\theta)\\), 使得\\((T-\\theta)/\\sigma_n(\\theta)\\)的分布收敛到标准正态分布，则称\\(T(X_1,\\dots,X_n)\\)为\\(\\theta\\)的渐近正态估计，或称\\(T\\)具备渐近正态性，记为 \\[T\\stackrel{\\cdot}{\\sim} N(\\theta, \\sigma_n(\\theta)^2).\\] 渐近正态性在构建参数的渐近置信区间中扮演着非常重要的角色。下面定理给出最大似然估计的渐近正态性。 定理 2.7 设\\(X\\)的密度为\\(f(x;\\theta)\\), 其参数空间\\(\\Theta\\)是非退化区间，且满足下列正则性条件： 对一切\\(\\theta\\in\\Theta\\), \\(\\frac{\\partial \\ln f}{\\partial\\theta}, \\frac{\\partial^2 \\ln f}{\\partial\\theta^2}, \\frac{\\partial^3 \\ln f}{\\partial\\theta^3}\\) 都存在 对一切\\(\\theta\\in\\Theta\\), 有\\(|\\frac{\\partial \\ln f}{\\partial\\theta}|&lt;F_1(x),\\ |\\frac{\\partial^2 \\ln f}{\\partial\\theta^2}|&lt;F_2(x),\\ |\\frac{\\partial^3 \\ln f}{\\partial\\theta^3}|&lt;H(x),\\) 其中\\(F_1(x),F_2(x)\\)在实数轴上可积，且\\(\\int_{-\\infty}^\\infty H(x)f(x;\\theta)dx&lt;M\\), \\(M\\)与\\(\\theta\\)无关。 对一切\\(\\theta\\in\\Theta\\), 有\\(0&lt;I(\\theta)=E[(\\frac{\\partial \\ln f}{\\partial\\theta})^2]&lt;\\infty\\). 则在参数真值\\(\\theta\\)为\\(\\Theta\\)内点的情况下，其似然方程有一个解\\(\\hat{\\theta}_L\\)存在，且 \\[\\hat{\\theta}_L\\stackrel{p}{\\to}\\theta,\\ \\hat{\\theta}_L\\stackrel{\\cdot}{\\sim} N(\\theta,[nI(\\theta)]^{-1}).\\] 值得注意的是，最大似然估计的渐近方差为C-R不等式的下界，从这个角度可以说明最大似然估计具有良好性质。证明参考：陈希孺. 概率论与数理统计. 中国科技大学出版社, 1992 2.3 区间估计 2.3.1 区间估计的定义 定义 2.9 设总体\\(X\\sim F(x;\\theta),\\ \\theta\\in \\Theta\\). 如果统计量\\(T_1(X_1,\\dots,X_n)\\), \\(T_2(X_1,\\dots,X_n)\\)使得对给定的\\(\\alpha\\in(0,1)\\)有 \\[P(T_1\\le g(\\theta)\\le T_2)=1-\\alpha,\\ \\forall \\theta\\in\\Theta,\\] 则称随机区间\\([T_1,T_2]\\)为参数\\(g(\\theta)\\)的置信度（置信概率）为\\(1-\\alpha\\)的置信区间(Confidence Interval)，\\(T_1,T_2\\)分别称为置信下界和置信上界。 说明: 在一些情况下，定义中的“等式”无解，此时考虑的置信区间\\([T_1,T_2]\\)应满足 \\[P(T_1\\le g(\\theta)\\le T_2)\\ge 1-\\alpha,\\ \\forall \\theta\\in\\Theta.\\] 这里允许\\(T_1=-\\infty\\)或者\\(T_2=\\infty\\)，这两种情况为单侧置信区间，否则称为双侧置信区间。 图 2.4: 置信区间示意图 2.3.2 枢轴量法 目标：找到\\(g(\\theta)\\)的区间估计，置信度为\\(1-\\alpha\\). Step 1: 选择恰当的枢轴量(Pivot quantity)\\(G(X_1,\\dots,X_n;g(\\theta))\\)，其满足以下性质 \\(G\\)不含有其他未知参数 \\(G\\)的分布确定，即不含未知参数\\(\\theta\\) 一般地，\\(G\\)是关于参数\\(g(\\theta)\\)的单调函数 Step 2: 求\\(a,b\\)使得\\(P(a\\le G\\le b)=1-\\alpha\\) Step 3: 转化不等式\\(a\\le G\\le b\\)为如下形式： \\[ T_1 \\le g(\\theta) \\le T_2. \\] 例 2.22 若总体为指数分布\\(Exp(\\lambda)\\)，求未知参数\\(\\lambda\\)的置信区间。 Step 1: 选择枢轴量 \\[G(X_1,\\dots,X_n;\\lambda) = 2\\lambda n\\bar X\\sim Ga(n,1/2)=\\chi^2(2n)\\] Step 2: 求\\(a,b\\)使得\\(P(a\\le G\\le b)=1-\\alpha\\)，即 \\[P(a\\le 2\\lambda n\\bar X\\le b)=1-\\alpha\\] Step 3: \\(\\lambda\\)的置信区间为\\([a/(2n\\bar X),b/(2n\\bar X)]\\). 如何选择\\(a,b\\)? 平分法：\\(a=\\chi^2_{\\alpha/2}(2n), b=\\chi^2_{1-\\alpha/2}(2n)\\) 最优方案？参考书p35 图 2.5: 平分法示意图 以下通过R模拟来实现这个过程。 set.seed(0) # generate data from exponential distribution lambda &lt;- 2 n &lt;- 1000 X &lt;- rexp(n,lambda) # find out the confidence interval (CI) alpha &lt;- 0.05 a &lt;- qchisq(p=alpha/2,df=2*n) b &lt;- qchisq(p=1-alpha/2,df=2*n) CI &lt;- c(a/2/sum(X),b/2/sum(X)) cat((1-alpha)*100,&quot;% CI is [&quot;,CI[1],&quot;, &quot;, CI[2],&quot;]&quot;,sep=&quot;&quot;) ## 95% CI is [1.824, 2.065] # generate R batches of the data R &lt;- 100 CIs &lt;- matrix(0,R,3) for(i in 1:R){ X &lt;- rexp(n,lambda) CIs[i,] = c(a/2/sum(X),1/mean(X),b/2/sum(X)) } ## plot the CIs plot(0, xlim=c(0, R), ylim=c(min(CIs)-0.02, max(CIs)+0.02), type=&quot;n&quot;,xlab=&quot;Sample ID&quot;,ylab=&quot;&quot;) count &lt;- 0 for (i in 1:nrow(CIs)) { if (CIs[i, 1]&gt;lambda | CIs[i, 3]&lt;lambda){ color = &quot;red&quot; count = count +1 if(CIs[i, 1]&gt;lambda) text(i,CIs[i, 3]+0.02,count) else text(i,CIs[i, 1]-0.02,count) }else{ color = &quot;blue&quot; } lines(x=rep(i, 2), y=c(CIs[i, 1], CIs[i, 3])) points(x=i, y=CIs[i,2], pch=16, col=color) } abline(h=lambda,lty = 3,col=&quot;red&quot;) 2.3.3 单个正态总体的区间估计 设总体\\(X\\sim N(\\mu,\\sigma^2)\\), 如何找出未知参数\\(\\mu\\)和\\(\\sigma^2\\)的置信区间？ 已知\\(\\sigma^2\\), 找出\\(\\mu\\)的置信区间 未知\\(\\sigma^2\\), 找出\\(\\mu\\)的置信区间 已知\\(\\mu\\), 找出\\(\\sigma^2\\)的置信区间 未知\\(\\mu\\), 找出\\(\\sigma^2\\)的置信区间 已知方差，求期望的置信区间 由抽样定理知，\\(\\bar{X}\\sim N(\\mu,\\sigma^2/n)\\). 因此 \\(U = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\) \\[P\\left(a\\le \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\le b\\right) = 1-\\alpha\\] \\(\\mu\\)的置信度为\\(1-\\alpha\\)的置信区间为 \\[\\left[\\bar{X}-b\\frac{\\sigma}{\\sqrt{n}},\\ \\bar{X}-a\\frac{\\sigma}{\\sqrt{n}}\\right]\\] 最优的选择：\\(b=-a=u_{1-\\alpha/2}\\), 此时置信区间为： \\[\\left[\\bar{X}-u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\ \\bar{X}+u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]=\\bar{X}\\pm u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\] 方差未知，求期望的置信区间 由抽样定理知， \\[T = \\frac{\\bar{X}-\\mu}{S_n/\\sqrt{n-1}}= \\frac{\\bar{X}-\\mu}{S_n^*/\\sqrt{n}}\\sim t(n-1)\\] \\[P\\left(a\\le \\frac{\\bar{X}-\\mu}{S_n^*/\\sqrt{n}}\\le b\\right) = 1-\\alpha\\] \\(\\mu\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[\\bar{X}-b\\frac{S_n^*}{\\sqrt{n}},\\ \\bar{X}-a\\frac{S_n^*}{\\sqrt{n}}\\right]=\\left[\\bar{X}-b\\frac{S_n}{\\sqrt{n-1}},\\ \\bar{X}-a\\frac{S_n}{\\sqrt{n-1}}\\right]\\] 最优的选择：\\(b=-a=t_{1-\\alpha/2}(n-1)\\), 此时置信区间为： \\[\\left[\\bar{X}-t_{1-\\alpha/2}(n-1)\\frac{S_n^*}{\\sqrt{n}},\\ \\bar{X}+t_{1-\\alpha/2}(n-1)\\frac{S_n^*}{\\sqrt{n}}\\right].\\] 也可以表示为\\(\\bar{X}\\pm t_{1-\\alpha/2}(n-1)S_n^*/\\sqrt{n}\\). 例 2.23 假设OPPO手机充电五分钟通话时间\\(X\\sim N(\\mu,\\sigma^2)\\). 随机抽取6部手机测试通话时间（单位：小时）为 \\[1.6,\\ 2.1,\\ 1.9,\\ 1.8,\\ 2.2,\\ 2.1,\\] 已知\\(\\sigma^2=0.06\\), 求\\(\\mu\\)的置信度为\\(95\\%\\)的置信区间。 \\(\\sigma^2\\)未知, 求\\(\\mu\\)的置信度为\\(95\\%\\)的置信区间。 解. 查表知，\\(u_{1-\\alpha/2}=u_{0.975}=1.96,\\ t_{1-\\alpha/2}=t_{0.975}=2.5706\\). 且\\(\\bar X = 1.95,\\ S_n=0.206\\). \\(\\left[1.95-1.96\\frac{\\sqrt{0.06}}{\\sqrt{6}},\\ 1.95+1.96\\frac{\\sqrt{0.06}}{\\sqrt{6}}\\right]=[1.754,\\ 2.146]\\). \\(\\left[1.95-2.5706\\frac{0.206}{\\sqrt{6-1}},\\ 1.95+2.5706\\frac{0.206}{\\sqrt{6-1}}\\right]=[1.713,\\ 2.187]\\). 一些思考 分析这两种的结果会发现，由同一组样本观察值，按同样的置信概率，对\\(\\mu\\)计算出的置信区间因为\\(\\sigma\\)的是否已知会不一样。这因为：当\\(\\sigma\\)为已知时，我们掌握的信息多一些，在其他条件相同的情况下，对\\(\\mu\\)的估计精度要高一些，即表现为\\(\\mu\\)的置信区间长度要小些。反之，当\\(\\sigma\\)为未知时，对\\(\\mu\\)的估计精度要低一些，即表现为\\(\\mu\\)的置信区间长度在大一些。这是因为当\\(n\\)比较小时，\\(t_{1-\\alpha/2}(n-1)&gt;u_{1-\\alpha/2}\\). 还可以发现，当样本量\\(n\\)不断增大时，两种情况下的置信区间会慢慢接近。 也就意味着大样本信息可以弥补\\(\\sigma\\)的缺失带来的偏差（大数定律）。 已知期望，求方差的置信区间 选择枢轴量 \\[T =\\sum_{i=1}^n\\frac{(X_i-\\mu)^2}{\\sigma^2}\\sim \\chi^2(n)\\] \\[P\\left(\\chi^2_{\\alpha/2}(n)\\le\\sum_{i=1}^n\\frac{(X_i-\\mu)^2}{\\sigma^2}\\le \\chi^2_{1-\\alpha/2}(n)\\right) = 1-\\alpha\\] \\(\\sigma^2\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[\\frac{\\sum_{i=1}^n(X_i-\\mu)^2}{\\chi^2_{1-\\alpha/2}(n)},\\ \\frac{\\sum_{i=1}^n(X_i-\\mu)^2}{\\chi^2_{\\alpha/2}(n)}\\right]\\] 期望未知，求方差的置信区间 选择枢轴量 \\[T =\\frac{nS_n^2}{\\sigma^2}=\\sum_{i=1}^n\\frac{(X_i-\\bar X)^2}{\\sigma^2}\\sim \\chi^2(n-1)\\] \\[P\\left(\\chi^2_{\\alpha/2}(n-1)\\le\\sum_{i=1}^n\\frac{(X_i-\\bar X)^2}{\\sigma^2}\\le \\chi^2_{1-\\alpha/2}(n-1)\\right) = 1-\\alpha\\] \\(\\sigma^2\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[\\frac{\\sum_{i=1}^n(X_i-\\bar X)^2}{\\chi^2_{1-\\alpha/2}(n-1)},\\ \\frac{\\sum_{i=1}^n(X_i-\\bar X)^2}{\\chi^2_{\\alpha/2}(n-1)}\\right]=\\left[\\frac{nS_n^2}{\\chi^2_{1-\\alpha/2}(n-1)},\\ \\frac{nS_n^2}{\\chi^2_{\\alpha/2}(n-1)}\\right]\\] 2.3.4 两个独立正态总体的区间估计 设两个独立总体\\(X\\sim N(\\mu_1,\\sigma_1^2)\\), \\(Y\\sim N(\\mu_2,\\sigma^2)\\), 如何找出未知参数\\(\\mu\\)和\\(\\sigma^2\\)的置信区间？其中\\(X\\)的样本为\\(X_1,\\dots,X_m\\), 样本方差为\\(S_{1m}^2\\); \\(Y\\)的样本为\\(Y_1,\\dots,Y_n\\), 样本方差为\\(S_{2n}^2\\) 已知\\(\\sigma_1^2,\\sigma_2^2\\), 找出\\(\\mu_1-\\mu_2\\)的置信区间 以知\\(\\sigma_1^2=\\sigma_2^2=\\sigma^2\\), 找出\\(\\mu_1-\\mu_2\\)的置信区间 已知\\(\\mu_1,\\mu_2\\), 找出\\(\\sigma_1^2/\\sigma_2^2\\)的置信区间 未知\\(\\mu_1,\\mu_2\\), 找出\\(\\sigma_1^2/\\sigma_2^2\\)的置信区间 应用场景 比较男生、女生两个群体的身高/体重/成绩平均水平的差异 已知方差，求均值差的置信区间 选择枢轴量： \\[U=\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{\\sqrt{\\sigma_1^2/m+\\sigma_2^2/n}}\\sim N(0,1).\\] \\(\\mu_1-\\mu_2\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[(\\bar{X}-\\bar{Y})-u_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}m+\\frac{\\sigma_2^2}n},\\ (\\bar{X}-\\bar{Y})+u_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}m+\\frac{\\sigma_2^2}n}\\right]\\] 已知方差相同，求均值差的置信区间 选择枢轴量： \\[T=\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{S_w\\sqrt{1/m+1/n}}\\sim t(m+n-2).\\] 其中\\(S_w =\\sqrt{(mS_{1m}^2+nS_{2n}^2)/(m+n-2)}.\\) 令\\(t_{1-\\alpha/2}(m+n-2)=t_{1-\\alpha/2}\\)，\\(\\mu_1-\\mu_2\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[(\\bar{X}-\\bar{Y})-t_{1-\\alpha/2}S_w\\sqrt{\\frac 1m+\\frac 1n},\\ (\\bar{X}-\\bar{Y})+t_{1-\\alpha/2}S_w\\sqrt{\\frac 1m+\\frac 1n}\\right]\\] 例 2.24 假设OPPO手机充电五分钟通话时间\\(X\\sim N(\\mu_1,\\sigma_1^2)\\), VIVO手机充电五分钟通话时间\\(Y\\sim N(\\mu_2,\\sigma_2^2)\\). 随机抽取6部手机测试通话时间（单位：小时）为 \\[\\text{OPPO}:\\ 1.6,\\ 2.1,\\ 1.9,\\ 1.8,\\ 2.2,\\ 2.1\\] \\[\\text{VIVO}:\\ 1.8,\\ 2.2,\\ 1.5,\\ 1.4,\\ 2.0,\\ 1.7\\] 求\\(\\mu_1-\\mu_2\\)的置信度为\\(95\\%\\)的置信区间: 已知\\(\\sigma_1^2 = 0.06,\\ \\sigma_2^2 = 0.08\\). 已知\\(\\sigma_1^2 =\\sigma_2^2\\). 解. \\(m=n=6\\), \\(\\bar{X}=1.95,\\ \\bar{Y}=1.77\\), \\(S_{1m}^2=0.042, S_{2n}^2=0.064, S_w = 0.252.\\) 查表知，\\(u_{0.975}=1.96\\), \\(t_{0.975}(10)=2.23\\). 第一种情况为\\([-0.12,\\ 0.48]\\) 第二种情况为\\([-0.14,\\ 0.50]\\) 已知均值，求方差比的置信区间 \\[T_1 =\\sum_{i=1}^m\\frac{(X_i-\\mu_1)^2}{\\sigma_1^2}\\sim \\chi^2(m),\\ T_2 =\\sum_{i=1}^n\\frac{(Y_i-\\mu_2)^2}{\\sigma_2^2}\\sim \\chi^2(n)\\] \\[\\frac{T_1/m}{T_2/n}=\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2}\\frac{\\sigma_2^2}{\\sigma_1^2}\\sim F(m,n)\\] \\(\\sigma_1^2/\\sigma_2^2\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[\\frac{1}{F_{1-\\alpha/2}(m,n)}\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2},\\ \\frac{1}{F_{\\alpha/2}(m,n)}\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2} \\right]\\] 均值未知，求方差比的置信区间 \\[T_1=\\frac{(m-1)S_{1m}^{*2}}{\\sigma_1^2}=\\sum_{i=1}^m\\frac{(X_i-\\bar X)^2}{\\sigma_1^2}\\sim \\chi^2(m-1)\\] \\[T_2=\\frac{(n-1)S_{2n}^{*2}}{\\sigma_2^2}=\\sum_{i=1}^n\\frac{(Y_i-\\bar Y)^2}{\\sigma_2^2}\\sim \\chi^2(n-1)\\] \\[\\frac{T_1/(m-1)}{T_2/(n-1)}=\\frac{S_{1m}^{*2}}{S_{2n}^{*2}}\\frac{\\sigma_2^2}{\\sigma_1^2}\\sim F(m-1,n-1)\\] \\(\\sigma_1^2/\\sigma_2^2\\)的置信度为\\(1-\\alpha\\)的置信区间为： \\[\\left[\\frac{1}{F_{1-\\alpha/2}(m-1,n-1)}\\frac{S_{1m}^{*2}}{S_{2n}^{*2}},\\ \\frac{1}{F_{\\alpha/2}(m-1,n-1)}\\frac{S_{1m}^{*2}}{S_{2n}^{*2}} \\right]\\] 一些说明 枢轴量法的难点在于寻找枢轴量，没有统一的方法。正态总体下的应用应当熟练掌握。 另外一种求置信区间方法叫统计量方法，不作要求，感兴趣陈家鼎等编著的教材pp42-46. “最优的置信区间”是否存在？目前尚缺乏对置信区间的优良性讨论。 2.3.5 非正态总体参数的区间估计 令\\(\\mu=E[X],\\sigma^2=Var[X]\\)分别为总体\\(X\\)的期望和方差。 由中心极限定理， \\[\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}\\stackrel{\\cdot}\\sim N(0,1).\\] 当\\(\\sigma\\)已知时，总体期望\\(\\mu\\)的置信度为\\(1-\\alpha\\)的区间估计可以近似为 \\[\\left[\\bar X-u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar X+u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right].\\] 如果\\(\\sigma\\)未知，可以用样本标准差\\(S_n\\)（或者修正样本差\\(S_n^*\\)）替代\\(\\sigma\\)， \\[\\left[\\bar X-u_{1-\\alpha/2}\\frac{S_n}{\\sqrt{n}}, \\bar X+u_{1-\\alpha/2}\\frac{S_n}{\\sqrt{n}}\\right].\\] 2.4 分布的估计 本节考虑分布函数和密度函数的估计，目标是通过样本的观测值构造一种函数来近似这两种函数。本节所介绍的方法不需要知道总体的具体的分布类型，属于非参统计方法。 2.4.1 分布函数的估计 定义 2.10 设总体\\(X\\)的样本\\((X_1,\\dots,X_n)\\)的一次观测值\\((x_1,\\dots,x_n)\\), 并将它们由小到大排列\\(x_{(1)}\\le x_{(2)}\\le \\dots\\le x_{(n)}\\), 经验分布函数(或称样本分布函数)定义为 \\[ F_n(x) =\\frac{1}{n}\\sum_{i=1}^n 1\\{x_i\\le x\\} = \\begin{cases} 0,&amp;\\ x&lt;x_{(1)}\\\\ 1/n,&amp;\\ x_{(1)}\\le x&lt;x_{(2)}\\\\ 2/n,&amp;\\ x_{(2)}\\le x&lt;x_{(3)}\\\\ &amp;\\vdots\\\\ k/n,&amp;\\ x_{(k)}\\le x&lt;x_{(k+1)}\\\\ &amp;\\vdots\\\\ 1,&amp;\\ x&gt;x_{(n)}\\\\ \\end{cases}. \\] 经验分布函数示意图 经验分布函数的性质 固定的\\(x\\)和\\(n\\)，\\(F_n(x)\\)表示事件\\(\\{X\\le x\\}\\)的频率，由强大数定律知， \\[F_n(x)\\to P(X\\le x)=F(x),\\] 即 \\[P\\left(\\lim_{n\\to\\infty}F_n(x)=F(x)\\right)=1.\\] 格里汶科定理给出更强的结果（几乎处处一致收敛）: \\[P\\left(\\lim_{n\\to\\infty}\\sup_{x\\in \\mathbb{R}}|F_n(x)-F(x)|=0\\right)=1.\\] 注：由此可见，当\\(n\\)相当大时，经验分布函数\\(F_n(x)\\)是母体分布函数\\(F(x)\\)的一个良好近似。数理统计学中一切都以样本为依据，其理由就在于此。 2.4.2 直方图法 只考虑一维连续型总体\\(X\\sim f(x)\\)。设\\(X_1,\\dots,X_n\\)为样本，\\(R_n(a,b)\\)表示落在区间\\((a,b]\\)中的个数。由中值定理得，存在\\(x_0\\in(a,b]\\)使得 \\[f(x_0)=\\frac 1{b-a}\\int_a^b f(x)dx\\approx \\frac {R_n(a,b)}{n(b-a)}\\] 设\\(-\\infty&lt;t_0&lt;t_1&lt;\\dots&lt;t_m&lt;\\infty\\)，\\(t_{i+1}-t_i=h&gt;0\\). 直方图法的密度估计为： \\[ f_n(x)= \\begin{cases} \\frac{R_n(t_i,t_{i+1})}{nh},\\ x\\in(t_i,t_{i+1}],i=0,\\dots,m-1\\\\ 0, x\\le t_0,x&gt;t_m \\end{cases} \\] 实际上选取\\(t_0\\)为比\\(X_{(1)}\\)略小的数，选取\\(t_m\\)为比\\(X_{(n)}\\)略大的数。经验法则：\\(m\\approx 1+3.322\\log_{10} n.\\) 案例：身高数据 ## Warning: package &#39;dslabs&#39; was built under R version ## 3.5.3 直方图法的相合性 定理 2.8 设\\(f(\\cdot)\\)在点\\(x\\)连续且\\(\\lim_n h_n=0,\\lim_n nh_n=\\infty\\), 则对任何\\(\\epsilon&gt;0\\)有 \\[\\lim_{n\\to\\infty} P(|f_n(x)-f(x)|\\ge \\epsilon)=0.\\] 定理 2.9 设\\(f(\\cdot)\\)在\\(\\mathbb{R}\\)上一致连续，\\(\\int_{-\\infty}^\\infty |x|^\\delta d x&lt;\\infty\\)(对某个\\(\\delta&gt;0\\)), 且\\(\\lim_n h_n=0,h_n\\ge (\\ln n)^2/n\\), 则 \\[P(\\lim_{n\\to\\infty} \\sup_x|f_n(x)-f(x)|=0)=1.\\] 证明陈家鼎等编著的教材pp54-55. 2.4.3 核估计法 中心差分： \\[f(x)\\approx \\frac{F(x+h)-F(x-h)}{2h}\\approx \\frac{F_n(x+h)-F_n(x-h)}{2h}\\] \\[\\hat{f}_n(x) = \\frac{1}{2hn}\\sum_{i=1}^n 1\\{x-h&lt;X_i\\le x+h\\}=\\frac{1}{2hn}\\sum_{i=1}^n K_0\\left(\\frac{x-X_i}{h}\\right)\\] 其中 \\[K_0(x)= \\frac 12 1\\{-1\\le x&lt;1\\}\\] 核函数：\\(K(x)\\)是\\(\\mathbb{R}\\)上的非负函数且满足\\(\\int_{-\\infty}^\\infty K(x)=1\\). 核估计：\\(\\hat{f}_n(x) = \\frac{1}{2hn}\\sum_{i=1}^n K\\left(\\frac{x-X_i}{h}\\right)\\) 常用的核函数 均匀核函数： \\[K_0(x)= \\frac 12 1\\{-1\\le x\\le1\\}\\] \\[K_1(x)= 1\\{-1/2\\le x\\le1/2\\}\\] 正态核函数： \\[K_2(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\] 核估计的相合性 定理 2.10 设核函数\\(K(x)\\)满足条件 \\[\\int_{-\\infty}^\\infty (K(x))^2 dx&lt;\\infty,\\ \\lim_{|x|\\to \\infty} |x|K(x)=0,\\] 又密度函数\\(f\\)在点\\(x\\)处连续，且\\(h_n\\to 0\\), \\(nh_n\\to\\infty\\), 则对一切\\(\\epsilon&gt;0\\), 有 \\[\\lim_{n\\to\\infty} P(|\\hat{f}_n(x)-f(x)|\\ge \\epsilon) = 0.\\] 证明见pp56-58. 例 2.25 R软件包dslabs收集了男生和女生的身高数据（单位英寸），由此估计男女身高总体的密度函数。 一些说明 收敛速度的比较：在满足一些正则性的条件（如，\\(h_n\\to 0\\), \\(nh_n\\to\\infty\\)）下，可以证明 直方图法的均方误差为\\(O(n^{-2/3})\\) 核估计的均方误差为\\(O(n^{-4/5})\\) 核估计的带宽(bandwidth) \\(h_n\\)如何选取? 如果选择正态核函数，经验法则：\\(h_n\\approx 1.06S_nn^{-1/5}\\) 延伸阅读 https://en.wikipedia.org/wiki/Kernel_density_estimation Kernel smoothing techniques used in finance used in Approximate Bayesian Computation (ABC) 2.5 本章习题 习题 2.1 设\\(X\\)的分布密度函数为 \\[f(x)=\\frac{1}{2\\sigma} e^{-|x|/\\sigma}\\ (\\sigma&gt;0),\\] \\(X_1,\\dots,X_n\\)是\\(X\\)的样本，求\\(\\sigma\\)的最大似然估计。 习题 2.2 设\\(X_1,\\dots,X_n\\)是来自\\([\\theta,\\theta+1]\\)上均匀分布的样本，其中\\(\\theta\\in\\mathbb{R}\\), 证明\\(\\theta\\)的最大似然估计不止一个，并求出所有的最大似然估计。 习题 2.3 设随机变量\\(X\\)以均等机会按\\(N(0,1)\\)分布取值和按\\(N(\\mu,\\sigma^2)\\)分布取值，其中\\(\\mu\\in \\mathbb{R},\\sigma^2&gt;0\\). 这时\\(X\\)的分布密度函数为这两个分布的密度的平均，即 \\[f(x;\\mu,\\sigma^2) = \\frac 12\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}+\\frac 12\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x-\\mu)^2/(2\\sigma^2)},\\] 设\\(X_1,\\dots,X_n\\)为此混合分布的简单随机样本，证明\\(\\mu,\\sigma^2\\)不存在最大似然估计。能否通过矩法估计\\(\\mu,\\sigma^2\\)？ 习题 2.4 （附加题I，选做）考虑上题的模型。设\\(Y\\)为一随机变量，\\(Y=1\\)表示\\(X\\)来自\\(N(0,1)\\)分布，\\(Y=0\\)表示\\(X\\)来自\\(N(\\mu,\\sigma^2)\\)分布，即\\(Y\\sim b(1,0.5)\\). 假设我们可以观测\\(Y_i\\)的值，基于样本\\((X_i,Y_i),i=1,\\dots,n\\)，是否可以求出\\(\\mu,\\sigma^2\\)的最大似然估计？事实上，\\(Y_i\\)的值不可观测（通常称为潜变量），此时你有没有更好的办法估计\\(\\mu,\\sigma^2\\)？ 习题 2.5 （附加题II，选做）若考虑更一般的混合分布： \\[f(x;\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2)=\\frac{\\lambda}{\\sqrt{2\\pi}\\sigma_1}e^{-(x-\\mu_1)^2/(2\\sigma_1^2)}+\\frac{1-\\lambda}{\\sqrt{2\\pi}\\sigma_2}e^{-(x-\\mu_2)^2/(2\\sigma_2^2)}\\] 其中\\(\\lambda\\in[0,1],\\mu_1,\\mu_2\\in \\mathbb{R},\\sigma_1^2,\\sigma_2^2&gt;0\\), 你能求出未知参数\\(\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\)的矩估计吗？ 习题 2.6 设\\(X_1,\\dots,X_n\\)是来自分布密度为 \\[f(x;\\theta)=\\frac{\\Gamma(\\theta+1)}{\\Gamma(\\theta)\\Gamma(1)}x^{\\theta-1}1\\{0\\le x\\le 1\\}\\] 的总体的样本，其中\\(\\theta&gt;0\\), 试用矩法估计\\(\\theta\\). 习题 2.7 设\\(X_1,\\dots,X_n\\)是来自分布密度为 \\[f(x;c,\\theta)=\\frac{1}{2\\theta}1\\{c-\\theta\\le x\\le c+\\theta\\}\\] 的总体的样本，其中\\(\\theta&gt;0,c\\in\\mathbb{R}\\), 试用矩法估计\\(c,\\theta\\). 习题 2.8 设\\(X_1,\\dots,X_n\\)为来自参数为\\(\\lambda\\)的Poisson分布的样本. 在下列选项中选出用于估计参数\\(\\lambda\\)的无偏估计量。( ) A. \\(\\bar X\\) B. \\(S_n^{*2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar X)^2\\) C. \\(\\frac 1 {n-1}\\sum_{i=1}^{n-1}X_i\\) D. \\(S_n^2=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar X)^2\\) E. \\(\\frac{1}2 \\bar X + \\frac 12 S_n^{*2}\\) 习题 2.9 设\\(X_1,\\dots,X_n\\)为来自参数为\\(\\lambda\\)的Poisson分布的样本, 已知\\(\\bar X\\)是未知参数\\(\\lambda\\)的完全统计量。在下列选项中选出用于估计参数\\(\\lambda\\)的最有效的估计量。 ( ) A. \\(\\bar X\\) B. \\(S_n^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2\\) C. \\(\\frac 1 {n-1}\\sum_{i=1}^{n-1}X_i\\) D. \\(\\frac{1}2 \\bar X + \\frac 12 S_n^{*2}\\) 习题 2.10 设\\(X,\\dots,X_n\\)为来自参数为\\(\\lambda\\)的Poisson分布的样本，求\\(\\lambda^2\\)的无偏估计。已知\\(\\bar X\\)是参数\\(\\lambda\\)的完全统计量，能否找到\\(\\lambda^2\\)的最小方差无偏估计量？ 习题 2.11 设\\(X_1,\\dots,X_n\\)为\\(N(\\mu,\\sigma^2)\\)分布的样本，参数\\(\\mu,\\sigma^2\\)未知。证明样本方差\\(S_n^2\\)与修正样本方差\\(S_n^{*2}\\)均为\\(\\sigma^2\\)的弱相合估计量。 习题 2.12 设\\(X_1,\\dots,X_n\\)为总体\\(N(\\mu,\\sigma^2)\\), 其中\\(\\mu\\)已知，\\(\\sigma^2\\)未知。证明\\(\\sigma^2\\)的估计量 \\[T(X_1,\\dots,X_n)=\\frac 1n\\sum_{i=1}^n(X_i-\\mu)^2\\] 的方差达到C-R不等式的下界。 习题 2.13 Let \\(X_1,\\dots,X_n\\) be a simple random sample taken from the density \\[f(x;\\theta)=\\frac{2x}{\\theta^2},\\quad 0\\le x\\le \\theta.\\] Find an expression for \\(\\hat\\theta_L\\), the maximum likelihood estimator (MLE) for \\(\\theta\\). Find an expression for \\(\\hat\\theta_M\\), the method of moments estimator for \\(\\theta\\). For the two estimators \\(\\hat\\theta_L\\) and \\(\\hat\\theta_M\\), which one is more efficient in terms of mean squared error (MSE)? 习题 2.14 设\\(X_1,\\dots,X_n\\)是\\(U(0,\\theta)\\)的样本，求\\(\\theta\\)的置信水平为\\(1-\\alpha\\)的置信区间。设得到了\\(5\\)个样本值\\(0.08,0.28,0.53,0.91,0.89\\), 求\\(\\theta\\)的置信水平为\\(0.95\\)的置信区间。 习题 2.15 陈家鼎等编著教材P62第27题 习题 2.16 陈家鼎等编著教材P62第28题 习题 2.17 分析R软件的dslabs包中的身高数据heights, 利用R软件完成以下问题。 假设整个总体服从正态分布，求期望和方差的95%置信区间。 为了判断“正态总体”的假设的合理性，画图比较核估计密度与正态分布密度的差异？ 假设男生总体与女生总体均服从正态分布（方差相同）且独立，求这两个总体平均水平的差的95%置信区间。可否认为男生总体的平均身高大于女生总体的平均身高？你的理由是什么？ 为了考察第3问中“男女总体的方差相同”的假设是否合理，不妨求这两个总体的方差比的95%置信区间。并观察该置信区间是否包含1？ 若无法安装R的包“dslabs”，直接导入数据data.RData即可。 "],
["test.html", "第 3 章 假设检验 3.1 女士品茶 3.2 基本概念 3.3 UMP检验和似然比检验 3.4 单参数指数型分布族 3.5 广义似然比检验 3.6 置信区间与假设检验的联系 3.7 p值 3.8 多重检验 3.9 伯努利分布的检验 3.10 拟合优度检验 3.11 小结 3.12 本章习题", " 第 3 章 假设检验 3.1 女士品茶 R. A. Fisher 的名著《实验设计》讲了一个最简单的实验：女士品茶。这个故事非常有名，以至于Salsburg 的统计学通俗读物就以它命名：《女士品茶：统计学如何变革了科学和生活》。 先回顾一下这个故事。在英国的Rothamsted实验站，Fisher给一位名叫Muriel Bristol的女士倒了一杯茶，但是Bristol 表示，自己更喜欢先将牛奶倒入杯中，再倒入茶（也就是大街小巷常见的奶茶）。这位女士号称能够分辨先倒茶和先倒牛奶的区别。作为实验设计的鼻祖，Fisher 当然想用实验检验一下：这位女士的味觉是否有这么敏锐？检验如下命题是否可以接受： 假设H: 该女士无此种鉴别能力 他准备了10杯调好的奶茶（两种顺序的都有）给该女士鉴别，结果那位女士竟然能够正确地分辨出10杯奶茶中的每一杯的调制顺序。 如何做出你的判断？如果Bristol并没有任何分辨能力，仅凭运气，她也可能全部答对。不过这个事件的概率是\\(2^{-10}\\approx 9.77\\times10^{-4}\\). 这是一个小概率事件。所以，若是Bristol全部答对，那么“她无此种鉴别能力”这个假设就和数据不太相容，可以拒绝这个假设。假如该女士只猜对了8杯，又该如何判断？更一般地，若答对\\(k\\)杯, 如何根据\\(k\\)的值做出合理的判断？这时候我们的答案可能就模棱两可了，因此有必要针对这类问题建立严格的分析框架，并给出合理的决策规则。 更多的例子： 某产品的次品率是否不超过\\(3\\%\\)？ 男生群体平均身高是否大于女生群体平均身高？ 身高是否服从正态分布？ 抽烟与慢性支气管炎是否有关？ 从上述例子中不难发现，所关心的问题归纳为“是”与“否”的判断，并非给出一个具体的 数或者区间，这区别与参数估计问题。我们把这类问题称为假设检验问题(Hypothesis Tests)。 3.2 基本概念 对总体的某种规律提出一个假设，通过样本数据来推断，决定是否拒绝这一假设，这样的统计活动成为假设检验。本节考虑参数形式的假设检验，给出相关概念和假设检验的思想。 例 3.1 (Fisher’s iris flower data set) Fisher的鸢尾花数据集是个著名的数据集，是为了量化鸢尾花形态上的区别而收集得到的。它包含了三类鸢尾花——山鸢尾(Setosa)、杂色鸢尾(Versicolour)、 维吉尼亚鸢尾(Virginica)的花萼(sepal)、花瓣(petal)的长度与宽度。 下图展示了花萼长度的核密度估计以及部分数据, 完整数据点击这里 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 51 7.0 3.2 4.7 1.4 versicolor 52 6.4 3.2 4.5 1.5 versicolor 53 6.9 3.1 4.9 1.5 versicolor 101 6.3 3.3 6.0 2.5 virginica 102 5.8 2.7 5.1 1.9 virginica 103 7.1 3.0 5.9 2.1 virginica 考虑以下几个问题： 假设有个植物学家跟你说，通过基因组分析表明，山鸢尾(Setosa)花萼长度的均值是4.5cm，他这个论断是否可信？ 另一位植物学家说山鸢尾(Setosa)花萼长度的均值是一个不小于4.5cm的数，但具体是多少就不清楚了，那么他这个论断是否又可信？ 山鸢尾(Setosa)和杂色鸢尾(Versicolour)两种花的花萼长度的均值有没有显著差异？ 不妨设\\(X,Y\\)分别为山鸢尾(Setosa)和杂色鸢尾(Versicolour)花萼长度的两个总体，\\(\\mu_1,\\mu_2\\)分别为它们的均值。问题1要检验\\(\\mu_1=4.5\\)是否成立，问题2检验\\(\\mu_1\\ge 4.5\\)是否成立，而问题3则检验\\(\\mu_1=\\mu_2\\)是否成立。假设检验需要明确所讨论命题及其对立的命题。比如，对于问题1，我们关心\\(\\mu_1=4.5\\)以及它的对立面\\(\\mu_1\\neq 4.5\\)哪个成立，可表示为 \\[\\begin{equation} H_0:\\mu_1=4.5\\ vs.\\ H_1:\\mu_1\\neq 4.5,\\tag{3.1} \\end{equation}\\] 其中\\(H_0\\)称为原假设/零假设(Null Hypothesis)，\\(H_1\\)称为备选假设/对立假设/备择假设(Alternative Hypothesis)。检验的目标是通过样本数据判断\\(H_0\\)成立还是\\(H_1\\)成立，最终的结论只能有一个。 更一般地，设总体来自某一参数分布族\\(\\{F(x,\\theta),\\theta\\in\\Theta\\}\\), 其中\\(\\Theta\\)为参数空间，包含所有可能的参数。假设检验问题的基本形式为 \\[\\begin{equation} H_0:\\theta \\in \\Theta_0\\ vs. \\ H_1:\\theta \\in \\Theta_1,\\tag{3.2} \\end{equation}\\] 其中\\(\\varnothing\\neq \\Theta_0,\\Theta_1\\subset \\Theta,\\Theta_0\\cap \\Theta_1=\\varnothing\\). 最常见的情况\\(\\Theta_1=\\Theta-\\Theta_0\\). 简单原假设(simple null)：\\(\\Theta_0\\)只包含一个点，如\\(H_0:\\theta=\\theta_0\\) 复杂原假设(composite null)：\\(\\Theta_0\\)只包含多个点，如\\(H_0:\\theta\\le \\theta_0\\) 同样地，对于备选假设也有简单和复杂两种分类。如果原假设和备选假设都是简单的，那么称该检验是简单假设检验(simple hypothesis test)。 备选假设通常有三种形式： 双边(two-sided)：\\(H_1:\\theta\\neq \\theta_0\\) 单边(one-sided)：\\(H_1:\\theta&gt; \\theta_0\\) 单边(one-sided)：\\(H_1:\\theta&lt; \\theta_0\\) 假设检验基于样本数据作出接受或者拒绝\\(H_0\\)的判断。这相当于把样本空间划分成两个互不相交的部分\\(W\\)和\\(\\bar{W}\\), 当样本属于\\(W\\)时就拒绝\\(H_0\\); 否则接受\\(H_0\\). 我们称\\(W\\)为该检验的拒绝域(rejection region)，而\\(\\bar W\\)为接受域(acceptance region). 由于两者是互为补集的关系，下面只关注于拒绝域。一个拒绝域决定一种检验法则。 为了解决假设检验问题(3.1)，不妨假设山鸢尾花萼长度\\(X\\sim N(\\mu,0.12)\\)，其中\\(\\mu\\)未知。 由于\\(\\mu\\)未知，我们可以用\\(\\bar X\\)作为\\(\\mu\\)的一种估计，当\\(\\bar X\\)偏离4.5比较远时，我们就有理由拒绝\\(H_0:\\mu=4.5\\)的假设，故一种可行的拒绝域为 \\[W=\\{x_{1{:}n}:|\\bar x-4.5|&gt;c\\},\\] 其中\\(c\\)为待定的常数。R中的iris数据集给出了50个山鸢尾花萼长度的数据，样本均值\\(\\bar x=5.006\\) cm。所以，\\(|\\bar x-4.5|=0.506\\)，如果\\(0.506&gt;c\\)则拒绝原假设，否则接受原假设。我们后面将讨论如何选择合适的常数\\(c\\)。 由于样本的随机性，检验不可能\\(100\\%\\)正确，有可能出现错误。注意到客观事实只有两种可能的结果，要么原假设成立，要么备择假设成立；而检验的结果也只有两种可能，接受或者拒绝原假设。因此有下列四种情况：其中两种检验结果符合实际，另外两种则与实际相悖，对应所谓的两类错误。 接受原假设 拒绝原假设 原假设为真 正确 第一类（拒真, Type I）错误 备择假设为真 第二类（纳伪, Type II）错误 正确 假设检验的核心问题是如何控制犯两类错误的概率。值得注意的是，这两个概率为条件概率。犯第一类错误的概率常用\\(\\alpha\\)表示，其定义为： \\[\\alpha = P(X_{1{:}n}\\in W|H_0).\\] 犯第二类错误的概率常用\\(\\beta\\)表示，其定义为： \\[\\beta = P(X_{1{:}n}\\notin W|H_1).\\] 对于参数假设检验(3.2)，则两类错误发生的概率分别为： \\[\\begin{align*} \\alpha &amp;= P_\\theta(X_{1{:}n}\\in W)=\\rho_W(\\theta),\\ \\theta\\in\\Theta_0,\\\\ \\beta &amp;= P_\\theta(X_{1{:}n}\\notin W)=1-\\rho_W(\\theta),\\ \\theta\\in\\Theta_1, \\end{align*}\\] 其中\\(\\rho_W(\\theta)=P_\\theta(X_{1{:}n}\\in W)\\)表示在\\(X_i\\stackrel{iid}\\sim F_\\theta\\)下事件\\(\\{X_{1{:}n}\\in W\\}\\)的概率。我们称\\(\\rho_W(\\theta)\\)为功效函数，其中\\(\\theta\\in\\Theta\\)。如果原假设是简单的\\(H_0:\\theta=\\theta_0\\)，这时第一类错误发生的概率为\\(\\alpha=P_{\\theta_0}(X_{1{:}n}\\in W)\\), 只有一种可能；如果原假设是复杂的，则\\(\\alpha\\)的取值有很多种可能，取决于真实的参数\\(\\theta\\)的取值，不管怎样，\\(\\alpha=\\rho_W(\\theta)\\)，其中\\(\\theta\\in\\Theta_0\\). 当真实的\\(\\theta\\in\\Theta_1\\)时，我们称\\(\\rho_W(\\theta)\\)为检验的功效，不难发现\\(\\beta+\\rho_W(\\theta)=1\\). 然而，在大多数情况下，这两类错误的概率相背而驰。在样本量不变的情况下，如果其中一个变小则另一个变大。见下面例子。 例 3.2 设山鸢尾花萼长度\\(X\\sim N(\\mu,0.12)\\)，其中\\(\\mu\\)未知。现有样本量\\(n=50\\)。为解决假设检验问题(3.1)，我们选择拒绝域\\(W=\\{x_{1{:}n}:|\\bar x-4.5|&gt;c\\}\\). 该检验的功效函数为： \\[\\begin{align*} \\rho_W(\\mu)&amp;=P_{\\mu}(|\\bar{X}-4.5|&gt; c)\\\\ &amp;=1-\\Phi\\left(\\sqrt{50/0.12}(4.5-\\mu+c))+\\Phi(\\sqrt{50/0.12}(4.5-\\mu-c)\\right). \\end{align*}\\] 犯第一类错误的概率为: \\(\\alpha =\\rho_W(4.5)\\). 犯第二类错误的概率为: \\(\\beta= 1-\\rho_W(\\mu)\\)，其中\\(\\mu\\neq 4.5\\). 由上图可以发现：\\(c\\)增大，第一类错误发生的概率\\(\\alpha\\)变小，检验的功效变小，第二类错误发生的概率\\(\\beta\\)变大。一般情况下，在样本量不变的前提下，两类错误不能同时减小。 考虑到两类错误不能够同时被控制，在统计学中， 拒绝域的选取准则为：在保证犯第一类错误的概率不超过一定水平的前提下，选择犯第二类错误的概率尽可能小（等价地，检验的功率尽可能大）的拒绝域\\(W\\). 前面已提到，如果原假设是复合的，那么犯第一类错误的概率是不确定的，为了控制犯第一类错误的概率，我们对其最坏情况进行控制，使其在水平\\(\\alpha\\in(0,1)\\)下，即 \\[\\begin{equation} \\sup_{\\theta\\in\\Theta_0} \\rho_W(\\theta)= \\alpha.\\tag{3.3} \\end{equation}\\] 该水平\\(\\alpha\\)称为拒绝域\\(W\\)的检验水平/显著性水平(level of significance)。一般情况下，\\(\\alpha\\)为检验之前确定的比较小的数，如\\(0.1,0.05,0.01\\)。 如果给定\\(\\alpha\\)，不存在一个\\(W\\)使得式(3.3)成立，则将式(3.3)中的“\\(=\\)”替换成“\\(\\le\\)”，这同样保证犯第一类错误控制在\\(\\alpha\\)下。 小概率原理：小概率事件在一次试验中是几乎不发生的。若\\(H_0\\)为真，样本落在拒绝域\\(W\\)是小概率事件，不应发生。如发生，则拒绝原假设。 如何选择显著性水平? 人们自然会产生这样的问题：概率小到什么程度才当作“小概率事件”呢？这要据实际情况而定，例如即使下雨的概率为10%，仍有人会因为它太小而不带雨具。但某航空公司的事故率为1%，人们就会因为它太大而不敢乘坐该公司的飞机，通常把概率不超过0.05 (或0.01)的事件当作“小概率事件”。为此在假设检验时，必须先确定小概率即显著性的值\\(\\alpha\\) (即不超过\\(\\alpha\\)的概率认为是小概率)。 然而，正如前面提到，只是控制犯第一类错误控制在水平\\(\\alpha\\)下是远远不够的，而且满足这个条件的检验数不胜数。因为第二类错误不能忽略。因此，如果在保证\\(\\sup_{\\theta\\in\\Theta_0} \\rho_W(\\theta)= \\alpha\\)成立的前提下，使得犯第二类错误的概率最小，或者等价地，检验的功效最大化。一般情况下，这个问题不容易解决，不一定存在所谓的“最好”检验。为此下节将引入一致最大功效的概念。 3.3 UMP检验和似然比检验 3.3.1 UMP检验的定义 定义 3.1 称\\(W\\)为检验水平\\(\\alpha\\)的一致最大功效(uniformly most powerful, UMP)的拒绝域，若\\(W\\)的水平为\\(\\alpha\\)且对一切水平不超过\\(\\alpha\\)的拒绝域\\(W&#39;\\)均有 \\[\\rho_W(\\theta)\\ge \\rho_{W&#39;}(\\theta),\\ \\forall \\theta\\in \\Theta_1.\\] 定义 3.2 称\\(W\\)为检验水平\\(\\alpha\\)的无偏拒绝域，若\\(\\forall \\theta\\in \\Theta_1\\), 有\\(\\rho_W(\\theta)\\ge \\alpha\\). 定义 3.3 称\\(W\\)为检验水平\\(\\alpha\\)的一致最大功效无偏(uniformly most powerful unbiased, UMPU)的拒绝域，若\\(W\\)是水平为\\(\\alpha\\)的无偏拒绝域且对一切水平不超过\\(\\alpha\\)的无偏拒绝域\\(W&#39;\\)均有 \\[\\rho_W(\\theta)\\ge \\rho_{W&#39;}(\\theta),\\ \\forall \\theta\\in \\Theta_1.\\] 注：UMP意味着在犯第一类错误的概率不超过\\(\\alpha\\)的前提下，犯第二类错误的概率最小。无偏性是指如果备选假设成立，拒绝原假设的概率不小于显著性水平\\(\\alpha\\). 注意到此时检验的结果正确，该概率不应小于犯第一类错误的概率。下节将给出如何构造UMP的检验方法。 3.3.2 似然比检验方法 例 3.3 假设有两枚硬币，第一枚硬币正面朝上的概率为0.5，第二枚正面朝上的概率为0.7。现在我选择其中一枚硬币来抛十次，并告诉你多少次正面朝上，但不告诉你选择的是哪枚硬币。你的任务是根据正面朝上的次数判断我抛的是哪枚硬币。 你的决策是怎样？ 令\\(X\\)为正面朝上的次数，\\(\\theta\\)为硬币正面朝上的概率，则\\(X\\sim B(10,\\theta)\\), \\(\\theta\\in\\{0.5,0.7\\}\\)。假设检验为： \\[H_0: \\theta=0.5\\ vs.\\ H_1:\\theta=0.7.\\] 不难计算，两种情况下\\(X\\)的分布列为： \\(x\\) 0 1 2 3 4 5 6 7 8 9 10 \\(H_0\\) .0010 .0098 .0439 .1172 .2051 .2461 .2051 .1172 .0439 .0098 .0010 \\(H_1\\) .0000 .0001 .0014 .0090 .0368 .1029 .2001 .2668 .2335 .1211 .0282 假如观测到两次正面朝上，则有 \\[P(X=2|\\theta=0.5)/P(X=2|\\theta=0.7)\\approx 30.\\] 该比值称为似然比(likelihood ratio , LR)，表明相比第二枚硬币，第一枚硬币有大概30倍的可能性产生该结果。这个结果明显支持选择第一枚硬币。反之，如果观测到八次正面朝上，似然比 \\[P(X=8|\\theta=0.5)/P(X=8|\\theta=0.7)\\approx 0.19,\\] 这个结果支持第二枚硬币。似然比在接下来的假设检验中扮演重要的角色。 定义 3.4 设\\(L(x_{1{:}n};\\theta)\\)为似然函数，\\(\\theta_1,\\theta_2\\in\\Theta\\)。两个参数\\(\\theta_1,\\theta_2\\)下似然函数的比值称为似然比，即 \\[LR=\\frac{L(x_{1{:}n};\\theta_2)}{L(x_{1{:}n};\\theta_1)}.\\] 考虑最简单的假设检验\\((\\theta_1\\neq \\theta_2)\\)： \\[\\begin{equation} H_0: \\theta=\\theta_1\\ vs.\\ \\theta=\\theta_2. \\tag{3.4} \\end{equation}\\] 似然比检验的拒绝域为： \\[\\begin{equation} W=\\{x_{1{:}n}:\\frac{L(x_{1{:}n};\\theta_2)}{L(x_{1{:}n};\\theta_1)}&gt; \\lambda\\}=\\{x_{1{:}n}:LR&gt;\\lambda\\}, \\tag{3.5} \\end{equation}\\] 其中\\(\\lambda\\ge 0\\)满足 \\[P_{\\theta_1}(X_{1{:}n}\\in W)=\\alpha.\\] 定理 3.1 (Neyman-Pearson引理) 对于简单假设检验(3.4)，似然比检验得到的拒绝域(3.5)是UMP. 证明. 只证明连续总体的情况，离散总体类似。此时，\\(P_{\\theta}(X_{1{:}n}\\in W)=\\int_W L(x_{1{:}n};\\theta) d x_{1{:}n}\\). 对任意拒绝域\\(W&#39;\\)满足\\(P_{\\theta_1}(X_{1{:}n}\\in W&#39;)\\le\\alpha\\)有 \\[\\begin{align*} P_{\\theta_2}(X_{1{:}n}\\in W)-P_{\\theta_2}(X_{1{:}n}\\in W&#39;) &amp;= \\int_W L(x_{1{:}n};\\theta_2) d x_{1{:}n}-\\int_{W&#39;} L(x_{1{:}n};\\theta_2) d x_{1{:}n}\\\\&amp; =\\int_{W-W&#39;} L(x_{1{:}n};\\theta_2) d x_{1{:}n}-\\int_{W&#39;-W} L(x_{1{:}n};\\theta_2) d x_{1{:}n}\\\\ &amp;\\ge \\lambda \\left(\\int_{W-W&#39;} L(x_{1{:}n};\\theta_1) d x_{1{:}n}-\\int_{W&#39;-W} L(x_{1{:}n};\\theta_1) d x_{1{:}n}\\right)\\\\ &amp;=\\lambda \\left(\\int_{W} L(x_{1{:}n};\\theta_1) d x_{1{:}n}-\\int_{W&#39;} L(x_{1{:}n};\\theta_1) d x_{1{:}n}\\right)\\\\ &amp;=\\lambda(\\alpha-P_{\\theta_1}(X_{1{:}n}\\in W&#39;))\\ge 0. \\end{align*}\\] 这表明\\(W\\)的功效不小于\\(W&#39;\\)的功效，根据UMP的定义，得证。 该定理为著名的Neyman-Pearson引理，是假设检验中非常重要的结果，而且该UMP拒绝域在概率意义下是唯一的。但注意到，该结论只适用简单假设，且\\(\\lambda\\)必须满足\\(P_{\\theta_1}(X_{1{:}n} \\in W)=\\alpha\\). 定理 3.2 似然比检验得到的拒绝域(3.5)是无偏的，即\\(\\rho_{W}(\\theta_2)\\ge \\alpha.\\) 证明见陈家鼎等编著的教材P71页。 3.3.3 正态分布均值的UMP检验 例 3.4 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\sigma^2\\)已知， 样本为\\(X_{1{:}n}\\). 考虑检验水平为\\(\\alpha\\)的检验问题\\((\\mu_2&gt;\\mu_1)\\), \\[H_0:\\mu=\\mu_1\\ vs. \\ H_1:\\mu=\\mu_2.\\] 解. 似然比检验的拒绝域为：\\(W=\\{x_{1{:}n}:\\frac{L(x_{1{:}n};\\mu_2)}{L(x_{1{:}n};\\mu_1)}&gt; \\lambda\\}\\). 似然比为 \\[LR=\\frac{L(x_{1{:}n};\\mu_2)}{L(x_{1{:}n};\\mu_1)}=\\prod_{i=1}^n\\frac{f(x_i;\\mu_2,\\sigma^2)}{f(x_i;\\mu_1,\\sigma^2)}=e^{\\frac{n(\\mu_2-\\mu_1)(2\\bar x-\\mu_1-\\mu_2)}{2\\sigma^2}}.\\] \\(LR&gt;\\lambda\\)等价于\\(\\bar x&gt;C\\), 其中\\(P_{\\mu_1}(\\bar X&gt; C)=\\alpha\\)。注意到\\(\\bar X \\stackrel{H_0}\\sim N(\\mu_1,\\sigma^2/n)\\), 于是有 \\(C=\\mu_1+u_{1-\\alpha}\\sigma/\\sqrt{n}\\), 所以似然比检验的拒绝域为 \\[W=\\{x_{1{:}n}:\\bar x&gt; \\mu_1+u_{1-\\alpha}\\sigma/\\sqrt{n}\\}.\\] 思考：与双侧拒绝域比较：\\(W&#39;=\\{x_{1{:}n}:|\\bar x-\\mu_1|&gt; u_{1-\\alpha/2}\\sigma/\\sqrt{n}\\}\\), 哪个功效大？ 图 3.1: 单侧与双侧功效比较：红色虚线为单侧临界值，绿色虚线为双侧临界值 从上图中容易分析出，\\(W\\)的功效比\\(W&#39;\\)的功效大，与N-P引理的结论吻合。 例 3.5 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\sigma^2\\)已知。考虑检验水平为\\(\\alpha\\)的检验问题\\((\\mu_2&lt;\\mu_1)\\), \\[H_0:\\mu=\\mu_1\\ vs. \\ H_1:\\mu=\\mu_2.\\] 解. 似然比检验的拒绝域为：\\(W=\\{x_{1{:}n}:\\frac{L(x_{1{:}n};\\mu_2)}{L(x_{1{:}n};\\mu_1)}&gt; \\lambda\\}\\). 似然比与上个例子一样， \\[LR=e^{\\frac{n(\\mu_2-\\mu_1)(2\\bar X-\\mu_1-\\mu_2)}{2\\sigma^2}}\\] 区别在于UMP拒绝域为\\(W=\\{x_{1{:}n}:\\bar x&lt;C\\}\\), \\(C\\)满足\\(P_{\\mu_1}(\\bar X&lt; C)=\\alpha\\), 则有\\(C=\\mu_1+u_{\\alpha}\\sigma/\\sqrt{n}\\), 所以似然比检验的拒绝域为\\(W=\\{x_{1{:}n}:\\bar x&lt; \\mu_1+u_{\\alpha}\\sigma/\\sqrt{n}\\}\\). 例 3.6 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\sigma^2\\)已知， 样本为\\(X_{1{:}n}\\). 分别求以下假设检验的一致最大功效拒绝域： \\[H_0:\\mu=\\mu_1\\ vs. \\ H_1:\\mu&gt;\\mu_1,\\] \\[H_0:\\mu\\le\\mu_1\\ vs. \\ H_1:\\mu&gt;\\mu_1,\\] \\[H_0:\\mu=\\mu_1\\ vs. \\ H_1:\\mu&lt;\\mu_1,\\] \\[H_0:\\mu\\ge\\mu_1\\ vs. \\ H_1:\\mu&lt;\\mu_1.\\] 解. 先考虑第一种情况\\(H_0:\\mu=\\mu_1\\ vs. \\ H_1:\\mu&gt;\\mu_1\\). 取\\(\\mu_2\\)为大于\\(\\mu_1\\)的任意常数，构造例3.5中的简单检验 \\[H_0:\\mu=\\mu_1\\ vs. \\ H_1:\\mu=\\mu_2.\\] 设\\(W&#39;\\)为满足\\(P_{\\mu_1}(X_{1{:}n}\\in W&#39;)\\le\\alpha\\)的任意拒绝域。注意到例3.5中的UMP拒绝域\\(W=\\{x_{1{:}n}:\\bar x&gt; \\mu_1+u_{1-\\alpha}\\sigma/\\sqrt{n}\\}\\)不依赖\\(\\mu_2\\)的值。对于任意\\(\\mu_2&gt;\\mu_1\\), 由于\\(W\\)是上述简单假设的UMP拒绝域，所以\\(P_{\\mu_2}(X_{1{:}n}\\in W)\\ge P_{\\mu_2}(X_{1{:}n}\\in W&#39;)\\). 根据UMP的定义，该拒绝域\\(W\\)用于复合的备选假设\\(H_1:\\mu&gt;\\mu_1\\)同样是UMP的。 现在证明\\(W\\)用于第二种情况\\(H_0:\\mu\\le\\mu_1\\ vs. \\ H_1:\\mu&gt;\\mu_1\\)同样是UMP。注意到功效函数 \\[\\rho_W(\\mu)=P_\\mu(X_{1{:}n}\\in W)= P_\\mu(\\bar X&gt; \\mu_1+u_{1-\\alpha}\\sigma/\\sqrt{n})\\] 在\\(\\mu\\le \\mu_1\\)上是单调递增的。所以，\\(\\sup_{\\mu\\le\\mu_1}\\rho_W(\\mu)=\\rho_W(\\mu_1)=\\alpha\\). 设\\(W&#39;\\)为满足\\(\\sup_{\\mu\\le\\mu_1}P(X_{1{:}n}\\in W&#39;)\\le \\alpha\\)的任意拒绝域。所以\\(P_{\\mu_1}(X_{1{:}n}\\in W&#39;)\\le \\alpha\\). 注意到\\(W\\)为上述简单假设检验的UMP拒绝域，所以它比\\(W&#39;\\)更有效。对于任意\\(\\mu_2&gt;\\mu_1\\), 同样有\\(P_{\\mu_2}(X_{1{:}n}\\in W)\\ge P_{\\mu_2}(X_{1{:}n}\\in W&#39;)\\). 根据UMP的定义，该拒绝域\\(W\\)用于第二种情况是UMP。 同样地，后面两种情况的UMP拒绝域和例3.6一样，即 \\[W=\\{x_{1{:}n}:\\bar x&lt; \\mu_1+u_{\\alpha}\\sigma/\\sqrt{n}\\}.\\] 3.4 单参数指数型分布族 为了得到一般UMP拒绝域的一般形式，本节考虑一大类分布族——单参数指数型分布族。 定义 3.5 设\\(X\\)的可能的集合为\\(\\mathcal{X}\\). 称\\(X\\)服从单参数指数型分布(single-parameter exponential family)，若\\(X\\)的密度函数（或者分布列）有下列表达式 \\[\\begin{equation} f(x;\\theta) = S(\\theta)h(x)\\exp\\{Q(\\theta)V(x)\\}, \\tag{3.6} \\end{equation}\\] 其中\\(\\theta\\in\\Theta=(a,b),-\\infty\\le a&lt;b\\le \\infty,S(\\theta)&gt;0,x\\in \\mathcal{X},h(x)&gt;0,Q(\\theta)\\)是\\(\\theta\\)的严格增函数。 常见的分布都是指数型分布，比如： 指数分布：\\(f(x;\\lambda)=\\lambda e^{-\\lambda x}\\), \\(Q(\\lambda)=\\lambda\\), \\(V(x)=-x\\) Poisson分布：\\(f(x;\\lambda)=\\frac{e^{-\\lambda}\\lambda^x}{x!}=\\frac{e^{-\\lambda}e^{\\log(\\lambda) x}}{x!}\\), \\(Q(\\lambda)=\\log(\\lambda)\\), \\(V(x)=x\\) 正态分布(\\(\\sigma^2\\)已知)：\\(f(x;\\mu) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\), \\(Q(\\mu)=\\mu/\\sigma^2\\), \\(V(x)=x\\) 正态分布(\\(\\mu\\)已知)：\\(Q(\\sigma^2)=-\\frac{1}{2\\sigma^2}\\), \\(V(x)=(x-\\mu)^2\\) 我们将在单参数分布族总体下考虑以下五种常见的检验类型并给出UMP/UMPU拒绝域的一般形式。涉及的证明将省略，详情可参考陈家鼎等编著的教材P72-P87. \\[H_0:\\theta\\le \\theta_1\\ vs.\\ H_1:\\theta&gt;\\theta_1,\\] \\[H_0:\\theta\\ge \\theta_1\\ vs.\\ H_1:\\theta&lt;\\theta_1,\\] \\[H_0:\\theta\\notin (\\theta_1,\\theta_2)\\ vs.\\ H_1:\\theta\\in (\\theta_1,\\theta_2),\\] \\[H_0:\\theta\\in [\\theta_1,\\theta_2]\\ vs.\\ H_1:\\theta\\notin [\\theta_1,\\theta_2].\\] \\[H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta\\neq\\theta_0\\] 定理 3.3 考虑单参数指数型分布(3.6)，给定检验问题 \\[H_0:\\theta\\le \\theta_1\\ vs.\\ H_1:\\theta&gt;\\theta_1.\\] 对\\(\\alpha\\in(0,1)\\), 若存在\\(C\\)满足 \\[P_{\\theta_1}\\left(\\sum_{i=1}^n V(X_i)&gt;C\\right)=\\alpha,\\] 则检验水平为\\(\\alpha\\)的UMP拒绝域为：\\[W=\\{x_{1{:}n}:\\sum_{i=1}^n V(x_i)&gt;C\\}.\\] 定理 3.4 考虑单参数指数型分布(3.6)，给定检验问题 \\[H_0:\\theta\\ge \\theta_1\\ vs.\\ H_1:\\theta&lt;\\theta_1.\\] 对\\(\\alpha\\in(0,1)\\), 若存在\\(C\\)满足 \\[P_{\\theta_1}\\left(\\sum_{i=1}^n V(X_i)&lt;C\\right)=\\alpha,\\] 则检验水平为\\(\\alpha\\)的UMP拒绝域为： \\[W=\\{x_{1{:}n}:\\sum_{i=1}^n V(x_i)&lt;C\\}.\\] 例 3.7 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\sigma^2\\)已知， 样本为\\(X_{1{:}n}\\). 求下列检验的UMP \\[H_0:\\mu\\le\\mu_1\\ vs. \\ H_1:\\mu&gt;\\mu_1.\\] 解. 因为在指数分布族形式中\\(V(x)=x\\), UMP拒绝域为 \\[W=\\{x_{1{:}n}:\\sum_{i=1}^nx_i&gt;C\\}=\\{x_{1{:}n}:\\bar x&gt;C&#39;\\},\\] 其中\\(C&#39;\\)满足 \\(P_{\\mu_1}(\\bar X&gt;C&#39;)=\\alpha\\), \\(C&#39;=\\mu_1+u_{1-\\alpha}\\sigma/\\sqrt{n}.\\) 例 3.8 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\mu\\)已知，样本为\\(X_{1{:}n}\\). 求下列检验的UMP \\[H_0:\\sigma^2\\le\\sigma^2_1\\ vs. \\ H_1:\\sigma^2&gt;\\sigma^2_1.\\] 解. 因为在指数分布族形式中\\(V(x)=(x-\\mu)^2\\), UMP拒绝域为 \\[W=\\{x_{1{:}n}:\\sum_{i=1}^n(x_i-\\mu)^2&gt;C\\}=\\{x_{1{:}n}:\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{\\sigma^2_1}&gt;C&#39;\\},\\] 其中\\(C&#39;\\)满足\\(P_{\\sigma^2_1}(\\sum_{i=1}^n\\frac{(X_i-\\mu)^2}{\\sigma^2_1}&gt;C&#39;)=\\alpha\\), 所以\\(C&#39;=\\chi^2_{1-\\alpha}(n)\\). 定理 3.5 考虑单参数指数型分布(3.6)，给定检验问题 \\[ H_0:\\theta\\notin (\\theta_1,\\theta_2)\\ vs.\\ H_1:\\theta\\in (\\theta_1,\\theta_2). \\] 令 \\[W=\\{x_{1{:}n}:C_1&lt;\\sum_{i=1}^n V(x_i)&lt;C_2\\}.\\] 若存在\\(C_1,C_2\\)满足 \\[P_{\\theta_1}(X_{1{:}n}\\in W)=P_{\\theta_2}(X_{1{:}n}\\in W)=\\alpha,\\] 则检验水平为\\(\\alpha\\)的一致最大功效的拒绝域为\\(W\\). 定理 3.6 考虑单参数指数型分布(3.6)，给定检验问题 \\[H_0:\\theta\\in [\\theta_1,\\theta_2]\\ vs.\\ H_1:\\theta\\notin [\\theta_1,\\theta_2].\\] 令 \\[W=\\{x_{1{:}n}:\\sum_{i=1}^n V(x_i)\\notin[C_1,C_2]\\}.\\] 若存在\\(C_1,C_2\\)满足 \\[P_{\\theta_1}(X_{1{:}n}\\in W)=P_{\\theta_2}(X_{1{:}n}\\in W)=\\alpha\\] 则检验水平为\\(\\alpha\\)的一致最大功效无偏(UMPU)的拒绝域为\\(W\\). 定理 3.7 考虑上述单参数指数型分布，给定检验问题 \\[H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta\\neq\\theta_0.\\] 令\\(W=\\{x_{1{:}n}:\\sum_{i=1}^n V(x_i)\\notin[C_1,C_2]\\}.\\) 若存在\\(C_1,C_2\\)满足 \\[P_{\\theta_0}(X_{1{:}n}\\in W)=\\alpha\\] \\[E_{\\theta_0}\\left[1\\{X_{1{:}n}\\in W\\}\\sum_{i=1}^n V(X_i)\\right]=\\alpha E_{\\theta_0}\\left[\\sum_{i=1}^n V(X_i)\\right],\\] 则检验水平为\\(\\alpha\\)的一致最大功效无偏(UMPU)的拒绝域为\\(W\\). 推论 3.1 考虑上述单参数指数型分布，给定检验问题 \\[H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta\\neq\\theta_0.\\] 如果在\\(\\theta=\\theta_0\\)下，\\(T(x_{1{:}n}) = \\sum_{i=1}^n V(X_i)\\)的分布关于某数\\(r_0\\)对称，取 \\[W=\\{x_{1{:}n}:|T(x_{1{:}n})-r_0|&gt;C\\}.\\] 若存在\\(C\\)满足 \\[P_{\\theta_0}(X_{1{:}n}\\in W)=\\alpha,\\] 则检验水平为\\(\\alpha\\)的一致最大功效无偏(UMPU)的拒绝域为\\(W\\). 例 3.9 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\sigma^2\\)已知。求下列检验的UMPU拒绝域 \\[H_0:\\mu=\\mu_0\\ vs. \\ H_1:\\mu\\neq\\mu_0.\\] 解. 因为在指数分布族形式中\\(V(x)=x\\), 此时\\(T(x_{1{:}n})=\\sum_{i=1}^nX_i\\). 在\\(\\mu=\\mu_0\\)下，\\(T(X_{1{:}n})\\sim N(n\\mu_0,n\\sigma^2)\\), 故其分布关于\\(r_0=n\\mu_0\\)对称， UMPU拒绝域为 \\[W=\\{x_{1{:}n}:|T(x_{1{:}n})-n\\mu_0|&gt;C\\}=\\{x_{1{:}n}:|\\bar x-\\mu_0|&gt;C&#39;\\},\\] 其中\\(C&#39;\\)满足\\(P_{\\mu_0}(|\\bar X-\\mu_0|&gt;C&#39;)=\\alpha\\), 所以\\(C&#39;=u_{1-\\alpha/2}\\sigma/\\sqrt{n}\\). 例 3.10 假设总体为\\(N(\\mu,\\sigma^2)\\), 其中\\(\\mu\\)已知。求下列检验的UMPU拒绝域 \\[H_0:\\sigma^2=\\sigma_0^2\\ vs. \\ H_1:\\sigma^2\\neq\\sigma_0^2.\\] 解. 为方便起见，令\\(T(x_{1{:}n})=\\sum_{i=1}^n(X_i-\\mu)^2/\\sigma_0^2\\). 这样， 在\\(\\sigma^2=\\sigma_0^2\\)下，\\(T(X_{1{:}n})\\sim \\chi^2(n)\\). UMPU拒绝域表示为 \\(W=\\{x_{1{:}n}:T(x_{1{:}n})\\notin [C_1,C_2]\\}\\)，其中\\(C_1,C_2\\)满足 \\[P_{\\sigma^2_0}(T(X_{1{:}n})\\notin W)=\\int_{C_1}^{C_2} f(x;n) dx=1-\\alpha,\\] \\[E_{\\sigma^2_0}[1\\{X_{1{:}n}\\notin W\\}T(X_{1{:}n})]=\\int_{C_1}^{C_2} x f(x;n)dx=(1-\\alpha)E_{\\sigma_0^2}[T(X_{1{:}n})]=n(1-\\alpha).\\] 其中，\\(f(x;n)\\)为\\(\\chi^2(n)\\)的密度函数，即 \\[f(x;n)=\\frac{1}{2^{n/2}\\Gamma(n/2)}x^{n/2-1}e^{-x/2}1\\{x&gt;0\\}.\\] 所以， \\[\\begin{align*} \\int_{C_1}^{C_2} \\frac{x}{n} f(x;n)dx&amp;=\\int_{C_1}^{C_2} \\frac{x}{n} \\frac{1}{2^{n/2}\\Gamma(n/2)}x^{n/2-1}e^{-x/2}dx\\\\ &amp;=\\int_{C_1}^{C_2} f(x;n+2)dx=\\int_{C_1}^{C_2} f(x;n) dx=1-\\alpha. \\end{align*}\\] 实际上，求解\\(C_1,C_2\\)比较困难，为方便起见，不妨用平均法取\\(C_1=\\chi_{\\alpha/2}^2(n), C_2=\\chi_{1-\\alpha/2}^2(n)\\). 3.4.1 小结 我们已经在单参数指数型分布总体下给出常见的假设检验的UMP/UMPU, 具体步骤可以归纳如下： 根据指数型分布写出检验统计量(test statistic): \\(T(X_{1{:}n})=\\sum_{i=1}^nV(X_i)\\)，或者它的常数倍 根据假设检验的类型写出拒绝域\\(W\\)的形式，一般有 \\[T(x_{1{:}n})&gt;C,T(x_{1{:}n})&lt;C,Tx_{1{:}n} \\in (C_1,C_2),T(x_{1{:}n})\\notin [C_1,C_2]\\] 如果是只有一个待定参数\\(C\\)时，可以检验水平\\(\\alpha\\)来得到\\(C\\)的值，即 \\[P_{\\theta_0}(T(X_{1{:}n}) \\in W)=\\alpha,\\] 其中\\(\\theta_0\\)为\\(\\Theta_0\\)的边界点。 如果有两个待定参数\\(C_1,C_2\\)时, 可能还需另外一个等式来求解（比如双边假设检验）。 正态总体的期望的检验统计量为\\(n\\bar X\\)或者\\(\\bar X\\), 称为U检验；方差的检验统计量为\\(\\sum_{i=1}^n(X_i-\\mu)^2\\), 称为卡方检验。 3.5 广义似然比检验 似然比检验只适合简单假设检验，为了适用于复合假设检验情形，我们需要将似然比检验进行推广。 考虑一般的参数假设检验问题 \\[H_0:\\theta\\in \\Theta_0\\ vs.\\ H_1:\\theta \\notin \\Theta_0.\\] 定义广义似然比为： \\[\\lambda(x_{1{:}n}):=\\frac{\\sup_{\\theta\\in \\Theta}L(x_{1{:}n};\\theta)}{\\sup_{\\theta\\in \\Theta_0}L(x_{1{:}n};\\theta)}=\\frac{L(x_{1{:}n};\\hat\\theta)}{L(x_{1{:}n};\\hat\\theta_0)},\\] 其中\\(\\hat\\theta\\)为\\(\\theta\\)的最大似然估计，\\(\\hat\\theta_0\\)为\\(\\theta\\)限制在\\(\\Theta_0\\)上的最大似然估计。 广义似然比拒绝域为： \\[W=\\{x_{1{:}n}:\\lambda(x_{1{:}n})&gt;\\lambda_0\\},\\] 其中\\(\\lambda_0\\ge 1\\)满足\\(\\sup_{\\theta\\in\\Theta_0}P_{\\theta}(X_{1{:}n}\\in W)=\\alpha\\), \\(\\alpha\\)为给定的显著性水平。 广义似然比检验的思想：如果\\(H_0\\)成立，则似然函数在\\(\\theta\\in\\Theta_0\\)的最大值应当与全局最大值接近，如果两者相差很大，则有理由拒绝原假设。 注1：如果退化成简单检验问题，\\(\\Theta_0=\\{\\theta_1\\},\\ \\Theta_1=\\{\\theta_2\\}\\)，则广义似然比简化为 \\[\\lambda(x_{1{:}n}):=\\frac{\\max(L(x_{1{:}n};\\theta_1),L(x_{1{:}n};\\theta_2))}{L(x_{1{:}n};\\theta_1)}.\\] \\(\\lambda(x_{1{:}n})&gt;\\lambda_0\\)等价于\\(L(x_{1{:}n};\\theta_2)/L(x_{1{:}n};\\theta_1)&gt;\\lambda_0&#39;\\)，该形式与简单似然比拒绝域相同，故称之为“广义”似然比方法。 注2：广义似然比检验适用范围非常广，虽然它不一定是UMP的，但在多数情况下可以证明随着样本量\\(n\\to\\infty\\)，检验的功效收敛到1，也就是犯第二类错误的概率收敛到0. 注3：设充分统计量为\\(\\psi(x_{1{:}n})\\), 由因子分解定理知，\\(L(x_{1{:}n};\\theta)=g(\\psi(x_{1{:}n}),\\theta)h(x_{1{:}n})\\). \\[\\begin{align*} \\lambda(x_{1{:}n}):&amp;=\\frac{\\sup_{\\theta\\in\\Theta} g(\\psi(x_{1{:}n}),\\theta)h(x_{1{:}n})}{\\sup_{\\theta\\in\\Theta_0} g(\\psi(x_{1{:}n}),\\theta)h(x_{1{:}n})}\\\\&amp;=\\frac{\\sup_{\\theta\\in\\Theta} g(\\psi(x_{1{:}n}),\\theta)}{\\sup_{\\theta\\in\\Theta_0} g(\\psi(x_{1{:}n}),\\theta)}=:\\ell(\\psi(x_{1{:}n})). \\end{align*}\\] 可以看出，广义似然比是充分统计量的函数，所以拒绝域可以写成 \\[W=\\{x_{1{:}n}:\\lambda(x_{1{:}n})&gt;\\lambda_0\\}=\\{x_{1{:}n}:\\psi(x_{1{:}n}) \\in B\\}.\\] 问题转化成求解集合\\(B\\)使得检验水平为\\(\\alpha\\). 如果充分统计量在给定\\(\\theta=\\theta_0\\)下容易得到，这个问题则比较容易处理。 下面只针对正态总体来分析。 3.5.1 正态总体的假设检验 设总体\\(X\\sim N(\\mu,\\sigma^2)\\)。前面我们已经分析了正态总体下参数的简单假设检验问题，现在利用广义似然比来分析复合假设检验。只考虑下面三种情况： \\[\\theta=\\theta_0\\ vs.\\ \\theta\\neq \\theta_0\\] \\[\\theta\\le \\theta_0\\ vs.\\ \\theta&gt;\\theta_0\\] \\[\\theta\\ge \\theta_0\\ vs.\\ \\theta&lt;\\theta_0\\] 其中\\(\\theta=\\mu\\) 或者 \\(\\sigma^2\\), 另外一个参数已知或者未知（分情况讨论）。 似然函数为： \\[L(\\mu,\\sigma^2)=(2\\pi \\sigma^2)^{-n/2}\\exp\\left(-\\frac 1{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right).\\] 期望的假设检验（方差已知）： 考虑双边假设检验 \\[H_0:\\mu=\\mu_0\\ vs.\\ H_1:\\mu\\neq \\mu_0.\\] 因为方差\\(\\sigma^2\\)已知，\\(\\theta=\\mu\\)的最大似然估计为\\(\\bar X\\)。此时的广义似然比为 \\[\\begin{align*} \\lambda(x_{1{:}n})&amp;=\\frac{L(\\bar x,\\sigma^2)}{L(\\mu_0,\\sigma^2)}\\\\ &amp;=\\exp\\left(\\frac{\\sum_{i=1}^n[(x_i-\\mu_0)^2-(x_i-\\bar x)^2]}{2\\sigma^2}\\right)\\\\ &amp;=\\exp\\left(\\frac{n(\\bar x-\\mu_0)^2}{2\\sigma^2}\\right). \\end{align*}\\] 则拒绝域为\\(W=\\{x_{1{:}n}:|\\bar x-\\mu_0|&gt;c\\}\\)，其中\\(c\\)满足\\(P_{\\mu_0}(|\\bar X-\\mu_0|&gt;c)=\\alpha\\)。所以，\\(c=u_{1-\\alpha/2}\\sigma/\\sqrt{n}\\). 令 \\[U:=\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}.\\] 当\\(|U|&gt;u_{1-\\alpha/2}\\)时，拒绝\\(H_0\\). 它称为该检验的检验统计量，\\(U\\stackrel{H_0}\\sim N(0,1)\\)。这个检验也称U检验（或者Z检验）。 接下来考虑单边假设检验 \\[H_0:\\mu\\le \\mu_0\\ vs.\\ H_1:\\mu&gt; \\mu_0.\\] 在\\(\\mu\\le \\mu_0\\)下，\\(\\mu\\)的最大似然估计为\\(\\hat\\mu_0=\\min(\\bar X,\\mu_0)\\). 此时广义似然比为 \\[\\begin{align*} \\lambda(x_{1{:}n})=\\frac{L(\\bar x,\\sigma^2)}{L(\\min(\\bar X,\\mu_0),\\sigma^2)}=\\begin{cases} e^\\frac {n(\\bar x-\\mu_0)^2}{2\\sigma^2},&amp;\\ \\bar x&gt; \\mu_0\\\\ 1,&amp;\\ \\bar x\\le\\mu_0. \\end{cases} \\end{align*}\\] 注意到\\(\\lambda(x_{1{:}n})\\)应当严格大于1，否则拒绝域为样本全空间，故\\(\\bar x&gt;\\mu_0\\)且\\(\\lambda(x_{1{:}n})&gt;\\lambda_0\\ge 1\\). 这等价于拒绝域为\\(W=\\{x_{1{:}n}:\\bar x-\\mu_0&gt;c\\}\\)，其中\\(c\\ge 0\\)满足 \\[\\sup_{\\mu\\le\\mu_0}P_{\\mu}(\\bar X-\\mu_0&gt;c)=P_{\\mu_0}(\\bar X-\\mu_0&gt;c)=\\alpha.\\] 所以，\\(c=u_{1-\\alpha}\\sigma/\\sqrt{n}\\)（注意到\\(\\alpha\\)是接近于0的正数，所以\\(c&gt;0\\)）。 当\\(U&gt;u_{1-\\alpha}\\)时，拒绝\\(H_0\\). 类似地，对于另一种单边假设检验 \\[H_0:\\mu\\ge \\mu_0\\ vs.\\ H_1:\\mu&lt; \\mu_0,\\] 当\\(U&lt;u_{\\alpha}\\)时，拒绝\\(H_0\\). 这三种情况汇总如下： \\(H_1\\) \\(\\mu\\neq \\mu_0\\) \\(\\mu&gt; \\mu_0\\) \\(\\mu&lt; \\mu_0\\) 拒绝域 \\(|u|&gt;u_{1-\\alpha/2}\\) \\(u&gt;u_{1-\\alpha}\\) \\(u&lt;u_{\\alpha}\\) 期望的假设检验（方差未知）：在方差未知的情况下考虑期望的检验问题，相应的检验统计量为 \\[T=\\frac{\\bar X-\\mu_0}{S_n/\\sqrt{n-1}}=\\frac{\\bar X-\\mu_0}{S_n^*/\\sqrt{n}}.\\] 当\\(\\mu=\\mu_0\\)时，\\(T\\sim t(n-1)\\). 该检验称为t检验。拒绝域\\(W\\)形式如下： \\(H_1\\) \\(\\mu\\neq \\mu_0\\) \\(\\mu&gt; \\mu_0\\) \\(\\mu&lt; \\mu_0\\) 拒绝域 \\(|t|&gt;t_{1-\\alpha/2}(n-1)\\) \\(t&gt;t_{1-\\alpha}(n-1)\\) \\(t&lt;t_{\\alpha}(n-1)\\) 图 3.2: 均值的假设检验 方差的假设检验（期望已知）： 检验统计量为 \\[V_1 = \\frac{1}{\\sigma^2_0}\\sum_{i=1}^n(X_i-\\mu)^2.\\] 当\\(\\sigma^2=\\sigma^2_0\\)时，\\(V_1\\sim \\chi^2(n)\\)。该检验称为卡方检验。拒绝域\\(W\\)形式如下： \\(H_1\\) \\(\\sigma^2\\neq \\sigma^2_0\\) \\(\\sigma^2&gt; \\sigma^2_0\\) \\(\\sigma^2&lt; \\sigma^2_0\\) 拒绝域 \\(v_1&gt;\\chi^2_{1-\\alpha/2}(n)\\) 或 \\(v_1&lt;\\chi^2_{\\alpha/2}(n)\\) \\(v_1&gt;\\chi^2_{1-\\alpha}(n)\\) \\(v_1&lt;\\chi^2_{\\alpha}(n)\\) 方差的假设检验（期望未知）： 检验统计量为 \\[V_2 = \\frac{1}{\\sigma^2_0}\\sum_{i=1}^n(X_i-\\bar X)^2=nS_n^2/\\sigma_0^2.\\] 当\\(\\sigma^2=\\sigma^2_0\\)时，\\(V_2\\sim \\chi^2(n-1)\\)。该检验同样为卡方检验。拒绝域\\(W\\)形式如下： \\(H_1\\) \\(\\sigma^2\\neq \\sigma^2_0\\) \\(\\sigma^2&gt; \\sigma^2_0\\) \\(\\sigma^2&lt; \\sigma^2_0\\) 拒绝域 \\(v_2&gt;\\chi^2_{1-\\alpha/2}(n-1)\\) 或 \\(v_2&lt;\\chi^2_{\\alpha/2}(n-1)\\) \\(v_2&gt;\\chi^2_{1-\\alpha}(n-1)\\) \\(v_2&lt;\\chi^2_{\\alpha}(n-1)\\) 例 3.11 (血液酒精浓度测试) 下面是一台已使用三年的仪器测出某人血液酒精浓度的30个数据（百分比）。已知精准的机器给出读数为12.6%。请根据这些数据检验这台仪器是否精准，是否需要校准。 12.3 12.7 13.6 12.7 12.9 12.6 12.6 13.1 12.6 13.1 12.7 12.5 13.2 12.8 12.4 12.6 12.4 12.4 13.1 12.9 13.3 12.6 12.6 12.7 13.1 12.4 12.4 13.1 12.4 12.9 如果这台老仪器的方差\\(\\sigma=0.4\\). 在置信水平\\(\\alpha=0.05\\)下，你是否建议对该仪器进行校准？如果置信水平减小到\\(\\alpha=0.01\\)，你的结论会不会发生改变？ 解. 假设这台仪器读数服从正态分布\\(N(\\mu,\\sigma^2)\\). 题中问题可描述成 \\[H_0:\\mu=12.6\\ vs.\\ H_1:\\mu\\neq 12.6.\\] 由数据知，\\(\\bar x=12.757.\\) 拒绝域为 \\[W=\\{|\\bar x-12.6|&gt;u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\}.\\] 如果\\(\\alpha=0.05\\), \\(W=\\{|\\bar x-12.6|&gt;0.143\\}\\). 此时，样本落在拒绝域里面，故建议机器进行校准。 如果\\(\\alpha\\)减小到\\(0.01\\), 拒绝域为\\(W=\\{|\\bar x-12.6|&gt;0.188\\}\\). 此时，样本落在拒绝域外，故不建议机器进行校准。 如果上题方差未知，我们需要用到t检验，R的命令为： t.test(x, alternative = c(“two.sided”, “less”, “greater”), mu = 0, conf.level = 0.95, …) x = c(12.3, 12.7, 13.6, 12.7, 12.9, 12.6, 12.6, 13.1, 12.6, 13.1, 12.7, 12.5, 13.2, 12.8, 12.4, 12.6, 12.4, 12.4, 13.1, 12.9, 13.3, 12.6, 12.6, 12.7, 13.1, 12.4, 12.4, 13.1, 12.4, 12.9) t.test(x,mu=12.6) ## ## One Sample t-test ## ## data: x ## t = 2.6, df = 29, p-value = 0.01 ## alternative hypothesis: true mean is not equal to 12.6 ## 95 percent confidence interval: ## 12.64 12.88 ## sample estimates: ## mean of x ## 12.76 因为\\(|t|=2.6444&gt;t_{0.975}(29)=2.04523\\)，所以在显著性水平\\(\\alpha=0.05\\)下拒绝\\(H_0\\)。但\\(|t|=2.6444&lt;t_{0.995}(29)=2.756386\\)，所以在显著性水平\\(\\alpha=0.01\\)下接受\\(H_0\\)。结论与之前一致。 例 3.12 1. 假设有个植物学家跟你说，通过基因组分析表明，山鸢尾(Setosa)花萼长度的均值是4.5cm，他这个论断是否可信？ 另一位植物学家说山鸢尾(Setosa)花萼长度的均值是一个不小于4.5cm的数，但具体是多少就不清楚了，那么他这个论断是否又可信？ x = iris[iris$Species==&quot;setosa&quot;,1] t.test(x,mu=4.5) ## ## One Sample t-test ## ## data: x ## t = 10, df = 49, p-value = 1e-13 ## alternative hypothesis: true mean is not equal to 4.5 ## 95 percent confidence interval: ## 4.906 5.106 ## sample estimates: ## mean of x ## 5.006 t.test(x,mu=4.5, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: x ## t = 10, df = 49, p-value = 1 ## alternative hypothesis: true mean is less than 4.5 ## 95 percent confidence interval: ## -Inf 5.09 ## sample estimates: ## mean of x ## 5.006 3.5.2 两个独立正态总体的检验 有时候，我们需要比较两个总体的差异性。例如，男生群体平均身高是否大于女生群体平均身高？山鸢尾和杂色鸢尾两种花的花萼有没有显著差异？大部分问题归结为比较两个总体均值或者方差的差异性。 前提条件：设总体\\(X\\sim N(\\mu_1,\\sigma_1^2)\\), 另有与\\(X\\)独立的总体\\(Y\\sim N(\\mu_2,\\sigma_2^2)\\). 两个总体的样本分别为\\(X_1,\\dots,X_m\\); \\(Y_1,\\dots,Y_n\\)，修正样本方差分别为\\(S_X^{*2},\\ S_Y^{*2}\\). 考虑检验问题： \\[H_0: \\mu_1-\\mu_2=\\delta\\ vs.\\ H_1: \\mu_1-\\mu_2\\neq \\delta.\\] 大部分应用场景取\\(\\delta=0\\). 如果\\(\\sigma_1^2,\\sigma_2^2\\)已知, 选择U检验统计量： \\[U=\\frac{\\bar X-\\bar Y-\\delta}{\\sqrt{\\sigma_1^2/m+\\sigma_2^2/n}}\\stackrel{H_0}\\sim N(0,1).\\] 若\\(|U|&gt; u_{1-\\alpha/2}\\)拒绝\\(H_0\\)，否则接受\\(H_0\\). 如果\\(\\sigma_1^2,\\sigma_2^2\\)未知，已知\\(\\sigma_1^2=\\sigma_2^2\\)，选择t检验统计量： \\[T=\\frac{\\bar X-\\bar Y-\\delta}{S_w\\sqrt{1/m+1/n}}\\stackrel{H_0}\\sim t(m+n-2),\\] 其中\\(S_w^2=[(m-1)S_X^{2*}+(n-1)S_Y^{*2}]/(m+n-2)\\)为合并的样本方差。 若\\(|T|&gt; t_{1-\\alpha/2}(n+m-2)\\)拒绝\\(H_0\\)，否则接受\\(H_0\\). 如果\\(\\sigma_1^2,\\sigma_2^2\\)未知，但\\(\\sigma_1^2\\neq\\sigma_2^2\\), 选择检验统计量： \\[T=\\frac{(\\bar X-\\bar Y)-\\delta}{\\sqrt{S_X^{*2}/m+S_Y^{*2}/n}}.\\] 在\\(\\mu_1-\\mu_2=\\delta\\)下，\\(T\\)近似服从自由度为\\(k\\)的\\(t\\)分布，其中\\(k\\)为接近\\(k^*\\)的整数， \\[k^*=\\frac{(S_{X}^{*2}/m+S_{Y}^{*2}/n)^2}{(S_{X}^{*2}/m)^2/(m-1)+(S_Y^{*2}/n)^2/(n-1)}\\] 这就是著名的Behrens-Fisher问题，该检验为Welch’s t-test。 配对检验：假设两个总体的样本量都一样，即\\(m=n\\)。此时，令\\[Z=X-Y\\sim N(\\mu_1-\\mu_2,\\sigma_1^2+\\sigma^2_2),\\] \\(Z\\)的样本为\\(Z_i=X_i-Y_i\\), \\(i=1,\\dots,n.\\) 原问题可以转化成关于\\(Z\\)的均值的检验，故可构造t检验统计量 \\[T=\\frac{\\bar Z-\\delta}{S_Z^*/\\sqrt{n}}\\sim t(n-1),\\] 其中\\(S_Z^{*2}\\)为\\(Z_i\\)的修正样本方差。这种检验方法称配对检验(paired test), 优点是对两个总体的方差没有要求，甚至不要求两个总体独立。但只适用于\\(m=n\\)的情况。如果\\(m\\neq n\\), 这种配对检验就不太适合，尤其是两个样本量相差很大时，做配对检验就不得不舍弃大量的样本，造成一定的信息丢失。 考虑方差比值的检验问题 \\[H_0: \\sigma_1^2=\\sigma_2^2,\\ H_1: \\sigma_1^2\\neq \\sigma_2^2\\] 如果\\(\\mu_1,\\mu_2\\)已知，选择F检验统计量： \\[F_1=\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2}\\stackrel{H_0}\\sim F(m,n).\\] 若\\(F_1&gt; F_{1-\\alpha/2}(m,n)\\)或者\\(F_1&lt;F_{\\alpha/2}(m,n)\\)拒绝\\(H_0\\)，否则接受\\(H_0\\). 如果\\(\\mu_1,\\mu_2\\)未知，选择F检验统计量： \\[F_2=\\frac{\\frac 1 {m-1}\\sum_{i=1}^m(X_i-\\bar X)^2}{\\frac 1 {n-1}\\sum_{i=1}^n(Y_i-\\bar Y)^2}=\\frac{S_X^{*2}}{S_Y^{*2}}\\stackrel{H_0}\\sim F(m-1,n-1)\\quad.\\] 若\\(F_2&gt; F_{1-\\alpha/2}(m-1,n-1)\\)或者\\(F_2&lt;F_{\\alpha/2}(m-1,n-1)\\)拒绝\\(H_0\\)，否则接受\\(H_0\\). 3.5.3 案例分析：山鸢尾和杂色鸢尾花差异性比较 均值差的假设检验R命令： t.test(x, y = NULL, alternative = c(“two.sided”, “less”, “greater”), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, …) 方差比的假设检验R命令： var.test(x, y, ratio = 1, alternative = c(“two.sided”, “less”, “greater”), conf.level = 0.95, …) 假设山鸢尾和杂色鸢尾两种花的花萼长度方差相等时，花萼长度均值的双边假设检验 x = iris[iris$Species==&quot;setosa&quot;,1] y = iris[iris$Species==&quot;versicolor&quot;,1] t.test(x,y,var.equal = TRUE) ## ## Two Sample t-test ## ## data: x and y ## t = -11, df = 98, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.1054 -0.7546 ## sample estimates: ## mean of x mean of y ## 5.006 5.936 因为\\(|t|=10.521&gt;t_{0.975}(98)=1.984467\\)，所以在显著性水平\\(\\alpha=0.05\\)下拒绝\\(H_0\\)，即认为两种花的花萼长度均值相等不显著。 方差不相等时双边假设检验，即Welch的t检验。 t.test(x,y) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -11, df = 87, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.1057 -0.7543 ## sample estimates: ## mean of x mean of y ## 5.006 5.936 因为\\(|t|=10.521&gt;t_{0.975}(87)=1.987608\\)，在显著性水平\\(\\alpha=0.05\\)下同样拒绝\\(H_0\\)，即认为两种花的花萼长度均值相等不显著。 方差不相等时单边假设检验 t.test(x,y,alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -11, df = 87, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.783 ## sample estimates: ## mean of x mean of y ## 5.006 5.936 因为\\(t=-10.521&lt;t_{0.05}(87)=-1.662557\\)，在显著性水平\\(\\alpha=0.05\\)下拒绝\\(H_0\\)，即认为山鸢尾花萼平均长度小于杂色鸢尾花萼平均长度。 山鸢尾和杂色鸢尾两种花的花萼长度方差比的检验 var.test(x,y) ## ## F test to compare two variances ## ## data: x and y ## F = 0.47, num df = 49, denom df = 49, p-value = ## 0.009 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2646 0.8218 ## sample estimates: ## ratio of variances ## 0.4663 因为\\(F=0.46634&lt; F_{0.025}(49,49)=0.5674762\\)，在显著性水平\\(\\alpha=0.05\\)下拒绝\\(H_0\\)，即认为两种花的花萼长度方差相等是不显著的。 所以一开始认为两者方差相等对均值差进行检验是不合适的。 通过配对检验山鸢尾和杂色鸢尾两种花的花萼长度平均水平的差异 t.test(x,y,paired = TRUE) ## ## Paired t-test ## ## data: x and y ## t = -10, df = 49, p-value = 1e-13 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.1142 -0.7458 ## sample estimates: ## mean of the differences ## -0.93 因为\\(|t|=10.146&gt;t_{0.975}(49)=.009575\\)，所以在显著性水平\\(\\alpha=0.05\\)下拒绝\\(H_0\\)，即认为两种花的花萼长度均值相等不显著（与Welch的t检验结果一致）。 3.6 置信区间与假设检验的联系 假设\\(\\theta\\)的100\\((1-\\alpha)\\%\\) 置信区间为 \\([L(X_{1{:}n}),U(X_{1{:}n})]\\). 这表明 \\[P_{\\theta}(\\theta\\in [L,U])=1-\\alpha,\\ \\forall\\theta\\in\\Theta.\\] 考虑假设检验: \\[H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta\\neq\\theta_0.\\] 检验法则: 如果\\(\\theta_0\\notin [L,U]\\), 拒绝原假设；否则接受原假设。于是得到一个拒绝域：\\(W=\\{x_{1{:}n}:\\theta_0\\notin [L(x_{1{:}n}),U(x_{1{:}n})]\\}\\), 显著性水平为 \\[P_{\\theta_0}(\\theta_0\\notin [L,U])=\\alpha\\] 有置信区间诱导的拒绝域可以控制犯第一类错误的概率在水平\\(\\alpha\\)，但这样得到的拒绝域不一定是UMP或者UMPU! 对于正态总体，这种方式得到的拒绝域与之前通过似然比或者广义似然比方法得到一样。 反过来，假如我们有以下检验的一个拒绝域\\(W(\\theta_0)\\) \\[H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta\\neq\\theta_0,\\] 其中\\(P_{\\theta_0}(X_{1{:}n} \\in W(\\theta_0))=\\alpha, \\forall \\theta_0\\in\\Theta\\). 可以得到一个置信集(confidence set): \\[S(X_{1{:}n})=\\{\\theta:X_{1{:}n}\\notin W(\\theta)\\}\\] \\[P_\\theta(\\theta\\in S) = P_{\\theta}(X_{1{:}n}\\notin W(\\theta)) = 1-\\alpha,\\ \\forall\\theta\\in\\Theta.\\] 该置信集是由所有“接受”的\\(\\theta\\)的值组成的 若该置信集为区间形式，则可以得到一个置信区间。 例 3.13 考虑总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其中\\(\\mu\\)未知，方差\\(\\sigma^2\\)已知。则有 \\[P(\\bar X-u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\le \\mu\\le \\bar X+u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}})=1-\\alpha\\] 于是得到一个置信区间：\\([\\bar X-u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\bar X+u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}]\\) 由此可以构造拒绝域： \\[W=\\{\\mu_0\\notin [\\bar x-u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\bar x+u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}]\\}\\] \\[W=\\{x_{1{:}n}:|\\bar x-\\mu_0|&gt;u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\}\\] 此拒绝域和我们之前得到的一样的。 3.7 p值 由例3.11可以看出，当\\(\\alpha\\)变化时，我们需要重新计算拒绝域再进行判断接受或者拒绝。不同的显著性水平得到结论可能不一样。不难看出，对一个固定的样本，\\(\\alpha\\)越大越容易拒绝该样本，越小越容易接受。于是便存在一个临界状态，如果我们能够获得这个临界值，那么检验问题就可以等价转化成显著性水平与该临界值比较大小问题。这就避免了拒绝域的频繁计算。这个临界值称为p值(p-value). 假设拒绝域具备如下形式（似然比和广义似然比检验经常出现这种形式） \\[W=\\{x_{1{:}n}:T(x_{1{:}n})&gt;\\lambda_\\alpha\\},\\] \\(T\\)为检验统计量 \\(\\lambda_\\alpha\\)满足\\(\\sup_{\\theta\\in\\Theta_0}P_{\\theta}(T(X_{1{:}n})&gt;\\lambda_\\alpha)=\\alpha\\) 由此可得： 对于固定的样本，显著性水平\\(\\alpha\\)越大，\\(\\lambda\\)越小，这样越容易拒绝原假设 对于固定的样本，是否存在一个临界值\\(p\\), 使得当\\(p&lt;\\alpha\\)时拒绝原假设，当\\(p\\ge \\alpha\\)时接受原假设？这个临界值称为\\(p\\)值 \\[p=p(x_{1{:}n})=\\sup_{\\theta\\in\\Theta_0}P_\\theta(T(X_{1{:}n})&gt; T(x_{1{:}n})).\\] 定理 3.8 令\\(f(\\lambda) = \\sup_{\\theta\\in\\Theta_0}P_{\\theta}(T(X_{1{:}n})&gt;\\lambda)\\). 如果对任意的\\(\\alpha\\in(0,1)\\), \\(f^{-1}(\\alpha)\\)存在，那么\\(T(x_{1{:}n})&gt;\\lambda_\\alpha\\)当且仅当\\(p(x_{1{:}n})&lt;\\alpha\\). 证明. 由定理条件知，\\(f(\\lambda)\\)为严格单调递减函数。注意到\\(f(\\lambda_\\alpha)=\\alpha\\)且\\(f(T(x_{1{:}n}))=p\\). 所以，\\(T(x_{1{:}n})&gt;\\lambda_\\alpha\\)当且仅当\\(p(x_{1{:}n})&lt;\\alpha\\). 如果原假设是简单的, 即\\(H_0:\\theta=\\theta_0\\)，则 \\[p = P_{\\theta_0}(T(X_{1{:}n})&gt; T(x_{1{:}n})).\\] 对于一些复合的原假设，\\(\\sup_{\\theta\\in\\Theta_0}P_{\\theta}(T(X_{1{:}n})&gt;\\lambda)=P_{\\theta_0}(T(X_{1{:}n})&gt; \\lambda)\\)，其中\\(\\theta_0\\)为\\(\\Theta_0\\)的边界点。 类似地， 如果拒绝域另一种单边情况，\\(W=\\{x_{1{:}n}:T(x_{1{:}n})&lt;\\lambda\\},\\) 则p值为 \\[p=p(x_{1{:}n})=\\sup_{\\theta\\in\\Theta_0}P_\\theta(T(X_{1{:}n})&lt; T(x_{1{:}n})).\\] 如果对检验问题\\(H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta\\neq\\theta_0\\)选择双边拒绝域，\\(W=\\{T(x_{1{:}n})&lt;\\lambda_1\\}\\cup\\{T(x_{1{:}n})&gt;\\lambda_2\\}\\)，其中 \\[P_{\\theta_0}(T(X_{1{:}n})&lt;\\lambda_1)=P_{\\theta_0}(T(X_{1{:}n})&gt;\\lambda_2)=\\alpha/2,\\] 假设\\(\\lambda^*\\)满足\\(P_{\\theta_0}(T(X_{1{:}n})\\le\\lambda^*)=0.5\\)，则\\(\\lambda_1\\leq\\lambda^*\\leq\\lambda_2\\)。则p值为 \\[p= \\begin{cases} 2P_{\\theta_0}(T(X_{1{:}n})&lt;T(x_{1{:}n})),\\ &amp; T(x_{1{:}n})&lt;\\lambda^*\\\\ 2P_{\\theta_0}(T(X_{1{:}n})&gt;T(x_{1{:}n})),\\ &amp; T(x_{1{:}n})\\ge \\lambda^*. \\end{cases} \\] 例 3.14 总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其中\\(\\mu\\)未知，方差\\(\\sigma^2\\)已知，考虑检验 \\[H_0:\\mu\\le\\mu_0\\ vs.\\ H_1:\\mu&gt;\\mu_0.\\] 检验统计量为\\(U = \\sqrt{n}(\\bar X-\\mu_0)/\\sigma\\). 拒绝域为\\(W=\\{u&gt;c\\}\\). 故p值为 \\[p = \\sup_{\\mu\\le\\mu_0}P_{\\mu}(U&gt; u)=P_{\\mu_0}(U&gt; u)=1-\\Phi\\left(\\frac{\\bar x-\\mu_0}{\\sigma/\\sqrt{n}}\\right).\\] 例 3.15 总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其中\\(\\mu\\)未知，方差\\(\\sigma^2\\)已知，考虑检验 \\[H_0:\\mu=\\mu_0\\ vs.\\ H_1:\\mu\\neq\\mu_0.\\] 拒绝域为\\(W=\\{x_{1{:}n}:\\sqrt{n}|\\bar x-\\mu_0|/\\sigma&gt;u_{1-\\alpha/2}\\}\\), 其中检验统计量为\\(T = \\sqrt{n}|\\bar X-\\mu_0|/\\sigma\\). 故p值为 \\[p = P_{\\mu_0}(T(X_{1{:}n})&gt; T(x_{1{:}n}))=2-2\\Phi(T(x_{1{:}n}))=2-2\\Phi\\left(\\frac{|\\bar x-\\mu_0|}{\\sigma/\\sqrt{n}}\\right).\\] 对应到例3.11， \\[p = 2-2\\Phi\\left(\\frac{|12.757-12.6|}{0.4/\\sqrt{30}}\\right)=0.032.\\] 因此, 如果\\(\\alpha&gt; 0.032\\), 拒绝\\(H_0\\)（建议校正）；否则, 接受\\(H_0\\)（不建议校正）。 例 3.16 总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其中\\(\\mu\\)已知，考虑检验 \\[H_0:\\sigma^2=\\sigma^2_0\\ vs.\\ H_1:\\sigma^2\\neq\\sigma^2_0.\\] 检验统计量为\\(V = \\frac{1}{\\sigma^2_0}\\sum_{i=1}^n(X_i-\\mu)^2\\stackrel{H_0}{\\sim}\\chi^2(n).\\) 拒绝域为\\(W=\\{v&lt;c_1\\}\\cup\\{v&gt;c_2\\}\\)，不难发现\\(c_1\\le \\chi^2_{0.5}(n)\\le c_2\\). 故p值为 \\[p = \\begin{cases} 2F(v),\\ &amp; v&lt;\\chi^2_{0.5}(n)\\\\ 2(1-F(v)),\\ &amp; v\\ge \\chi^2_{0.5}(n), \\end{cases} \\] 其中\\(v\\)为\\(V\\)的观测值，\\(F(x)\\)为\\(\\chi^2(n)\\)的CDF. 注： p值可以看作样本与原假设相容程度的度量。p值越大相容度越高；反之，p值越小相容度越低。当p值小于\\(\\alpha\\)时认为两者不相容，拒绝原假设。 做检验时不需要事先确定显著性水平\\(\\alpha\\)（它具有一定的主观性），如果p值非常小，则毫不犹豫地拒绝原假设；同样地，如果p值比较大，则接受原假设，这样就不用争论\\(\\alpha=0.1,0.05\\)或者其他。 p值提供更多的信息，可以用于保护隐私数据。 统计软件提供的是p值。 3.8 多重检验 如果独立检验同一个假设\\(k\\)次，我们可以得到\\(k\\)个p值: \\(p_1,\\dots,p_k\\), 可否由这\\(k\\)个p值汇总成一个p值来检验该假设？元分析(meta-analysis) 假如我们有\\(k\\)个不同的原假设\\(H_{0j},j=1,\\dots,k\\)，这种问题称为多重假设(multiple tests)问题。可否利用\\(k\\)个不同假设的p值: \\(p_1,\\dots,p_k\\)来进一步控制错误发生率? 例子：吃果冻与长青春痘的联系：https://xkcd.com/882/ How to annoy a statistician: https://xkcd.com/2118/ 3.9 伯努利分布的检验 设\\(X\\)服从两点分布\\(B(1,p)\\), 下面考虑以下三种常见的假设检验 \\(H_0:p\\le p_0\\ vs.\\ H_1:p&gt;p_0\\) \\(H_0:p\\ge p_0\\ vs.\\ H_1:p&lt;p_0\\) \\(H_0:p= p_0\\ vs.\\ H_1:p\\neq p_0\\) 对于该总体，我们选\\(S=\\sum_{i=1}^nX_i\\sim B(n,p)\\)为检验统计量。相应的拒绝域形式为 \\(W=\\{s\\ge c\\}\\) \\(W=\\{s\\le c\\}\\) \\(W=\\{s\\ge c_2\\}\\cup\\{s\\le c_1\\}\\) 注意到\\(S\\)为离散型随机变量，所以满足\\(\\sup_{p\\in\\Theta_0}P_p(X_{1{:}n}\\in W)=\\alpha\\)的分界点不一定存在。因此，我们考虑\\(\\sup_{p\\in\\Theta_0}P_p(X_{1{:}n}\\in W)\\le \\alpha\\)下分界点的选取。 3.9.1 单侧检验I 考虑单边假设\\(H_0:p\\le p_0\\ vs.\\ H_1:p&gt;p_0\\), 为了使得检验功效最大化，临界值\\(c\\)为满足下式最小的整数 \\[\\sup_{p\\le p_0}P_p(S\\ge c)\\le \\alpha.\\] 引理 3.1 设\\(F_{\\beta}(x;a,b)\\)为\\(Beta(a,b)\\)分布的累积分布函数，则 \\[P_p(S\\ge c)=\\sum_{i=c}^nC_n^ip^i(1-p)^{n-i}=F_{\\beta}(p;c,n-c+1),\\] 其中\\(F_{\\beta}(p;c,n-c+1)\\)表示\\(Beta(c,n-c+1)\\)分布的CDF在\\(p\\)点处的取值。 由于\\(P_p(S\\ge c)\\)关于\\(p\\)单调递增，所以 \\[\\sup_{p\\le p_0}P_p(S\\ge c)=P_{p_0}(S\\ge c)=\\sum_{i=c}^nC_n^ip_0^i(1-p_0)^{n-i}\\le \\alpha.\\] 计算\\(c\\)比较复杂，为了避免此，我们将拒绝域\\(\\{s\\ge c\\}\\)等价转化为 \\[W=\\{\\sum_{i=s}^nC_n^ip_0^i(1-p_0)^{n-i}\\le \\alpha\\}.\\] 更进一步，假设\\(p_\\alpha(s)\\)为方程\\(\\sum_{i=s}^nC_n^ip^i(1-p)^{n-i}=\\alpha\\)的根，则拒绝域等价转化为 \\[W=\\{p_0\\le p_\\alpha(s)\\},\\] 其中\\(p_\\alpha(s)=F_{\\beta}^{-1}(\\alpha;s,n-s+1)\\), 或者可以表示成\\(F\\)分布分位数的函数 \\[p_\\alpha(s)=\\left(1+\\frac{n-s+1}{s}F_{1-\\alpha}(2(n-s+1),2s)\\right)^{-1}.\\] 详细的转化见陈家鼎等编著的教材P105引理4.2. 3.9.2 女士品茶问题求解 考虑女士品茶问题，设该女士鉴别的成功率为\\(p\\). 设\\(X_i\\)表示第\\(i\\)次鉴别结果，即\\(X_i=1\\)表示成功，\\(X_i=0\\)表示失败。如果\\(p&gt;p_0\\)我们认为该女士具备这种辨别能力，其中\\(p_0\\ge 1/2\\)为给定的数。故考虑检验 \\[H_0: p\\le p_0\\ vs.\\ H_1:p&gt;p_0.\\] 二项分布检验的R代码 binom.test(x, n, p = 0.5, alternative = c(“two.sided”, “less”, “greater”), conf.level = 0.95) n=10的检验结果： alpha = 0.1 n = 10 s = 1:n pr = qbeta(alpha,s,n-s+1) par(mfrow = c(1,2),mar=c(4,4,2,0.5)) plot(s,pr,type=&quot;b&quot;,ylab=expression(p[alpha](s)), main=expression(alpha==0.1)) abline(h=0.5,col=&quot;red&quot;) lb = expression(p[0]==0.5) text(3,0.55,lb) alpha = 0.05 pr = qbeta(alpha,s,n-s+1) plot(s,pr,type=&quot;b&quot;,ylab=expression(p[alpha](s)), main=expression(alpha==0.05)) abline(h=0.5,col=&quot;red&quot;) text(3,0.55,lb) binom.test(8,10,0.5,alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 8 and 10 ## number of successes = 8, number of trials = 10, ## p-value = 0.05 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.4931 1.0000 ## sample estimates: ## probability of success ## 0.8 3.9.3 单侧检验II 考虑单边假设\\(H_0:p\\ge p_0\\ vs.\\ H_1:p&lt;p_0\\), 临界值\\(c\\)为满足下式最大的整数 \\[\\sup_{p\\ge p_0}P_p(S\\le c)\\le \\alpha.\\] 由于\\(P_p(S\\le c)=\\sum_{i=0}^cC_n^ip^i(1-p)^{n-i}\\)关于\\(p\\)单调递减，所以只需考虑 \\[P_{p_0}(S\\le c)=\\sum_{i=0}^cC_n^ip_0^i(1-p_0)^{n-i}\\le \\alpha.\\] 计算\\(c\\)比较复杂，为了避免此，我们将拒绝域\\(\\{s\\ge c\\}\\)等价转化为 \\[W=\\{\\sum_{i=s+1}^nC_n^ip_0^i(1-p_0)^{n-i}\\ge 1-\\alpha\\}.\\] 更进一步，假设\\(\\tilde p_\\alpha(s)\\)为方程\\(\\sum_{i=s+1}^nC_n^ip^i(1-p)^{n-i}=1-\\alpha\\)的根，则拒绝域等价转化为 \\[W=\\{p_0\\ge \\tilde p_\\alpha(s)\\}\\] 其中\\(\\tilde p_\\alpha(s)=F_{\\beta}^{-1}(1-\\alpha;s+1,n-s)\\), 或者可以表示成 \\[\\tilde p_\\alpha(s)=\\left(1+\\frac{n-s}{(s+1)F_{1-\\alpha}(2s+2,2n-2s)}\\right)^{-1}\\] 3.9.4 双侧检验 考虑双边假设\\[H_0:p= p_0\\ vs.\\ H_1:p\\neq p_0\\] 拒绝域为\\(\\{s\\le c_1\\}\\cup\\{s\\ge c_2\\}\\)，其中临界值\\(c_1\\)为满足\\(P_{p_0}(S\\le c_1)=\\alpha/2\\)最大的整数，临界值\\(c_1\\)为满足\\(P_{p_0}(S\\ge c_2)=\\alpha/2\\)最小的整数。由前面分析，该拒绝域等价于 \\[\\{p_0\\le p_{\\alpha/2}(s)\\}\\cup\\{p_0\\ge \\tilde{p}_{\\alpha/2}(s)\\}.\\] 3.10 拟合优度检验 3.10.1 Mendel的数据 In one of his famous experiments, Mendel crossed 556 smooth, yellow male peas with wrinkled, green female peas. The counts that Mendel recorded are smooth yellow smooth green wrinkled yellow wrinkled green 315 108 102 31 According to now established genetic theory, the relative frequencies of the progeny should be as given below. \\[P(\\text{smooth yellow}) = 9/16, P(\\text{smooth green}) = 3/16\\] \\[P(\\text{wrinkled yellow}) = 3/16, P(\\text{wrinkled green}) = 1/16\\] Would you conclude that Mendal’s experiment is correct at the level of significance \\(\\alpha=0.05\\)? 3.10.2 卡方检验 考虑离散型分布的假设检验，\\(X\\in \\{t_1,\\dots,t_m\\}\\). \\[H_0: P(X=t_i)=p^0_i,\\ i=1,\\dots,m,\\ vs.\\ H_1: P(X=t_i)\\neq p^0_i,\\] 其中\\(p^0_i\\in(0,1)\\)为给定的数且\\(\\sum_{i=1}^m p^0_i=1\\). 皮尔逊卡方检验法(Pearson, 1900)选择检验统计量： \\[V=\\sum_{i=1}^{m} \\frac{(v_i-np^0_i)^2}{np^0_i},\\] 其中\\(v_i\\)表示\\(x_{1{:}n}\\)中包含\\(t_i\\)的个数，即\\(v_i=\\sum_{j=1}^n 1\\{X_j=t_i\\}\\). 注意到，\\(v_i\\)是\\(np_i\\)的无偏估计。如果\\(H_0\\)成立，求和里面的\\(\\frac{(v_i-np^0_i)^2}{np^0_i}\\)可以看作\\(v_i\\)的相对平方误差。 拒绝域\\(W=\\{V&gt;\\lambda\\}\\). 可以证明在\\(H_0\\)下，\\(V\\stackrel{\\cdot}{\\sim} \\chi^2(m-1)\\)；参见专著E. L. Lehmann and J. R. Romano. Testing Statistical Hypothesis (3rd Edition). P591页定理14.3.1. 故取\\(\\lambda=\\chi^2_{1-\\alpha}(m-1)\\). 对Mendel的数据，我们有 卡方检验统计量\\(V=0.604\\), \\(\\chi^2_{1-0.05}(3)=7.81\\), 所以接受原假设。p值为\\(0.90\\)。 R代码关键命令为： chisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000) x = c(315, 108, 102, 31) p = c(9/16,3/16,3/16,1/16) chisq.test(x,p=p) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 0.6, df = 3, p-value = 0.9 注：上述检验同样可以通过广义似然比检验得到。令\\(V_i\\)为\\(X_1,\\dots,X_n\\)中包含\\(t_i\\)的个数，\\(v_i\\)为其观测值，\\(i=1,\\dots,m\\). 注意到，\\((V_1,\\dots,V_m)\\)服从多项式分布，数据\\((v_1,\\dots,v_m)\\)的似然函数为 \\[L(p_1,\\dots,p_m) = \\frac{n!}{\\prod_{i=1}^m v_i!}\\prod_{i=1}^m p_i^{v_i}.\\] 其中，参数空间为\\(\\Theta=\\{(p_1,\\dots,p_m)\\in(0,1)^m|\\sum_{i=1}^mp_i=1\\}\\). 由拉格朗日乘子法，不难发现\\(p_1,\\dots,p_m\\)的MLE为\\(\\hat p_i=v_i/n\\). 广义似然比为 \\[\\begin{align*} \\lambda(v_1,\\dots,v_m)&amp;=\\frac{\\sup_{(p_1,\\dots,p_m)\\in\\Theta}L(p_1,\\dots,p_m)}{L(p_1^0,\\dots,p_m^0)}\\\\ &amp;=\\frac{L(\\hat p_1,\\dots,\\hat p_m)}{L(p_1^0,\\dots,p_m^0)}\\\\ &amp;=\\prod_{i=1}^m(\\hat p_i/p_i^0)^{v_i}=\\prod_{i=1}^m\\left(\\frac{v_i}{n p_i^0}\\right)^{v_i}. \\end{align*}\\] 于是， \\[2\\log \\lambda(v_1,\\dots,v_m) = 2n\\sum_{i=1}^m \\frac{v_i}{n} [\\log\\left (\\frac{v_i}{n }\\right)-\\log(p_i^0)].\\] 如果\\(H_0\\)成立，当\\(n\\)充分大时，\\(v_i/n\\approx p_i^0\\). 对函数\\(f(x) = x[\\log(x)-\\log (x_0)]\\)在\\(x=x_0\\)处进行二阶泰勒展开，有 \\[f(x)\\approx (x-x_0)+\\frac{(x-x_0)^2}{2x_0}.\\] 所以， \\[2\\log \\lambda(v_1,\\dots,v_m)\\approx 2n\\sum_{i=1}^m \\left(\\frac{v_i}{n }-p_i^0\\right)+n\\sum_{i=1}^m\\left(\\frac{v_i}{n }-p_i^0\\right)^2/p_i^0=\\sum_{i=1}^m\\frac{\\left(v_i-np_i^0\\right)^2}{np_i^0}=V,\\] 其中用到\\(\\sum_{i=1}^m v_i/n=1,\\ \\sum_{i=1}^m p_i^0=1\\). 这表明，广义似然比检验的拒绝域与卡方检验法的拒绝域接近，但是卡方检验法的统计量较为简单。 推广：考虑连续型分布 \\[H_0:F(x)=F_0(x)\\ vs.\\ H_1:\\ F(x)\\neq F_0(x),\\] 其中\\(F_0\\)为给定的分布，比如\\(N(0,1)\\). 为了使用卡方检验法，我们需要把连续分布离散化得到一个离散分布进行检验。 把整个实轴分成\\(m\\)份，\\((-\\infty,t_1],\\ (t_1,t_2],\\dots,(t_{m-2},t_{m-1}],\\ (t_{m-1},\\infty)\\), 分别计算这\\(m\\)个区间的概率\\(p_i,i=1,\\dots,m\\), \\(v_i\\)表示\\(x_{1{:}n}\\)落到第\\(i\\)个区间的个数， 类似离散的分布的检验。 借鉴直方图法的选取\\(t_{i}\\)和\\(m\\). 3.11 小结 其他检验： 独立性检验 正态性检验 柯尔莫哥洛夫检验法 3.12 本章习题 习题 3.1 Let \\(X_1,\\dots,X_{100}\\) be a sample from \\(N(\\mu,1)\\). Given a significance level \\(\\alpha=0.05\\), derive a UMP rejection region \\(W\\) of \\[H_0:\\mu=0\\ vs.\\ H_1:\\mu&gt;0.\\] Let \\(W&#39;=\\{x_{1{:}n}:|\\bar x| &gt; u_{0.975}/10\\}\\) be another rejection region. Show that the significance level for \\(W&#39;\\) is \\(0.05\\), and graph the power functions for \\(W\\) and \\(W&#39;\\). Try to explain that you observed. 习题 3.2 Let \\(X_{1{:}n}\\) be a sample from an exponential distribution \\(Exp(\\lambda)\\). Given a significance level \\(\\alpha\\), derive a likelihood ratio test of \\[H_0:\\lambda=\\lambda_1\\ vs.\\ H_1:\\lambda=\\lambda_2,\\] where \\(\\lambda_1\\neq\\lambda_2\\). 习题 3.3 Let \\(X_{1{:}n}\\) be a sample from an exponential distribution \\(Exp(\\lambda)\\). Given a significance level \\(\\alpha\\), derive a UMPU test of \\[H_0:\\lambda=\\lambda_0\\ vs.\\ H_1:\\lambda\\neq\\lambda_0.\\] 习题 3.4 Let \\(X_{1{:}n}\\) be a sample from \\(U[0,\\theta]\\). Given a significance level \\(\\alpha\\), derive a UMP test of \\[H_0:\\theta=\\theta_0\\ vs.\\ H_1:\\theta&gt;\\theta_0.\\] 习题 3.5 Let \\(X_1,X_2,X_3,X_4\\) be a sample from \\(N(\\theta,1)\\). Given a significance level \\(\\alpha=0.1\\), derive a UMP test of \\[H_0:\\theta\\ge 10\\ vs.\\ H_1:\\theta&lt;10.\\] Calculate the power of the test when \\(\\theta=9\\). 习题 3.6 True or false, and state why: The generalized likelihood ratio statistic \\(\\lambda(x_{1{:}n})\\) (see P.87 of our textbook) is always greater than or equal to 1. If the p-value is 0.03, the corresponding test will reject at the significance level 0.02. If a test rejects at significance level 0.06, then the p-value is less than or equal to 0.06. The p-value of a test is the probability that the null hypothesis is correct. In testing a simple versus simple hypothesis via the likelihood ratio test, the p-value equals the inverse of the likelihood ratio. 习题 3.7 Case study 1: Mutual funds are investment vehicles consisting of a portfolio of various types of investments. If such an investment is to meet annual spending needs, the owner of shares in the fund is interested in the average of the annual returns of the fund. Investors are also concerned with the volatility of the annual returns, measured by the variance or standard deviation. One common method of evaluating a mutual fund is to compare it to a benchmark, the Lipper Average being one of these. This index number is the average of returns from a universe of mutual funds. The Global Rock Fund is a typical mutual fund, with heavy investments in international funds. It claimed to best the Lipper Average in terms of volatility over the period from 1989 through 2007. Its returns are given in the table below. Year Investment Return % Year Investment Return % 1989 15.32 1999 27.43 1990 1.62 2000 8.57 1991 28.43 2001 1.88 1992 11.91 2002 −7.96 1993 20.71 2003 35.98 1994 −2.15 2004 14.27 1995 23.29 2005 10.33 1996 15.96 2006 15.94 1997 11.12 2007 16.71 1998 0.37 The standard deviation for the Lipper Average is \\(11.67\\%\\). Let \\(\\sigma^2\\) denote the variance of the population represented by the return percentages shown in the table above. Consider the test \\[H_0: \\sigma^2=(11.67)^2\\ vs.\\ H_1:\\sigma^2&lt;(11.67)^2.\\] If the significance level \\(\\alpha=0.05\\), what’s your decision? Show up the p-value of your test. 习题 3.8 Case study 2: Forensic scientists sometimes have difficulty identifying the sex of a murder victim whose body is discovered badly decomposed. Often, dental structure can provide useful clues because female teeth and male teeth have different physical and chemical characteristics. The extent to which X-rays can penetrate tooth enamel, for instance, is not the same for the two sexes. Table below lists the enamel spectropenetration gradients for eight male teeth and eight female teeth. These measurements have all the characteristics of the two-sample format: the data are quantitative, the units are similar, two factor levels (male and female) are involved, and the observations are independent. Male Female 4.9 4.8 5.4 5.3 5.0 3.7 5.5 4.1 5.4 5.6 6.6 4.0 6.3 3.6 4.3 5.0 Assume that the enamel spectropenetration gradients for male teeth and female teeth are normally distributed. Based on the data above, conduct a test (the significance level \\(\\alpha=0.05\\)) to judge whether female teeth and male teeth have different physical and chemical characteristics. Assume that their variances are the same, what’s your decision? If you were not able to have the prior information that their variances are the same, what would you do? This is the case of Behrens-Fisher Problem. The data are paired. Is it possible to do a paired test, without judging whether their variances are the same? 习题 3.9 Case study 3: The National Center for Health Statistics (1970) gives the following data on distribution of suicides in the United States by month in 1970. Is there any evidence that the suicide rate varies seasonally, or are the data consistent with the hypothesis that the rate is constant (the significance level \\(\\alpha=0.05\\))? (Hint: Under the latter hypothesis, model the number of suicides in each month as a multinomial random variable with the appropriate probabilities and conduct a goodness-of-fit test.) Month Number of Suicides Days/Month Jan. 1867 31 Feb. 1789 28 Mar. 1944 31 Apr. 2094 30 May 2097 31 June 1981 30 July 1887 31 Aug. 2024 31 Sept. 1928 30 Oct. 2032 31 Nov. 1978 30 Dec. 1859 31 习题 3.10 Case study 4: Under (the assumption of) simple Mendelian inheritance, a cross between plants of two particular genotypes produces progeny 1/4 of which are “dwarf” and \\(3/4\\) of which are “giant”, respectively. In an experiment to determine if this assumption is reasonable, a cross results in progeny having 243 dwarf and 682 giant plants. If “giant” is taken as success, the null hypothesis is that \\(p =3/4\\) and the alternative that \\(p \\neq 3/4\\). Let \\(X_i,i=1,\\dots,n\\) be the sample of the population \\(B(1,p)\\). By central limit theorem (CLT), the distribution of \\(\\bar X\\) can be approximated by a normal distribution \\(N(p,p(1-p)/n)\\). Please use this approximation to do the binominal test above. Actually, we can do the exact binominal test according to the formula given in P.114 of our textbook. Compare the results in the exact test and the approximate test for significance levels \\(\\alpha=0.05,0.01,0.001\\). "],
["regression.html", "第 4 章 线性回归 4.1 一元线性模型 4.2 多元线性模型 4.3 线性模型的推广 4.4 回归诊断 4.5 本章习题", " 第 4 章 线性回归 本章的R代码见：https://rstudio.cloud/project/798617 4.1 一元线性模型 The linear model is given by \\[y_i=\\beta_0+\\beta_1x_i+\\epsilon_i,\\ i=1,\\dots,n.\\] \\(\\epsilon_i\\) are random (need some assumptions) \\(x_i\\) are fixed (independent/predictor variable) \\(y_i\\) are random (dependent/response variable) \\(\\beta_0\\) is the intercept \\(\\beta_1\\) is the slope 4.1.1 最小二乘估计 Choose \\(\\beta_0,\\beta_1\\) to minimize \\[Q(\\beta_0,\\beta_1) = \\sum_{i=1}^n(y_i-\\beta_0-\\beta_1x_i)^2.\\] The minimizers \\(\\hat\\beta_0,\\hat\\beta_1\\) satisfy \\[ \\begin{cases} \\frac{\\partial Q}{\\partial \\beta_0} = -2\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0\\\\ \\frac{\\partial Q}{\\partial \\beta_1} = -2\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)x_i=0 \\end{cases} \\] This gives \\[\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(y_i-\\bar y)x_i}{\\sum_{i=1}^n(x_i-\\bar x)x_i},\\ \\hat\\beta_0=\\bar y-\\hat\\beta_1\\bar x.\\] Define \\[\\ell_{xx} = \\sum_{i=1}^n(x_i-\\bar x)^2,\\] \\[\\ell_{yy} = \\sum_{i=1}^n(y_i-\\bar y)^2,\\] \\[\\ell_{xy} = \\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y).\\] We thus have \\[\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(y_i-\\bar y)(x_i-\\bar x)}{\\sum_{i=1}^n(x_i-\\bar x)(x_i-\\bar x)}=\\frac{\\ell_{xy}}{\\ell_{xx}}=\\frac{1}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)y_i.\\] Regression function: \\(\\hat y=\\hat\\beta_0+\\hat\\beta_1x\\). 4.1.2 期望和方差 Assumption A1: \\(E[\\epsilon_i]=0,i=1,\\dots,n\\). 定理 4.1 Under Assumption A1, \\(\\hat\\beta_0,\\hat\\beta_1\\) are unbiased estimators for \\(\\beta_0,\\beta_1\\), respectively. 证明. \\[\\begin{align*} E[\\hat \\beta_1] &amp;= \\frac{1}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)E[y_i]\\\\ &amp;=\\frac{1}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)(\\beta_0+\\beta_1x_i)\\\\ &amp;=\\frac{\\beta_0}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)+\\frac{\\beta_1}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)x_i\\\\ &amp;=\\frac{\\beta_1}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)(x_i-\\bar x)\\\\ &amp;=\\beta_1 \\end{align*}\\] \\[\\begin{align*} E[\\hat \\beta_0] &amp;= E[\\bar y-\\hat\\beta_1\\bar x]=E[\\bar y]-\\beta_1\\bar x=\\beta_0+\\beta_1\\bar x-\\beta_1\\bar x=\\beta_0. \\end{align*}\\] Assumption A2: \\(Cov(\\epsilon_i,\\epsilon_j)=\\sigma^21\\{i=j\\}\\). 定理 4.2 Under Assumption A2, we have \\[Var[\\hat\\beta_0] = \\left(\\frac 1n+\\frac{\\bar x^2}{\\ell_{xx}}\\right)\\sigma^2,\\] \\[Var[\\hat\\beta_1] =\\frac{\\sigma^2}{\\ell_{xx}},\\] \\[Cov(\\hat\\beta_0,\\hat\\beta_1) = \\frac{-\\bar x}{\\ell_{xx}}\\sigma^2.\\] 证明. Since \\(Cov(\\epsilon_i,\\epsilon_j)=0\\) for any \\(i\\neq j\\), \\(Cov(y_i,y_j)=0\\). We thus have \\[\\begin{align*} Var[\\hat\\beta_1] &amp;= \\frac{1}{\\ell_{xx}^2}\\sum_{i=1}^n(x_i-\\bar x)^2Var[y_i]\\\\ &amp;= \\frac{\\sigma^2}{\\ell_{xx}^2}\\sum_{i=1}^n(x_i-\\bar x)^2=\\frac{\\sigma^2}{\\ell_{xx}}. \\end{align*}\\] We next show that \\(Cov(\\bar y,\\hat \\beta_1)=0\\). \\[\\begin{align*} Cov(\\bar y,\\hat \\beta_1) &amp;= Cov\\left(\\frac{1}{n}\\sum_{i=1}^n y_i,\\frac{1}{\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)y_i\\right)\\\\ &amp;=\\frac{1}{n\\ell_{xx}}Cov\\left(\\sum_{i=1}^n y_i,\\sum_{i=1}^n(x_i-\\bar x)y_i\\right)\\\\ &amp;=\\frac{1}{n\\ell_{xx}}\\sum_{i=1}^nCov(y_i,(x_i-\\bar x)y_i)\\\\ &amp;=\\frac{1}{n\\ell_{xx}}\\sum_{i=1}^n(x_i-\\bar x)\\sigma^2 = 0. \\end{align*}\\] \\[Var[\\hat\\beta_0] = Var[\\bar y-\\hat\\beta_1\\bar x]=Var[\\bar y]+Var[\\hat \\beta_1\\bar x]=\\frac{\\sigma^2}{n}+\\frac{\\bar x^2\\sigma^2}{\\ell_{xx}}.\\] \\[Cov(\\hat\\beta_0,\\hat\\beta_1) = Cov(\\bar y-\\hat\\beta_1\\bar x,\\hat\\beta_1) = -\\bar x Var[\\hat\\beta_1]=\\frac{-\\bar x}{\\ell_{xx}}\\sigma^2.\\] So bigger \\(n\\) is better. Get a bigger sample size if you can. Smaller \\(\\sigma\\) is better. The most interesting one is that bigger \\(\\ell_{xx}\\) is better. The more spread out the \\(x_i\\) are the better we can estimate the slope \\(\\beta_1\\). When you’re picking the \\(x_i\\), if you can spread them out more, then it is more informative. 4.1.3 误差项的方差的估计 For Assumption A2, it is common that the variance \\(\\sigma^2\\) is unknown. The next theorem gives an unbiased estimate of \\(\\sigma^2\\). 定义 4.1 The sum of squared errors (SSE) is defined by \\[S_e^2 = \\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)^2.\\] 定理 4.3 Let \\[\\hat{\\sigma}^2 := \\frac{Q(\\hat \\beta_0,\\hat\\beta_1)}{n-2}=\\frac{\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)^2}{n-2}=\\frac{S_e^2}{n-2}.\\] Under Assumptions A1 and A2, we have \\(E[\\hat\\sigma^2]=\\sigma^2\\). 证明. Let \\(\\hat y_i = \\hat\\beta_0+\\hat\\beta_1x_i=\\bar y+\\hat\\beta_1(x_i-\\bar x)\\). \\[\\begin{align*} E[Q(\\hat \\beta_0,\\hat\\beta_1)] &amp;= \\sum_{i=1}^nE[(y_i-\\hat y_i)^2]=\\sum_{i=1}^nVar[y_i-\\hat y_i]+(E[y_i]-E[\\hat y_i])^2\\\\ &amp;=\\sum_{i=1}^n[Var[y_i]+Var[\\hat y_i]-2Cov(y_i,\\hat y_i)]. \\end{align*}\\] \\[\\begin{align*} Var[\\hat y_i]&amp;= Var[\\hat\\beta_0+\\hat\\beta_1x_i]=Var[\\bar y+\\hat\\beta_1(x_i-\\bar x)]\\\\ &amp;=Var[\\bar y]+(x_i-\\bar x)^2Var[\\hat\\beta_1]\\\\ &amp;=\\frac{\\sigma^2}{n}+\\frac{(x_i-\\bar x)^2\\sigma^2}{\\ell_{xx}}. \\end{align*}\\] \\[\\begin{align*} Cov(y_i,\\hat y_i) &amp;= Cov(\\beta_0+\\beta_1x_i+\\epsilon_i, \\bar y+\\hat\\beta_1(x_i-\\bar x))\\\\ &amp;=Cov(\\epsilon_i,\\bar y)+(x_i-\\bar x)Cov(\\epsilon_i,\\hat\\beta_1)\\\\ &amp;=\\frac{\\sigma^2}{n}+\\frac{(x_i-\\bar x)^2\\sigma^2}{\\ell_{xx}}. \\end{align*}\\] As a result, we have \\[E[Q(\\hat \\beta_0,\\hat\\beta_1)] = \\sum_{i=1}^n \\left[\\sigma^2-\\frac{\\sigma^2}{n}-\\frac{(x_i-\\bar x)^2\\sigma^2}{\\ell_{xx}}\\right]=(n-2)\\sigma^2.\\] 4.1.4 抽样分布定理 Assumption B: \\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2),i=1,\\dots,n\\). Assumption B leads to Assumptions A1 and A2. 定理 4.4 Under Assumption B, we have \\(\\hat\\beta_0\\sim N(\\beta_0,(\\frac 1n+\\frac{\\bar x^2}{\\ell_{xx}})\\sigma^2)\\) \\(\\hat\\beta_1\\sim N(\\beta_1,\\frac{\\sigma^2}{\\ell_{xx}})\\) \\(\\frac{(n-2)\\hat\\sigma^2}{\\sigma^2}=\\frac{S_e^2}{\\sigma^2}\\sim \\chi^2(n-2)\\) \\(\\hat\\sigma^2\\) is independent of \\((\\hat\\beta_0,\\hat\\beta_1)\\). 证明. Under Assumption B, \\(y_i=\\beta_0+\\beta_1x_i+\\epsilon_i\\sim N(\\beta_0+\\beta_1x_i,\\sigma^2)\\) independently. Both \\(\\hat\\beta_0,\\hat\\beta_1\\) are linear combinations of \\(y_i\\)s. Consequently, they are normally distributed. We have known their expected values and variances from Theorems 4.1 and 4.2. The claims (1) and (2) are thus verified. The proofs of claims (3) and (4) are deferred to the general case. It is \\(n-2\\) degrees of freedom because we have fit two parameters to the \\(n\\) data points. 4.1.5 置信区间与假设检验 For known \\(\\sigma\\) we can make tests and confidence intervals using \\[\\frac{\\hat\\beta_1-\\beta_1}{\\sigma/\\sqrt{\\ell_{xx}}}\\sim N(0,1).\\] The \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\beta_1\\) is given by \\(\\hat\\beta_1\\pm u_{1-\\alpha/2}\\sigma/\\sqrt{\\ell_{xx}}\\). For testing \\[H_0:\\beta_1=\\beta_1^*\\ vs.\\ H_1:\\beta_1\\neq\\beta_1^*,\\] we reject \\(H_0\\) if \\(|\\hat\\beta_1-\\beta_1^*|&gt;u_{1-\\alpha/2}\\sigma/\\sqrt{\\ell_{xx}}\\) with the most popular hypothesized value being \\(\\beta_1^*=0\\) (i.e., the regession function is significant or not at significance level \\(\\alpha\\).) In the more realistic setting of unknown \\(\\sigma\\), so long as \\(n \\ge 3\\), using claims (2-4) gives \\[\\frac{\\hat\\beta_1-\\beta_1}{\\hat{\\sigma}/\\sqrt{\\ell_{xx}}}\\sim t(n-2).\\] The \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\beta_1\\) is \\(\\hat\\beta_1\\pm t_{1-\\alpha/2}(n-2)\\hat{\\sigma}/\\sqrt{\\ell_{xx}}\\). For testing \\[H_0:\\beta_1=\\beta_1^*\\ vs.\\ H_1:\\beta_1\\neq\\beta_1^*,\\] we reject \\(H_0\\) if \\(|\\hat\\beta_1-\\beta_1^*|&gt;t_{1-\\alpha/2}(n-2)\\hat\\sigma/\\sqrt{\\ell_{xx}}\\). For drawing inferences about \\(\\beta_0\\), we can use \\[\\frac{\\hat\\beta_0-\\beta_0}{\\sigma\\sqrt{1/n+\\bar x^2/\\ell_{xx}}}\\sim N(0,1),\\] \\[\\frac{\\hat\\beta_0-\\beta_0}{\\hat\\sigma\\sqrt{1/n+\\bar x^2/\\ell_{xx}}}\\sim t(n-2).\\] The \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\left[\\frac{(n-2)\\hat\\sigma^2}{\\chi_{1-\\alpha/2}^2(n-2)},\\frac{(n-2)\\hat\\sigma^2}{\\chi_{\\alpha/2}^2(n-2)}\\right]=\\left[\\frac{S_e^2}{\\chi_{1-\\alpha/2}^2(n-2)},\\frac{S_e^2}{\\chi_{\\alpha/2}^2(n-2)}\\right].\\] 4.1.6 案例分析1 A manufacturer of air conditioning units is having assembly problems due to the failure of a connecting rod (连接杆) to meet finished-weight specifications. Too many rods are being completely tooled, then rejected as overweight. To reduce that cost, the company’s quality-control department wants to quantify the relationship between the weight of the finished rod, \\(y\\), and that of the rough casting (毛坯铸件), \\(x\\). Castings likely to produce rods that are too heavy can then be discarded before undergoing the final (and costly) tooling process. The data are displayed below. rod = data.frame( id = seq(1:25), rough_weight = c(2.745, 2.700, 2.690, 2.680, 2.675, 2.670, 2.665, 2.660, 2.655, 2.655, 2.650, 2.650, 2.645, 2.635, 2.630, 2.625, 2.625, 2.620, 2.615, 2.615, 2.615, 2.610, 2.590, 2.590, 2.565), finished_weight = c(2.080, 2.045, 2.050, 2.005, 2.035, 2.035, 2.020, 2.005, 2.010, 2.000, 2.000, 2.005, 2.015, 1.990, 1.990, 1.995, 1.985, 1.970, 1.985, 1.990, 1.995, 1.990, 1.975, 1.995, 1.955) ) knitr::kable(rod, caption = &quot;rough weight vs. finished weight&quot;) 表 4.1: rough weight vs. finished weight id rough_weight finished_weight 1 2.745 2.080 2 2.700 2.045 3 2.690 2.050 4 2.680 2.005 5 2.675 2.035 6 2.670 2.035 7 2.665 2.020 8 2.660 2.005 9 2.655 2.010 10 2.655 2.000 11 2.650 2.000 12 2.650 2.005 13 2.645 2.015 14 2.635 1.990 15 2.630 1.990 16 2.625 1.995 17 2.625 1.985 18 2.620 1.970 19 2.615 1.985 20 2.615 1.990 21 2.615 1.995 22 2.610 1.990 23 2.590 1.975 24 2.590 1.995 25 2.565 1.955 Consider the linear model \\[y_i=\\beta_0+\\beta_1x_i+\\epsilon_i,\\ \\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2).\\] The observed data gives \\(\\bar x = 2.643\\), \\(\\bar y=2.0048\\), \\(\\ell_{xx}=0.0367\\), \\(\\ell_{xy}=0.023565\\), \\(\\hat\\sigma = 0.0113\\). The least square estimates are \\[\\hat\\beta_1=\\frac{\\ell_{xy}}{\\ell_{xx}}=\\frac{0.023565}{0.0367}=0.642,\\ \\hat\\beta_0=\\bar y-\\hat\\beta_1\\bar x=0.308.\\] The regession function \\(\\hat y = 0.308+0.642 x\\); see the blue line given below. attach(rod) par(mar=c(4,4,1,0.5)) plot(rough_weight,finished_weight,type=&quot;p&quot;,pch=16, xlab = &quot;Rough Weight&quot;,ylab = &quot;Finished Weight&quot;) lm.rod = lm(finished_weight~rough_weight) abline(coef(lm.rod),col=&quot;blue&quot;) summary(lm.rod) #output the results ## ## Call: ## lm(formula = finished_weight ~ rough_weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.02356 -0.00824 0.00107 0.00818 0.02423 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.308 0.156 1.97 0.061 . ## rough_weight 0.642 0.059 10.87 1.5e-10 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0113 on 23 degrees of freedom ## Multiple R-squared: 0.837, Adjusted R-squared: 0.83 ## F-statistic: 118 on 1 and 23 DF, p-value: 1.54e-10 4.1.7 拟合的评估 As an aid in assessing the quality of the fit, we will make extensive use of the residuals, which are the differences between the observed and fitted values: \\[\\hat \\epsilon_i = y_i-\\hat\\beta_0-\\hat\\beta_1x_i,\\ i=1,\\dots,n.\\] It is most useful to examine the residuals graphically. Plots of the residuals versus the \\(x\\) values may reveal systematic misfit or ways in which the data do not conform to the fitted model. Ideally, the residuals should show no relation to the \\(x\\) values, and the plot should look like a horizontal blur. The residuals for case study 1 are plotted below. par(mar=c(4,4,2,1)) plot(lm.rod$fitted.values,lm.rod$residuals,&quot;p&quot;, xlab=&quot;Fitted values&quot;,ylab = &quot;Residuals&quot;) Standardized Residuals are graphed below. The key command is rstandard. par(mar=c(4,4,2,1)) plot(lm.rod$fitted.values,rstandard(lm.rod),&quot;p&quot;, xlab=&quot;Fitted values&quot;,ylab = &quot;Standardized Residuals&quot;) abline(h=c(-2,2),lty=c(5,5)) 4.1.8 预测 Drawing Inferences about \\(E[y_{n+1}]\\) For a given \\(x_{n+1}\\), we want to estimate the expected value of \\(y_{n+1}\\), i.e., \\(E[y_{n+1}]=\\beta_0+\\beta_1x_{n+1}.\\) A natural unbiased estimate is \\(\\hat y_{n+1} = \\hat\\beta_0+\\hat\\beta_1x_{n+1}\\). From the proof of Theorem 4.3, we have the variance \\[Var[\\hat y_{n+1}] = \\left(\\frac{1}{n}+\\frac{(x_{n+1}-\\bar x)^2}{\\ell_{xx}}\\right)\\sigma^2.\\] Under Assumption B, by Theorem 4.4, we have \\[\\hat y_{n+1}\\sim N(\\beta_0+\\beta_1x_{n+1},(1/n+(x_{n+1}-\\bar x)^2/\\ell_{xx})\\sigma^2),\\] \\[\\frac{\\hat y_{n+1}-E[y_{n+1}]}{\\hat{\\sigma}\\sqrt{1/n+(x_{n+1}-\\bar x)^2/\\ell_{xx}}}\\sim t(n-2)\\] We thus have the following results. 定理 4.5 Suppose Assumption B is satisfied. Then we have \\[\\hat y_{n+1} = \\hat\\beta_0+\\hat\\beta_1x_{n+1} \\sim N(\\beta_0+\\beta_1x_{n+1},[1/n+(x_{n+1}-\\bar x)^2/\\ell_{xx}]\\sigma^2).\\] A \\(100(1−\\alpha)\\%\\) confidence interval for \\(E[y_{n+1}]=\\beta_0+\\beta_1x_{n+1}\\) is given by \\[\\hat y_{n+1}\\pm t_{1-\\alpha/2}(n-2)\\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x_{n+1}-\\bar x)^2}{\\ell_{xx}}}.\\] Notice from the formula in Theorem 4.5 that the width of a confidence interval for \\(E[y_{n+1}]\\) increases as the value of \\(x_{n+1}\\) becomes more extreme. That is, we are better able to predict the location of the regression line for an \\(x_{n+1}\\)-value close to \\(\\bar x\\) than we are for \\(x_{n+1}\\)-values that are either very small or very large. For case study 1, we plot the lower and upper limits for the \\(95\\%\\) confidence interval for \\(E[y_{n+1}]\\). x = seq(2.5,2.8,by=0.001) newdata = data.frame(rough_weight= x) pred_x = predict(lm.rod,newdata,interval = &quot;confidence&quot;) par(mar=c(4,4,2,1)) matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2, xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;) abline(v=mean(rough_weight),lty=5) points(rough_weight,finished_weight,pch=16) legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;), lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;)) library(ggplot2) ggplot(rod,aes(x=rough_weight,y=finished_weight)) + geom_smooth(method = lm) + geom_point() 图 4.1: CI by ggplot Drawing Inferences about Future Observations We now give a prediction interval for the future observation \\(y_{n+1}\\) rather than its expected value \\(E[y_{n+1}]\\). Note that here \\(y_{n+1}\\) is no longer a fixed parameter, which is assumed to be independent of \\(y_i\\)’s. A prediction interval is a range of numbers that contains \\(y_{n+1}\\) with a specified probability. Consider \\(y_{n+1}-\\hat y_{n+1}\\). If Assumption A1 is satisfied, then \\[E[y_{n+1}-\\hat y_{n+1}] = E[y_{n+1}]-E[\\hat y_{n+1}]= 0.\\] If Assumption A2 is satisfied, then \\[Var[y_{n+1}-\\hat y_{n+1}] = Var[y_{n+1}]+Var[\\hat y_{n+1}]=\\left(1+\\frac{1}{n}+\\frac{(x_{n+1}-\\bar x)^2}{\\ell_{xx}}\\right)\\sigma^2.\\] If Assumption B is satisfied, \\(y_{n+1}-\\hat y_{n+1}\\) is then normally distributed. 定理 4.6 Suppose Assumption B is satisfied. Let \\(y_{n+1}=\\beta_0+\\beta_1x_{n+1}+\\epsilon\\), where \\(\\epsilon\\sim N(0,\\sigma^2)\\) is independent of \\(\\epsilon_i\\)’s. A \\(100(1−\\alpha)\\%\\) prediction interval for \\(y\\) is given by \\[\\hat y_{n+1}\\pm t_{1-\\alpha/2}(n-2)\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_{n+1}-\\bar x)^2}{\\ell_{xx}}}.\\] For case study 1, we plot the lower and upper limits for the \\(95\\%\\) prediction interval for \\(y_{n+1}\\). x = seq(2.5,2.8,by=0.001) newdata = data.frame(rough_weight= x) pred_x = predict(lm.rod,newdata,interval = &quot;prediction&quot;) par(mar=c(4,4,2,1)) matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2, xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;) abline(v=mean(rough_weight),lty=5) points(rough_weight,finished_weight,pch=16) legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;), lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;)) 4.1.9 控制 How to control \\(y_{n+1}\\)? Consider case study 1 again. Castings likely to produce rods that are too heavy can then be discarded before undergoing the final (and costly) tooling process. The company’s quality-control department wants to produce the rod \\(y_{n+1}\\) with weights no large than 2.05 with probablity no less than 0.95. How to choose the rough casting? Now we want \\(y_{n+1}\\le y_0=2.05\\) with probability \\(1-\\alpha\\). Similarly to Theorem 4.6, we can construct one-side confidence interval for \\(y_{n+1}\\), that is \\[\\bigg(-\\infty,\\hat y_{n+1}+t_{1-\\alpha}(n-2)\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_{n+1}-\\bar x)^2}{\\ell_{xx}}}\\bigg].\\] This implies \\[\\hat\\beta_0+\\hat\\beta_1x_{n+1}+t_{1-\\alpha}(n-2)\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_{n+1}-\\bar x)^2}{\\ell_{xx}}}\\le y_0.\\] 4.2 多元线性模型 With problems more complex than fitting a straight line, it is very useful to approach the linear least squares analysis via linear algebra. Consider a model of the form \\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\dots+\\beta_{p-1}x_{i,p-1}+\\epsilon_i,\\ i=1,\\dots,n.\\] Let \\(Y=(y_1,\\dots,y_n)^\\top\\), \\(\\beta=(\\beta_0,\\dots,\\beta_{p-1})^\\top\\), \\(\\epsilon=(\\epsilon_1,\\dots,\\epsilon_n)^\\top\\), and let \\(X\\) be the \\(n\\times p\\) matrix \\[ X= \\left[ \\begin{matrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1,p-1}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2,p-1}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{n,p-1}\\\\ \\end{matrix} \\right]. \\] The model can be rewritten as \\[Y=X\\beta+\\epsilon,\\] the matrix \\(X\\) is called the design matrix, assume that \\(p&lt;n\\). The least squares problem can then be phrased as follows: Find \\(\\beta\\) to minimize \\[Q(\\beta)=\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1x_{i1}-\\dots-\\beta_{p-1}x_{i,p-1})^2:=||Y-X\\beta||^2,\\] where \\(||\\cdot||\\) is the Euclidean norm. Note that \\[Q=(Y-X\\beta)^\\top(Y-X\\beta) = Y^\\top Y-2Y^\\top X\\beta+\\beta^\\top X^\\top X\\beta.\\] If we differentiate \\(Q\\) with respect to each \\(\\beta_i\\) and set the derivatives equal to zero, we see that the minimizers \\(\\hat\\beta_0,\\dots,\\hat\\beta_{p-1}\\) satisfy \\[\\frac{\\partial Q}{\\partial \\beta_i}=-2(Y^\\top X)_i+2(X^{\\top}X)_{i\\cdot}\\hat\\beta=0.\\] We thus arrive at the so-called normal equations: \\[X^\\top X\\hat\\beta = X^\\top Y.\\] If the design matrix \\(X^\\top X\\) is nonsingular, the formal solution is \\[\\hat\\beta = (X^\\top X)^{-1}X^\\top Y.\\] The following lemma gives a criterion for the existence and uniqueness of solutions of the normal equations. 引理 4.1 The matrix \\(X^\\top X\\) is nonsingular if and only if \\(\\mathrm{rank}(X)=p\\). 证明. First suppose that \\(X^\\top X\\) is singular. There exists a nonzero vector \\(u\\) such that \\(X^\\top Xu = 0\\). Multiplying the left-hand side of this equation by \\(u^\\top\\), we have \\(0=u^\\top X^\\top Xu=(Xu)^\\top (Xu)\\) So \\(Xu=0\\), the columns of \\(X\\) are linearly dependent, and the rank of \\(X\\) is less than \\(p\\). Next, suppose that the rank of \\(X\\) is less than \\(p\\) so that there exists a nonzero vector \\(u\\) such that \\(Xu = 0\\). Then \\(X^\\top Xu = 0\\), and hence \\(X^\\top X\\) is singular. In what follows, we assume that \\(\\mathrm{rank}(X)=p&lt;n\\). 4.2.1 期望和方差 Assumption A: Assume that \\(E[\\epsilon]=0\\) and \\(Var[\\epsilon]=\\sigma^2I_n\\). 定理 4.7 Suppose that Assumption A is satisfied and \\(\\mathrm{rank}(X)=p&lt;n\\), we have \\(E[\\hat\\beta]=\\beta,\\) \\(Var[\\hat\\beta]=\\sigma^2(X^\\top X)^{-1}\\). 证明. \\[\\begin{align*} E[\\hat\\beta]&amp;= E[(X^\\top X)^{-1}X^{\\top}Y] \\\\ &amp;= (X^\\top X)^{-1}X^{\\top}E[Y]\\\\ &amp;=(X^\\top X)^{-1}X^{\\top}(X\\beta)=\\beta. \\end{align*}\\] \\[\\begin{align*} Var[\\hat\\beta] &amp;= Var[(X^\\top X)^{-1}X^{\\top}Y]\\\\ &amp;=(X^\\top X)^{-1}X^{\\top}Var(Y)X(X^\\top X)^{-1}\\\\ &amp;=\\sigma^2(X^\\top X)^{-1}. \\end{align*}\\] We used the fact that \\(Var(AY) = AVar(Y)A^\\top\\) for any fixed matrix \\(A\\), and \\(X^\\top X\\) and therefore \\((X^\\top X)^{-1}\\) are symmetric. 4.2.2 误差项的方差的估计 定义 4.2 We give the following concepts. The fitted values: \\(\\hat Y = X\\hat\\beta\\) The vector of residuals: \\(\\hat\\epsilon = Y-\\hat Y\\) The sum of squared errors (SSE): \\(S_e^2=Q(\\hat\\beta)=||Y-\\hat Y||^2=||\\hat\\epsilon||^2\\) Note that \\[\\hat Y = X\\hat\\beta=X(X^\\top X)^{-1}X^\\top Y=:PY.\\] The projection matrix: \\(P = X(X^\\top X)^{-1}X^\\top\\) The vector of residuals is then \\(\\hat\\epsilon=(I_n-P)Y\\). Two useful properties of \\(P\\) are given in the following lemma. 引理 4.2 Let \\(P\\) be defined as before. Then \\[P = P^\\top=P^2,\\] \\[I_n-P = (I_n-P)^\\top=(I_n-P)^2.\\] We may think geometrically of the fitted values, \\(\\hat Y=X\\hat\\beta=PY\\), as being the projection of \\(Y\\) onto the subspace spanned by the columns of \\(X\\). The sum of squared residuals is then \\[\\begin{align*} S_e^2 := ||\\hat \\epsilon||^2 = Y^\\top(I_n-P)^\\top(I_n-P)Y=Y^\\top(I_n-P)Y. \\end{align*}\\] 定义 4.3 Let \\(A\\) be an \\(n\\times n\\) matrix. The trace of the matrix \\(A\\) is defined as \\(tr(A) = \\sum_{i=1}^n a_{ii}\\), where \\(a_{ii}\\) are the elements on the main diagonal. 引理 4.3 If \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times m\\) matrix, then \\[tr(AB)=tr(BA).\\] This is the cyclic property of the trace. Using Lemma 4.3, we have \\[\\begin{align*} E[S_e^2]&amp;= E[Y^\\top(I_n-P)Y]=E[tr(Y^\\top(I_n-P)Y)] \\\\&amp;= E[tr((I_n-P)YY^\\top)]=tr((I_n-P)E[YY^\\top])\\\\ &amp;=tr((I_n-P)(Var[Y]+E[Y]E[Y^\\top]))\\\\ &amp;=tr((I_n-P)(\\sigma^2 I_n))+tr((I_n-P)X\\beta\\beta^\\top X^\\top)\\\\ &amp;=\\sigma^2(n-tr(P)), \\end{align*}\\] where we used \\((I_n-P)X=X-X(X^\\top X)^{-1}X^\\top X=0\\). Using the cyclic property of the trace again gives \\[\\begin{align*} tr(P)&amp;= tr(X(X^\\top X)^{-1}X^\\top)\\\\ &amp;=tr(X^\\top X(X^\\top X)^{-1})=tr(I_p)=p. \\end{align*}\\] We therefore have \\(E[S_e^2]=(n-p)\\sigma^2\\). 定理 4.8 Suppose that Assumption A is satisfied and \\(\\mathrm{rank}(X)=p&lt;n\\). Then \\[\\hat\\sigma^2 = \\frac{S_e^2}{n-p}\\] is an unbiased estimate of \\(\\sigma^2\\). 4.2.3 抽样分布定理 Assumption B: Assume that \\(\\epsilon\\sim N(0,\\sigma^2I_n)\\). 定理 4.9 Suppose that Assumption B is satisfied and \\(\\mathrm{rank}(X)=p&lt;n\\), we have \\(\\hat\\beta \\sim N(\\beta, \\sigma^2(X^\\top X)^{-1})\\), \\(\\frac{(n-p)\\hat\\sigma^2}{\\sigma^2}=\\frac{S_e^2}{\\sigma^2}\\sim \\chi^2(n-p)\\), \\(\\hat\\epsilon\\) is independent of \\(\\hat Y\\), \\(S_e^2\\) (or equivalently \\(\\hat\\sigma^2\\)) is independent of \\(\\hat\\beta\\). 证明. If Assumption B is satisfied, then \\(Y = X\\beta+\\epsilon\\sim N(X\\beta,\\sigma^2I_n)\\). Recall that \\(\\hat\\beta = (X^\\top X)^{-1}X^\\top Y\\) is normally distributed. The mean and covariance are given in Theorem 4.7 since Assumption A is satisfied. Let \\(\\xi_1,\\dots,\\xi_p\\) be the orthonormal basis (标准正交基) of the subspace \\(\\mathrm{span}(X)\\) of \\(\\mathbb{R}^n\\) generated by the \\(p\\) columns of the matrix \\(X\\), and \\(\\xi_{p+1},\\dots,\\xi_n\\) be the orthonormal basis of the orthogonal complement \\(\\mathrm{span}(X)^\\perp\\). Since \\(\\hat Y = X\\hat\\beta\\in \\mathrm{span}(X)\\), there exists \\(z_1,\\dots,z_p\\) such that \\[\\hat Y = \\sum_{i=1}^p z_i\\xi_i.\\] On the other hand, by Lemma 4.2, we have \\[\\hat \\epsilon^\\top X = Y^\\top(I_n-P)^\\top X= Y^\\top (X-P X) = 0.\\] As a result, \\(\\hat \\epsilon\\in \\mathrm{span}(X)^\\perp\\). So there exsits \\(z_{p+1},\\dots,z_n\\) such that \\[\\hat \\epsilon = \\sum_{i=p+1}^n z_i\\xi_i.\\] Let \\(U = (\\xi_1,\\dots,\\xi_n)\\), then \\(U\\) is an orthogonal matrix, and let \\(z=(z_1,\\dots,z_n)^\\top\\). We thus have \\[Y = \\hat Y+\\hat\\epsilon =\\sum_{i=1}^nz_i\\xi_i=Uz.\\] Therefore, \\[z=U^{-1}Y=U^\\top Y\\sim N(U^\\top X\\beta,U^\\top(\\sigma^2 I_n)U)=N(U^\\top X\\beta,\\sigma^2 I_n).\\] This implies that \\(z_i\\) are independently normally distributed. So \\(\\hat Y\\) and \\(\\hat\\epsilon\\) are independent. We next prove that \\(E[z_i]=0\\) for all \\(i&gt;p\\). Let \\(A=(\\xi_{p+1},\\dots,\\xi_{n})\\), then \\[A(z_{p+1},\\dots,z_{n})^\\top = \\hat\\epsilon=(I_n-P)Y.\\] This gives \\[\\begin{align*} E[(z_{p+1},\\dots,z_{n})^\\top] &amp;= E[A^\\top (I_n-P)Y]\\\\ &amp;=A^\\top (I_n-P)E[Y]=A^\\top (I_n-P)X\\beta=0. \\end{align*}\\] Consequently, \\(z_i\\stackrel{iid}{\\sim} N(0,\\sigma^2),i=p+1,\\dots,n\\), implying \\[S_e^2/\\sigma^2 = \\hat\\epsilon^\\top\\hat\\epsilon/\\sigma^2 = \\sum_{i=p+1}^n (z_i/\\sigma)^2\\sim \\chi^2(n-p).\\] Note that \\(S_e^2\\) is a function of \\(\\hat\\epsilon_i\\) and \\(\\hat \\beta = (X^\\top X)^{-1}X^\\top Y =(X^\\top X)^{-1} X^\\top \\hat Y\\) are linear conbinations of \\(\\hat y_i\\). So \\(S_e^2\\) and \\(\\hat\\beta\\) are independent. 4.2.4 置信区间和假设检验 Let \\(C=(X^\\top X)^{-1}\\) with entries \\(c_{ij}\\). Suppose that Assumption B is satisfied and \\(\\mathrm{rank}(X)=p&lt;n\\). If \\(\\sigma^2\\) is known, for each \\(\\beta_i\\), the \\(100(1-\\alpha)\\%\\) CI is \\[\\hat\\beta_i \\pm u_{1-\\alpha/2}\\sigma\\sqrt{c_{i+1,i+1}},\\ i=0,\\dots,p-1.\\] If \\(\\sigma^2\\) is unknown, for each \\(\\beta_i\\), the \\(100(1-\\alpha)\\%\\) CI is \\[\\hat\\beta_i \\pm t_{1-\\alpha/2}(n-p)\\hat\\sigma\\sqrt{c_{i+1,i+1}}.\\] For testing hypothesis \\(H_0:\\beta_i= 0\\ vs.\\ H_1:\\beta_i\\neq 0\\), the rejection region is \\[W = \\{|\\hat\\beta_i|&gt;t_{1-\\alpha/2}(n-p)\\hat\\sigma\\sqrt{c_{i+1,i+1}}\\}.\\] The \\(100(1-\\alpha)\\%\\) CI for \\(\\sigma^2\\) is \\[\\left[\\frac{(n-p)\\hat\\sigma^2}{\\chi_{1-\\alpha/2}^2(n-p)},\\frac{(n-p)\\hat\\sigma^2}{\\chi_{\\alpha/2}^2(n-p)}\\right]=\\left[\\frac{S_e^2}{\\chi_{1-\\alpha/2}^2(n-p)},\\frac{S_e^2}{\\chi_{\\alpha/2}^2(n-p)}\\right].\\] 4.2.5 模型整体的显著性检验 Consider the hypothesis test: \\[H_0:\\beta_1=\\dots=\\beta_{p-1}=0\\ vs.\\ H_1: \\beta_{i^*}\\neq 0\\text{ for some }i^*\\ge 1.\\] 定义 4.4 We have the following concepts. The total sum of squares (SST): \\[S_T^2 = \\sum_{i=1}^n(y_i-\\bar Y)^2\\] The sum of squares due to regression (SSR): \\[S_R^2 = \\sum_{i=1}^n(\\hat y_i-\\bar Y)^2\\] The sum of squared errors (SSE): \\[S_e^2 = \\sum_{i=1}^n(y_i-\\hat y_i)^2\\] 定理 4.10 We have the following decomposition: \\[S_T^2=S_R^2+S_e^2.\\] 证明. It is easy to see that \\[\\begin{align*} S_T^2&amp;=\\sum_{i=1}^n (y_i-\\bar Y)^2 = \\sum_{i=1}^n (y_i-\\hat y_i+\\hat y_i-\\bar Y)^2\\\\ &amp;=\\sum_{i=1}^n [(y_i-\\hat y_i)^2+(\\hat y_i-\\bar Y)^2+2(y_i-\\hat y_i)(\\hat y_i-\\bar Y)]\\\\ &amp;=S_R^2+S_e^2 +2\\sum_{i=1}^n (y_i-\\hat y_i)(\\hat y_i-\\bar Y) \\end{align*}\\] Using Lemma 4.2, we have \\[\\begin{align*} \\sum_{i=1}^n (y_i-\\hat y_i)(\\hat y_i-\\bar Y) &amp;= \\hat\\epsilon^\\top\\hat Y-\\bar Y\\sum_{i=1}^n \\hat\\epsilon_i\\\\ &amp;= [(I_n-P)Y]^\\top PY-\\bar Y(1,1,\\dots,1) (I_n-P)Y\\\\ &amp;=Y^\\top (I_n-P)PY - \\bar Y[(1,1,\\dots,1)-(1,1,\\dots,1)P]Y\\\\ &amp;= 0. \\end{align*}\\] This is because \\(PX = X\\) and the first column of \\(X\\) is \\((1,1,\\dots,1)^\\top\\). This implies that \\(P(1,1,\\dots,1)^\\top = (1,1,\\dots,1)^\\top\\). 定理 4.11 Suppose that Assumption B is satisfied and \\(\\mathrm{rank}(X)=p&lt;n\\), we have \\(S_R^2,S_e^2,\\bar Y\\) are independent, and if the null \\(H_0:\\beta_1=\\dots=\\beta_{p-1}=0\\) is true, \\(S_R^2/\\sigma^2\\sim\\chi^2(p-1)\\). 证明. Using the same notations in the proof of Theorem 4.9. Since \\((1,\\dots,1)^\\top\\in \\mathrm{span}(X)\\), we set \\(\\xi_1 = (1/\\sqrt{n},1/\\sqrt{n},\\dots,1/\\sqrt{n})^\\top\\). Recall that \\[Y = \\sum_{i=1}^n z_i\\xi_i = Uz.\\] The average \\(\\bar Y = (1/n,\\dots,1/n)Y = (1/n,\\dots,1/n)Uz=z_1/\\sqrt{n},\\) where we used the fact that \\(U\\) is orthogonal matrix and the first colum of \\(U\\) is \\(\\xi_1 = (1/\\sqrt{n},1/\\sqrt{n},\\dots,1/\\sqrt{n})^\\top\\). Recall that \\(\\hat Y = \\sum_{i=1}^p z_i\\xi_i = (z_1/\\sqrt{n},z_1/\\sqrt{n},\\dots,z_1/\\sqrt{n})^\\top+\\sum_{i=2}^p z_i\\xi_i\\). This implies that \\[(\\hat y_1-\\bar Y,\\dots,\\hat y_n-\\bar Y)^\\top = \\sum_{i=2}^p z_i\\xi_i.\\] As a result, \\(S_R^2 = \\sum_{i=2}^p z_i^2\\). Recall that \\(S_e^2=\\sum_{i=p+1}^n z_i^2\\). Since \\(z_i\\) are independent, \\(S_R^2,S_e^2,\\bar Y\\) are independent. If \\(\\beta_1=\\dots=\\beta_{p-1}=0\\), we have \\[E[z] = U^\\top X\\beta = U^\\top (\\beta_0,\\dots,\\beta_0)^\\top.\\] \\[E[z^\\top]=\\beta_0(1,1,\\dots,1) U = \\beta_0(\\sqrt{n},0,\\dots,0).\\] We therefore have \\(z_i\\sim N(0,\\sigma^2)\\) for \\(i=2,\\dots,n\\). So \\[S_R^2/\\sigma^2 = \\sum_{i=2}^p (z_i/\\sigma)^2\\sim \\chi^2(p-1).\\] We now use generalized likelihood ratio (GLR) test. The likelihood function for \\(Y\\) is given by \\[L(\\beta,\\sigma^2) = (2\\pi \\sigma^2)^{-n/2} e^{-\\frac{||Y-X\\beta||^2}{2\\sigma^2}}.\\] The maximizers for \\(L(\\beta,\\sigma^2)\\) over the parameter space \\(\\Theta=\\{(\\beta,\\sigma^2)|\\beta\\in \\mathbb{R}^p,\\sigma^2&gt;0\\}\\) are \\[\\hat\\beta = (X^\\top X)^{-1}X^\\top Y,\\ \\hat\\sigma^2 = \\frac{||Y-X\\hat \\beta||}{n}=\\frac{S_e^2}{n}.\\] The maximizers for \\(L(\\beta,\\sigma^2)\\) over the sub-space \\(\\Theta_0=\\{(\\beta,\\sigma^2)|\\beta_i=0,i\\ge 1,\\sigma^2&gt;0\\}\\) are \\[\\hat\\beta^* = (\\bar Y,0,\\dots,0)^\\top,\\ \\hat\\sigma^{*2} = \\frac{||Y-X\\hat \\beta^*||}{n}=\\frac{S_T^2}{n}.\\] The likelihood ratio is then given by \\[\\lambda = \\frac{\\sup_{\\theta\\in\\Theta}L(\\beta,\\sigma^2)}{\\sup_{\\theta\\in\\Theta_0}L(\\beta,\\sigma^2)} = \\frac{L(\\hat\\beta,\\hat\\sigma^2)}{L(\\hat\\beta^*,\\hat\\sigma^{*2})}= \\left(\\frac{S_T^2}{S_e^2}\\right)^{n/2}= \\left(1+\\frac{S_R^2}{S_e^2}\\right)^{n/2}.\\] By Theorems 4.9 and 4.11, if the null is true we have \\[F=\\frac{S_R^2/(p-1)}{S_e^2/(n-p)}\\sim F(p-1,n-p).\\] We take \\(F\\) as the test statistic. The rejection region is \\(W=\\{F&gt;C\\}\\), where the critical value \\(C = F_{1-\\alpha}(p-1,n-p)\\) so that \\(\\sup_{\\theta\\in\\Theta_0}P_\\theta(F&gt;C)=\\alpha\\). 定义 4.5 The coefficient of determination is sometimes used as a crude measure of the strength of a relationship that has been fit by least squares. This coefficient is defined as \\[R^2 =\\frac{S_R^2}{S_T^2}=1-\\frac{S_e^2}{S_T^2}.\\] It can be interpreted as the proportion of the variability of the dependent variable that can be explained by the independent variables. 注意到，回归变量越多，残差平方和\\(S_e^2\\)越小，\\(R^2\\)自然变大。所以用\\(R^2\\)衡量模型拟合程度显然不合适，没有考虑过度拟合的情况。另一方面，随机项的方差的估计\\(\\hat\\sigma^2=S_e^2/(n-p)\\)并不一定随着\\(S_e^2\\)越小而变小，因为变量个数\\(p\\)变大，该估计量的分母变小。于是，有人提出对\\(R^2\\)进行调整，调整的决定系数(adjusted R-squared)为 \\[\\tilde{R}^2 = 1-\\frac{S_e^2/(n-p)}{S_T^2/(n-1)}=1-\\frac{(n-1)S_e^2}{(n-p)S_T^2}.\\] 不难看出\\(\\tilde{R}^2&lt;R^2\\)，这是因为\\(p&gt;1\\). 调整的决定系数\\(\\tilde{R}^2\\)综合考虑了模型变量个数\\(p\\)的影响。 It is easy to see that \\[F = \\frac{S_T^2 R^2/(p-1)}{S_T^2(1-R^2)/(n-p)}=\\frac{ R^2/(p-1)}{(1-R^2)/(n-p)}.\\] For the simple linear model \\(p=2\\), we have \\[S_R^2 = \\sum_{i=1}^n(\\hat y_i-\\bar y)^2 = \\hat\\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2 = \\frac{\\ell_{xy}^2}{\\ell_{xx}}.\\] This gives \\[R^2 = \\frac{\\ell_{xy}^2}{\\ell_{xx}\\ell_{yy}} = \\rho^2,\\] where \\[\\rho = \\frac{\\ell_{xy}}{\\sqrt{\\ell_{xx}\\ell_{yy}}}=\\frac{\\frac 1 n\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\frac 1 n\\sum_{i=1}^n(x_i-\\bar x)^2}\\sqrt{\\frac 1 n\\sum_{i=1}^n(y_i-\\bar y)^2}}\\] is the sample correlation coefficient between \\(x_i\\) and \\(y_i\\). 4.2.6 预测 Confidence interval for \\(E[y_{n+1}]\\) Consider \\[y_{n+1} = \\beta_0+\\beta_1x_{n+1,1}+\\dots+\\beta_{p-1}x_{n+1,p-1}+\\epsilon_{n+1}.\\] Under Assumption B, \\(y_{n+1}=v^\\top \\beta+\\epsilon_{n+1}\\sim N(v^\\top \\beta,\\sigma^2)\\) , where \\(v = (1,x_{n+1,1},x_{n+1,2},\\dots,x_{n+1,p-1})^\\top\\). An unbiased estimate of the expected value of \\(E[y_{n+1}]=v^\\top \\beta\\) is the fitted value \\[\\hat y_{n+1} = v^\\top \\hat\\beta \\sim N(v^\\top \\beta, \\sigma^2 v^\\top(X^\\top X)^{-1}v).\\] By Theorem 4.9, we have \\[\\frac{\\hat y_{n+1}-v^\\top \\beta}{\\hat\\sigma\\sqrt{v^\\top(X^\\top X)^{-1}v}}\\sim t(n-p).\\] The \\(100(1-\\alpha)\\%\\) CI for \\(E[y_{n+1}]\\) is \\[\\hat y_{n+1}\\pm t_{1-\\alpha/2}(n-p)\\hat{\\sigma}\\sqrt{v^\\top(X^\\top X)^{-1}v}.\\] Prediction interval for \\(y_{n+1}\\) Similarly, \\[\\frac{y_{n+1}-\\hat y_{n+1}}{\\hat{\\sigma}\\sqrt{1+v^\\top(X^\\top X)^{-1}v}}\\sim t(n-p).\\] The \\(100(1-\\alpha)\\%\\) prediction interval for \\(y\\) is \\[\\hat y_{n+1}\\pm t_{1-\\alpha/2}(n-p)\\hat{\\sigma}\\sqrt{1+v^\\top(X^\\top X)^{-1}v}.\\] 4.2.7 案例分析2 It is found that the systolic pressure is linked to the weight and the age. We now have the following data. blood=data.frame( weight=c(76.0,91.5,85.5,82.5,79.0,80.5,74.5, 79.0,85.0,76.5,82.0,95.0,92.5), age=c(50,20,20,30,30,50,60,50,40,55,40,40,20), pressure=c(120,141,124,126,117,125,123,125, 132,123,132,155,147)) knitr::kable(blood,caption=&quot;systolic pressure&quot;) 表 4.2: systolic pressure weight age pressure 76.0 50 120 91.5 20 141 85.5 20 124 82.5 30 126 79.0 30 117 80.5 50 125 74.5 60 123 79.0 50 125 85.0 40 132 76.5 55 123 82.0 40 132 95.0 40 155 92.5 20 147 plot(blood) lm.blood=lm(pressure~weight+age,data=blood) summary(lm.blood) ## ## Call: ## lm(formula = pressure ~ weight + age, data = blood) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.040 -1.018 0.464 0.691 4.327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -62.9634 16.9998 -3.70 0.00408 ** ## weight 2.1366 0.1753 12.19 2.5e-07 *** ## age 0.4002 0.0832 4.81 0.00071 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.85 on 10 degrees of freedom ## Multiple R-squared: 0.946, Adjusted R-squared: 0.935 ## F-statistic: 87.8 on 2 and 10 DF, p-value: 4.53e-07 The regression function is \\[\\hat y = -62.96336 + 2.13656 x_1+ 0.40022 x_2.\\] n = length(blood$weight) X = cbind(intercept=rep(1,n),weight=blood$weight,age=blood$age) C = solve(t(X)%*%X) SSE = sum(lm.blood$residuals^2) # sum of squared errors # SST = var(blood$pressure)*(n-1) # SSR = SST-SSE # Fstat = SSR/(3-1)/(SSE/(n-3)) cov = SSE/(n-3)*C knitr::kable(cov, caption = &quot;Estimated Covariance Matrix&quot;) 表 4.3: Estimated Covariance Matrix intercept weight age intercept 288.992 -2.9499 -1.1174 weight -2.950 0.0307 0.0102 age -1.117 0.0102 0.0069 Similarly to the simple linear model, we can construct the confidence intervals and prediction intervals for \\(E[y]\\) and \\(y\\), respectively. newdata = data.frame( age = rep(31,100), weight = seq(70,100,length.out = 100) ) CI = predict(lm.blood,newdata,interval = &quot;confidence&quot;) Pred = predict(lm.blood,newdata,interval = &quot;prediction&quot;) par(mar=c(4,4,2,1)) matplot(newdata$weight,cbind(CI,Pred[,-1]),type=&quot;l&quot;, lty = c(1,5,5,2,2), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;,&quot;brown&quot;,&quot;brown&quot;),lwd=2, xlab=&quot;Weight&quot;,ylab=&quot;Pressure&quot;,main = &quot;Age = 31&quot;) legend(70,160,c(&quot;Fitted&quot;,&quot;Confidence&quot;,&quot;Prediction&quot;), lty = c(1,5,2),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;brown&quot;)) newdata = data.frame( weight = rep(85,41), age = seq(20,60) ) CI = predict(lm.blood,newdata,interval = &quot;confidence&quot;) Pred = predict(lm.blood,newdata,interval = &quot;prediction&quot;) par(mar=c(4,4,2,1)) matplot(newdata$age,cbind(CI,Pred[,-1]),type=&quot;l&quot;, lty = c(1,5,5,2,2), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;,&quot;brown&quot;,&quot;brown&quot;),lwd=2, xlab=&quot;Age&quot;,ylab=&quot;Pressure&quot;,main = &quot;Weight = 85&quot;) legend(20,150,c(&quot;Fitted&quot;,&quot;Confidence&quot;,&quot;Prediction&quot;), lty = c(1,5,2),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;brown&quot;)) 4.3 线性模型的推广 Inherently Linear models: \\[\\begin{align*} f(y) &amp;= \\beta_0+\\beta_1 g_1(x_1,\\dots,x_{p-1})+\\dots\\\\&amp;+\\beta_{k-1} g_{k-1}(x_1,\\dots,x_{p-1})+\\epsilon \\end{align*}\\] Let \\(y^*=f(y),\\ x_i^*=g_i(x_1,\\dots,x_{p-1})\\). The transformed model is linear \\[y^*=\\beta_0+\\beta_1 x_1^*+\\dots+\\beta_{k-1} x_{k-1}^{*}+\\epsilon.\\] Below are some examples. Polynomial models: \\[y = \\beta_0+\\beta_1x+\\beta_2x^2+\\beta_{p-1}x^p+\\epsilon\\] Interaction models: \\[y = \\beta_0+\\beta_1x_1+\\beta_2x_2^2+\\beta_{3}x_1x_2+\\epsilon\\] Multiplicative models: \\[y = \\gamma_1X_1^{\\gamma_2}X_2^{\\gamma_3}\\epsilon^*\\] Exponential models: \\[y = \\exp\\{\\beta_0+\\beta_1x_1+\\beta_2x_2\\}+\\epsilon^*\\] Reciprocal models: \\[y=\\frac{1}{\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_{p-1}x^p+\\epsilon}\\] Semilog models: \\[y = \\beta_0+\\beta_1\\log(x)+\\epsilon\\] Logit models: \\[\\log\\left(\\frac{y}{1-y}\\right) = \\beta_0+\\beta_1 x+\\epsilon\\] Probit models: \\(\\Phi^{-1}(y) = \\beta_0+\\beta_1 x+\\epsilon\\), where \\(\\Phi\\) is the CDF of \\(N(0,1)\\). 4.4 回归诊断 4.4.1 动机 先考虑这样一个例子： Anscombe在1973年构造了4组数据，每组数据都是由11对点\\((x_i,y_i)\\)组成，试分析4组数据是否通过回归方程的检验。 knitr::kable(head(anscombe), caption = &#39;Anscombe的数据&#39;, booktabs = TRUE) 表 4.4: Anscombe的数据 x1 x2 x3 x4 y1 y2 y3 y4 10 10 10 8 8.04 9.14 7.46 6.58 8 8 8 8 6.95 8.14 6.77 5.76 13 13 13 8 7.58 8.74 12.74 7.71 9 9 9 8 8.81 8.77 7.11 8.84 11 11 11 8 8.33 9.26 7.81 8.47 14 14 14 8 9.96 8.10 8.84 7.04 回归结果如下所示，从中发现这四组数据得到的回归结果非常相似，各项检验也是显著的。这是否意味着所建立的回归模型都是合理的呢？ coef.list = data.frame() for(i in 1:4) { ff = as.formula(paste0(&quot;y&quot;,i,&quot;~&quot;,&quot;x&quot;,i)) lmi = lm(ff,data = anscombe) slmi = summary(lmi) pvalue = 1-pf(slmi$fstatistic[1],slmi$fstatistic[2],slmi$fstatistic[3]) df = as.data.frame(cbind(slmi$coef,p_value = c(pvalue,NA))) row.names(df)[1] = paste0(row.names(df)[1],i) coef.list = rbind(coef.list,df) } knitr::kable(coef.list,caption = &quot;四种情况的回归结果，最后一列为F检验p值&quot;) 表 4.5: 四种情况的回归结果，最后一列为F检验p值 Estimate Std. Error t value Pr(&gt;|t|) p_value (Intercept)1 3.0001 1.1247 2.667 0.0257 0.0022 x1 0.5001 0.1179 4.242 0.0022 NA (Intercept)2 3.0009 1.1253 2.667 0.0258 0.0022 x2 0.5000 0.1180 4.239 0.0022 NA (Intercept)3 3.0025 1.1245 2.670 0.0256 0.0022 x3 0.4997 0.1179 4.239 0.0022 NA (Intercept)4 3.0017 1.1239 2.671 0.0256 0.0022 x4 0.4999 0.1178 4.243 0.0022 NA 答案是否定的！不妨画出原始数据与回归直线。从中发现， 第一幅图用该线性回归方程较为合适，但其它三幅图则不然。 par(mfrow = c(2,2),mar=c(4,4,1,1)+.1,oma=c(0,0,2,0)) for(i in 1:4) { ff = as.formula(paste0(&quot;y&quot;,i,&quot;~&quot;,&quot;x&quot;,i)) lmi = lm(ff,data = anscombe) plot(ff,data = anscombe,col=&quot;red&quot;,pch = 21,bg=&quot;orange&quot;, cex = 1.2,xlim=c(3,19),ylim=c(3,13)) abline(coef(lmi),col=&quot;blue&quot;) } 图 4.2: 回归直线 library(ggplot2) rn = colnames(anscombe) df &lt;- data.frame( type = rep(rn[1:4],each=nrow(anscombe)), x = c(anscombe$x1,anscombe$x2,anscombe$x3,anscombe$x4), y = c(anscombe$y1,anscombe$y2,anscombe$y3,anscombe$y4) ) ggplot(df, aes(x, y, color=type)) + geom_smooth(method=lm, se=F) + geom_point() + facet_wrap(vars(type)) 对于一元回归问题，我们或许可以通过画图观察自变量和因变量是否可以用线性模型刻画。但是，对于多元回归模型，试图通过画图的方式来判断线性关系是不可行的。那么，一般情况下，我们如何验证线性模型的合理性呢？这个时候就需要对所建立模型进行误差诊断，通过分析其残差来判断回归分析的基本假设是否成立。如发现果不成立，那么所有的区间估计、显著性检验都是不可靠的！ 4.4.2 残差的定义和性质 回归分析都是基于误差项的假定进行的，最常见的假设 \\[\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2).\\] 如何考察数据基本上满足这些假设？自然从残差的角度来解决问题，这种方法叫残差分析。 研究那些数据对统计推断（估计、检验、预测和控制）有较大影响的点，这样的点叫做影响点。剔除那些有较强影响的异常/离群(outlier)数据，这就是所谓的影响分析(influence analysis). 残差的定义为 \\[\\hat\\epsilon = Y-\\hat Y\\] 残差的性质如下： 定理 4.12 在假设\\(\\epsilon\\sim N(0,\\sigma^2I_n)\\)下， \\(\\hat\\epsilon \\sim N(0,\\sigma^2(I_n-P))\\) \\(Cov(\\hat Y,\\hat\\epsilon) = 0\\) \\(1^\\top\\hat\\epsilon = 0\\) 从中可以看出，\\(Var[\\hat\\epsilon_i] = \\sigma^2(1-p_{ii})\\), 其中\\(p_{ij}\\)为投影矩阵的元素。该方差与\\(\\sigma^2\\)以及\\(p_{ii}\\)有关，因此直接比较残差\\(\\hat\\epsilon_i\\)是不恰当的。 为此，将残差标准化： \\[\\frac{\\hat\\epsilon_i-E[\\hat\\epsilon_i]}{\\sqrt{Var[\\hat\\epsilon_i}]}= \\frac{\\hat\\epsilon_i}{\\sigma\\sqrt{1-p_{ii}}},\\ i=1,\\dots,n\\] 由于\\(\\sigma\\)是未知的，所以用\\(\\hat\\sigma\\)来代替，其中\\(\\hat\\sigma^2 = S_e^2/(n-p)\\). 于是得到学生化(studentized residuals) \\[t_i = \\frac{\\hat\\epsilon_i}{\\hat{\\sigma}\\sqrt{1-p_{ii}}}\\] \\(t_i\\)虽然是\\(\\hat\\epsilon_i\\)的学生化，但它的分布并不服从\\(t\\)分布，它的分布通常比较复杂 \\(t_1,\\dots,t_n\\)通常是不独立的 在实际应用中，可以近似认为：\\(t_1,\\dots,t_n\\)是相互独立，服从\\(N(0,1)\\)分布 在实际应用中使用的残差图就是根据上述假定来对模型合理性进行诊断的。 4.4.3 残差图 残差图：以残差为纵坐标，其他的量（一般为拟合值\\(\\hat y_i\\)）为横坐标的散点图。 由于可以近似认为：\\(t_1,\\dots,t_n\\)是相互独立，服从\\(N(0,1)\\)分布，所以可以把它们看作来自\\(N(0,1)\\)的iid样本 根据标准正态的性质，大概有\\(95\\%\\)的\\(t_i\\)落入区间\\([-2,2]\\)中。由于\\(\\hat Y\\)与\\(\\hat\\epsilon\\)不相关，所以\\(\\hat y_i\\)与学生化残差\\(t_i\\)的相关性也很小。 这样在残差图中，点\\((\\hat y_i,t_i),i=1,\\dots,n\\)大致应该落在宽度为4的水平带\\(|t_i|\\le 2\\)的区域内，且不呈现任何趋势。 图 4.3: 正常的残差图 图 4.4: 误差随着横坐标的增加而增加 图 4.5: 误差随着横坐标的增加而减少 图 4.6: 误差中间大，两端小 图 4.7: 回归函数可能非线性，或者误差相关或者漏掉重要的自变量 图 4.8: 回归函数可能非线性 案例里面四组数据的残差图如下，从中发现只有第一组数据的残差较为合理。 par(mfrow = c(2,2),mar=c(4,4,1,1)+.1,oma=c(0,0,2,0)) for(i in 1:4) { ff = as.formula(paste0(&quot;y&quot;,i,&quot;~&quot;,&quot;x&quot;,i)) lmi = lm(ff,data = anscombe) plot(lmi$fitted.values,rstandard(lmi),pch = 21,bg=&quot;orange&quot;, cex = 1.2,xlim=c(3,15),ylim=c(-4,4), xlab = &quot;fitted values&quot;, ylab = &quot;standardized residuals&quot;) abline(h=c(-3,3),lty=2) } 图 4.9: 残差图 4.4.4 残差诊断的思路 如果残差图中显示非线性，可适当增加自变量的二次项或者交叉项。具体问题具体分析。 如果残差图中显示误差方差不相等(heterogeneity, 方差非齐性)，可以对变量做适当的变换，使得变换后的相应变量具有近似相等的方差(homogeneity, 方差齐性)。最著名的方法是Box-Cox变换，对因变量（响应变量）进行如下变换 \\[Y{(\\lambda)} = \\begin{cases} \\frac 1\\lambda (Y^{\\lambda}-1),&amp;\\ \\lambda\\neq 0\\\\ \\log Y,&amp;\\ \\lambda=0， \\end{cases} \\] 其中\\(\\lambda\\)是待定的变换参数，可由极大似然法估计。注意此变换只是针对\\(Y\\)为正数的情况。如果出现负数时，需要作出调整。 在R中使用命令boxcox() (需要首先运行library(MASS)) 详情见综述论文： R. M. Sakia. The Box-Cox Transformation Technique: A Review. The Statistician, 41: 169-178, 1992. 4.4.5 案例分析：基于Box-Cox变换 已知某公司从2000年1月至2005年5月的逐月销售量，利用所学的统计知识对所建立的模型进行诊断。 sales&lt;-c(154,96,73,49,36,59,95,169,210,278,298,245, 200,118,90,79,78,91,167,169,289,347,375, 203,223,104,107,85,75,99,135,211,335,460, 488,326,518,404,300,210,196,186,247,343, 464,680,711,610,613,392,273,322,189,257, 324,404,677,858,895,664,628,308,324,248,272) X&lt;-1:length(sales) plot(X,sales,type=&quot;b&quot;,main=&quot;Original values&quot;,ylab=&quot;sales&quot;) model1&lt;-lm(sales~X) 对原始数据跑回归，得到如下结果 summary(model1) ## ## Call: ## lm(formula = sales ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -262.6 -123.8 -28.6 97.1 419.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 64.19 39.68 1.62 0.11 ## X 6.97 1.05 6.67 7.4e-09 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 158 on 63 degrees of freedom ## Multiple R-squared: 0.414, Adjusted R-squared: 0.405 ## F-statistic: 44.5 on 1 and 63 DF, p-value: 7.43e-09 残差图如下，发现异方差情形。 plot(predict(model1),rstandard(model1),main=&quot;Standardized Residuals&quot;, xlab=&quot;Fitted values&quot;,ylab=&quot;residual values&quot;) 进行Box-Cox变换前，通过寻找似然函数最大值求出最优的\\(\\lambda\\): library(MASS) bc&lt;-boxcox(model1,lambda=seq(-0.13,1.2,by=0.01)) lambda&lt;-bc$x[which.max(bc$y)] cat(&#39;The optimal lambda is&#39;,lambda) ## The optimal lambda is 0.15 得到最优\\(\\lambda=0.15\\)后，对因变量进行Box-Cox变换，再一次跑回归，结果如下，从中发现回归系数和回归方程是显著的。 model2&lt;-update(model1,(.^lambda-1)/lambda~.) summary(model2) ## ## Call: ## lm(formula = (sales^lambda - 1)/lambda ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1685 -1.1171 0.0347 1.0952 1.8648 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.46276 0.30703 21.05 &lt; 2e-16 *** ## X 0.06135 0.00809 7.58 1.9e-10 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.22 on 63 degrees of freedom ## Multiple R-squared: 0.477, Adjusted R-squared: 0.469 ## F-statistic: 57.5 on 1 and 63 DF, p-value: 1.9e-10 比较变换之前和变换之后的残差图： par(mfrow=c(1,2)) plot(predict(model1),rstandard(model1),main=&quot;before boxcox&quot;, xlab=&quot;Fitted values&quot;,ylab=&quot;residual values&quot;,ylim = c(-3,3)) plot(predict(model2),rstandard(model2),main=&quot;after boxcox&quot;, xlab=&quot;Fitted values&quot;,ylab=&quot;residual values&quot;,ylim = c(-3,3)) 4.4.6 离群值 产生离群/异常值(outlier)的原因： 主观原因：收集和记录数据时出现错误 客观原因：重尾分布（比如，\\(t\\)分布）和混合分布 离群值的简单判断： 数据散点图 学生化残差图，如果\\(|t_i|&gt;3\\) (或者2.5,2)，则对应的数据判定为离群值。 离群值的统计检验方法，M-估计(Maximum likelihood type estimators) 利用Cook距离，其定义为： \\[D_i = \\frac{(\\hat\\beta-\\hat\\beta_{(i)})^\\top X^\\top X(\\hat\\beta-\\hat\\beta_{(i)}) }{p\\hat\\sigma^2},i=1,\\dots,n,\\] 其中\\(\\hat\\beta_{(i)}\\)为剔除第\\(i\\)个数据得到\\(\\beta\\)的最小二乘估计 在R中用命令cooks.distance()可以得到Cook统计量的值 如果发现特别大的\\(D_i\\)一定要特别注意： 检查原始数据是否有误，如有，改正后重新计算；否则，剔除对应的数据 如果没有足够理由剔除影响大的数据，就应该采取收集更多的数据或者采用更加稳健的方法以降低强影响数据对估计和推断的影响，从而得到比较稳定的回归方程。 下面通过两组数据比较离群值的影响，第二组数据在第一组数据上面添加了一个异常数据。观察发现，添加了一个异常数据导致回归系数有较大的变化。 lm1 = lm(y1~x1,data = anscombe) x = c(anscombe$x1, 18) # 人为添加异常数据 y = c(anscombe$y1,30) # 人为添加异常数据 lm.xy = lm(y~x) plot(x,y,pch = 21,bg=c(rep(&quot;black&quot;,11),&quot;red&quot;),ylim=c(0,50)) abline(coef(lm.xy),lty=2,col=&quot;red&quot;) abline(coef(lm1),lty=2,col=&quot;blue&quot;) 观察残差图和Cook距离，容易发现异常数据。 par(mfrow=c(1,2)) plot(c(fitted(lm1),fitted(lm.xy)), c(rstandard(lm1),rstandard(lm.xy)), ylim=c(-3,3),pch = c(rep(21,11),rep(22,12)), bg = c(rep(&quot;blue&quot;,11),rep(&quot;red&quot;,12)), xlab = &quot;fitted values&quot;, ylab=&quot;standardized residuals&quot;) abline(h=c(-2,2),lty=2) plot(lm.xy,4) #画出第二组数据的cook距离 4.4.7 变量选择 常见的变量选择如下： 完全子集法 向前回归法（每步只能增加） 向后回归法（每步只能剔除） 逐步回归法（可剔除也可增加，详细见陈家鼎编著教材P208-214） “添加”或者“删除”变量依赖某个准则，常见的准则有 AIC准则(Akaike information criterion)：由日本统计学家Hirotugu Akaike提出并命名 \\[AIC(A) = \\ln Q(A) +\\frac{\\color{red} 2}{n} \\#(A)\\] BIC准则(Bayesian information criterion)：由Gideon E. Schwarz (1978, Ann. Stat.)提出 \\[BIC(A) = \\ln Q(A) +\\frac{\\color{red}{\\log n}}{n} \\#(A)\\] 其中\\(A\\subset\\{1,\\dots,p-1\\}\\)，\\(Q(A)\\)为选择子集\\(A\\)中变量跑回归的残差平方和，\\(\\#(A)\\)表示\\(A\\)中元素的个数。AIC/BIC越小越好。“添加”或者“剔除”变量操作取决于相应的AIC/BIC是否达到最小值。 下面通过一个例子展示如何进行逐步回归分析。 某种水泥在凝固时放出的热量\\(Y\\)与水泥中四种化学成分\\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\)有关，现测得13组数据，如表所示。希望从中选出主要的变量，建立\\(Y\\)关于它们的线性回归方程。 cement=data.frame( x1=c(7,1,11,11,7,11,3,1,2,21,1,11,10), x2=c(26,29,56,31,52,55,71,31,54,47,40,66,68), x3=c(6,15,8,8,6,9,17,22,18,4,23,9,8), x4=c(60,52,20,47,33,22,6,44,22,26,34,12,12), y=c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5, 93.1,115.9,83.8,113.3,109.4)) knitr::kable(cement) x1 x2 x3 x4 y 7 26 6 60 78.5 1 29 15 52 74.3 11 56 8 20 104.3 11 31 8 47 87.6 7 52 6 33 95.9 11 55 9 22 109.2 3 71 17 6 102.7 1 31 22 44 72.5 2 54 18 22 93.1 21 47 4 26 115.9 1 40 23 34 83.8 11 66 9 12 113.3 10 68 8 12 109.4 lm_all=lm(y~x1+x2+x3+x4,data=cement) summary(lm_all) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3 + x4, data = cement) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.175 -1.671 0.251 1.378 3.925 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.405 70.071 0.89 0.399 ## x1 1.551 0.745 2.08 0.071 . ## x2 0.510 0.724 0.70 0.501 ## x3 0.102 0.755 0.14 0.896 ## x4 -0.144 0.709 -0.20 0.844 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.45 on 8 degrees of freedom ## Multiple R-squared: 0.982, Adjusted R-squared: 0.974 ## F-statistic: 111 on 4 and 8 DF, p-value: 4.76e-07 所有变量参与回归分析，得到如下结果。虽然回归方程整体检验是显著的，但是大部分系数的检验都是不显著的，因此需要进行变量选择。 R语言关键命令为 step(object, scope, scale = 0, direction = c(&quot;both&quot;, &quot;backward&quot;, &quot;forward&quot;), trace = 1, keep = NULL, steps = 1000, k = 2, ...) object: 初始回归模型，比如object = lm(y=1,data=cement) scope: 为逐步回归搜索区域，比如scope = ~x1+x2+x3+x4 trace: 表示是否保留逐步回归过程，默认保留 direction: 搜索方向，默认为“both” k: 为AIC惩罚因子，即\\(AIC(A) = \\ln Q(A) +\\frac{\\color{red} k}{n} \\#(A)\\)，默认\\(k=2\\)，当\\(k=\\log n\\)是为BIC准则。 逐步回归的筛选过程如下： lm0 = lm(y~1,data=cement) #只有截距项作为初始模型 lm.step = step(lm0,scope = ~x1+x2+x3+x4) ## Start: AIC=71.44 ## y ~ 1 ## ## Df Sum of Sq RSS AIC ## + x4 1 1832 884 58.9 ## + x2 1 1809 906 59.2 ## + x1 1 1450 1266 63.5 ## + x3 1 776 1939 69.1 ## &lt;none&gt; 2716 71.4 ## ## Step: AIC=58.85 ## y ~ x4 ## ## Df Sum of Sq RSS AIC ## + x1 1 809 75 28.7 ## + x3 1 708 176 39.9 ## &lt;none&gt; 884 58.9 ## + x2 1 15 869 60.6 ## - x4 1 1832 2716 71.4 ## ## Step: AIC=28.74 ## y ~ x4 + x1 ## ## Df Sum of Sq RSS AIC ## + x2 1 27 48 25.0 ## + x3 1 24 51 25.7 ## &lt;none&gt; 75 28.7 ## - x1 1 809 884 58.9 ## - x4 1 1191 1266 63.5 ## ## Step: AIC=24.97 ## y ~ x4 + x1 + x2 ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 48 25.0 ## - x4 1 10 58 25.4 ## + x3 1 0 48 26.9 ## - x2 1 27 75 28.7 ## - x1 1 821 869 60.6 逐步回归剔除了第三个变量，得到的回归结果如下，从中发现变量\\(X_4\\)是不显著的。为此，我们手工剔除该变量，以改进模型。 summary(lm.step) ## ## Call: ## lm(formula = y ~ x4 + x1 + x2, data = cement) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.092 -1.802 0.256 1.282 3.898 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.648 14.142 5.07 0.00068 *** ## x4 -0.237 0.173 -1.37 0.20540 ## x1 1.452 0.117 12.41 5.8e-07 *** ## x2 0.416 0.186 2.24 0.05169 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.31 on 9 degrees of freedom ## Multiple R-squared: 0.982, Adjusted R-squared: 0.976 ## F-statistic: 167 on 3 and 9 DF, p-value: 3.32e-08 剔除第四个变量后的结果如下，所有的检验都是显著的，残差诊断也合理。 lm_final = update(lm.step,.~.-x4) summary(lm_final) ## ## Call: ## lm(formula = y ~ x1 + x2, data = cement) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89 -1.57 -1.30 1.36 4.05 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 52.5773 2.2862 23.0 5.5e-10 *** ## x1 1.4683 0.1213 12.1 2.7e-07 *** ## x2 0.6623 0.0459 14.4 5.0e-08 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.41 on 10 degrees of freedom ## Multiple R-squared: 0.979, Adjusted R-squared: 0.974 ## F-statistic: 230 on 2 and 10 DF, p-value: 4.41e-09 plot(predict(lm_final),rstandard(lm_final),main=&quot;new model&quot;, xlab=&quot;Fitted values&quot;,ylab=&quot;residual values&quot;,ylim = c(-3,3)) 4.4.8 LASSO 现代变量选择方法：LASSO (Least Absolute Shrinkage &amp; Selection Operator)是斯坦福大学统计系Tibshirani于1996年发表的著名论文“Regreesion shrinkage and selection via the LASSO” (Journal of Royal Statistical Society, Seriers B, 58, 267-288)中所提出的一种变量选择方法。 Professor Rob Tibshirani: https://statweb.stanford.edu/~tibs/ \\[\\hat\\beta_{LASSO} = \\arg\\min ||Y-X\\beta||^2 \\text{ subject to } \\sum_{i=0}^{p-1}|\\beta_i|\\le t,\\] 等价于 \\[\\hat\\beta_{LASSO} = \\arg\\min ||Y-X\\beta||^2+ \\lambda\\sum_{i=0}^{p-1}|\\beta_i|.\\] 在R语言中可以使用glmnet包来实施LASSO算法。 4.4.9 回归分析与因果分析 即使建立了回归关系式并且统计检验证明相关关系成立，也只能说明研究的变量是统计相关的，而不能就此断定变量之间有因果关系。 案例(Ice Cream Causes Polio)：小儿麻痹症疫苗发明前，美国北卡罗来纳州卫生部研究人员通过分析冰淇淋消费量和小儿麻痹症的关系发现当冰淇淋消费量增加时，小儿麻痹疾病也增加。州卫生部发生警告反对吃冰淇淋来试图阻止这种疾病的传播。 没有观察的混杂因素——温度 Polio and ice cream consumption both increase in the summertime. Summer is when the polio virus thrived. 图 4.10: The danger of mixing up causality and correlation 4.5 本章习题 习题 4.1 Consider the linear model \\[y_i=\\beta_0+\\beta_1x_i+\\epsilon_i,\\ \\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2), i=1,\\dots,n.\\] Derive the maximum likelihood estimators (MLE) for \\(\\beta_0,\\beta_1\\). Are they consistent with the least square estimators (LSE)? Derive the MLE for \\(\\sigma^2\\) and look at its unbiasedness. A very slippery point is whether to treat the \\(x_i\\) as fixed numbers or as random variables. In the class, we treated the predictors \\(x_i\\) as fixed numbers for sake of convenience. Now suppose that the predictors \\(x_i\\) are iid random variables (independent of \\(\\epsilon_i\\)) with density \\(f_X(x;\\theta)\\) for some parameter \\(\\theta\\). Write down the likelihood function for all of our data \\((x_i,y_i),i=1,\\dots,n\\). Derive the MLE for \\(\\beta_0,\\beta_1\\) and see whether the MLE changes if we work with the setting of random predictors? 习题 4.2 Consider the linear model without intercept \\[y_i = \\beta x_i+\\epsilon_i,\\ i=1,\\dots,n,\\] where \\(\\epsilon_i\\) are independent with \\(E[\\epsilon_i]=0\\) and \\(Var[\\epsilon_i]=\\sigma^2\\). Write down the least square estimator \\(\\hat \\beta\\) for \\(\\beta\\), and derive an unbiased estiamtor for \\(\\sigma^2\\). For fixed \\(x_0\\), let \\(\\hat{y}_0=\\hat\\beta x_0\\). Work out \\(Var[\\hat{y}_0]\\). 习题 4.3 Case study: Genetic variability is thought to be a key factor in the survival of a species, the idea being that “diverse” populations should have a better chance of coping with changing environments. Table below summarizes the results of a study designed to test that hypothesis experimentally. Two populations of fruit flies (Drosophila serrata)-one that was cross-bred (Strain A) and the other, in-bred (Strain B)-were put into sealed containers where food and space were kept to a minimum. Recorded every hundred days were the numbers of Drosophila alive in each population. Date Day number Strain A Strain B Feb 2 0 100 100 May 13 100 250 203 Aug 21 200 304 214 Nov 29 300 403 295 Mar 8 400 446 330 Jun 16 500 482 324 Plot day numbers versus population sizes for Strain A and Strain B, respectively. Does the plot look linear? If so, please use least squares to figure out the coefficients and their standard errors, and plot the two regression lines. Let \\(\\beta_1^A\\) and \\(\\beta_1^B\\) be the true slopes (i.e., growth rates) for Strain A and Strain B, respectively. Assume the population sizes for the two strains are independent. Under the same assumptions of \\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\) for both strains, do we have enough evidence here to reject the null hypothesis that \\(\\beta_1^A\\le \\beta_1^B\\) (significance level \\(\\alpha=0.05\\))? Or equivalently, do these data support the theory that genetically mixed populations have a better chance of survival in hostile environments. (提示：仿照方差相同的两个正态总体均值差的假设检验，构造相应的t检验统计量) 习题 4.4 Let us consider fitting a straight line, \\(y = \\beta_0+\\beta_1x\\), to points \\((x_i,y_i)\\), where \\(i=1,\\dots,n\\). Write down the normal equations for the simple linear model via the matrix formalism. Solve the normal equations by tha matrix approach and see whether the solutions agree with the earlier calculation derived in the simple linear models. 习题 4.5 Prove that the projection matrix \\(P=X(X^\\top X)^{-1} X^\\top\\) has an eigenvalue 1, and \\((1,\\dots,1)^\\top\\) is one of the associated eigenvectors. 习题 4.6 (The QR Method) This problem outlines the basic ideas of an alternative method, the QR method, of finding the least squares estimate \\(\\hat \\beta\\). An advantage of the method is that it does not include forming the matrix \\(X^\\top X\\), a process that tends to increase rounding error. The essential ingredient of the method is that if \\(X_{n\\times p}\\) has \\(p\\) linearly independent columns, it may be factored in the form \\[\\begin{align*} X\\quad &amp;=\\quad Q\\quad \\quad R\\\\ n\\times p &amp;\\quad n\\times p\\quad p\\times p \\end{align*}\\] where the columns of \\(Q\\) are orthogonal (\\(Q^\\top Q = I_p\\)) and \\(R\\) is upper-triangular (\\(r_{ij} = 0\\), for \\(i &gt; j\\)) and nonsingular. For a discussion of this decomposition and its relationship to the Gram-Schmidt process, see https://en.wikipedia.org/wiki/QR_decomposition. Show that \\(\\hat \\beta = (X^\\top X)^{-1}X^\\top Y\\) may also be expressed as \\(\\hat \\beta = R^{-1}Q^\\top Y\\), or \\(R\\hat \\beta = Q^\\top Y\\). Indicate how this last equation may be solved for \\(\\hat \\beta\\) by back-substitution, using that \\(R\\) is upper-triangular, and show that it is thus unnecessary to invert \\(R\\). 习题 4.7 Consider fitting the curve \\(y = \\beta_0x+\\beta_1x^2\\) to points (\\(x_i,y_i\\)), where \\(i = 1,\\dots,n\\). Use the matrix formalism to find expressions for the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\). Find an expression for the covariance matrix of the estimates. 习题 4.8 Consider the simple linear model \\[y_i= \\beta_0+\\beta_1x_i+\\epsilon_i,\\ \\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2).\\] Use the F-test method derived in the multiple linear model to test the hypothesis \\(H_0:\\beta_1=0\\ vs.\\ H_1:\\beta_1\\neq 0\\), and see whether the F-test agrees with the earlier t-test derived in the simple linear models. 习题 4.9 The following table shows the monthly returns of stock in Disney, MacDonalds, Schlumberger, and Haliburton for January through May 1998. Fit a multiple regression to predict Disney returns from those of the other stocks. What is the standard deviation of the residuals? What is \\(R^2\\)? Disney MacDonalds Schlumberger Haliburton 0.08088 -0.01309 -0.08463 -0.13373 0.04737 0.15958 0.02884 0.03616 -0.04634 0.09966 0.00165 0.07919 0.16834 0.03125 0.09571 0.09227 -0.09082 0.06206 -0.05723 -0.13242 Next, using the regression equation you have just found, carry out the predictions for January through May of 1999 and compare to the actual data listed below. What is the standard deviation of the prediction error? How can the comparison with the results from 1998 be explained? Is a reasonable explanation that the fundamental nature of the relationships changed in the one year period? (最后一问选做) Disney MacDonalds Schlumberger Haliburton 0.1 0.02604 0.02695 0.00211 0.06629 0.07851 0.02362 -0.04 -0.11545 0.06732 0.23938 0.35526 0.02008 -0.06483 0.06127 0.10714 -0.08268 -0.09029 -0.05773 -0.02933 习题 4.10 Consider the multiple linear regression model \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol {\\beta} + \\boldsymbol\\epsilon, \\] where \\(\\boldsymbol Y=(y_1,\\dots,y_n)^\\top\\), \\(\\boldsymbol\\beta=(\\beta_0,\\dots,\\beta_{p-1})^\\top\\), \\(\\boldsymbol X\\) is the \\(n\\times p\\) design matrix, and \\(\\boldsymbol\\epsilon=(\\epsilon_1,\\dots,\\epsilon_n)^\\top\\). Assume that \\(\\mathrm{rank}(X)=p&lt;n\\), \\(E[\\boldsymbol\\epsilon]=\\boldsymbol 0\\), and \\(\\mathrm{Var}[\\boldsymbol\\epsilon]= \\sigma^2 I_n\\) with \\(\\sigma&gt;0\\). (a). Show that the covariance matrix of the least squares estimates is diagonal if and only if the columns of \\(\\boldsymbol{X}\\), \\(\\boldsymbol{X}_1,\\dots,\\boldsymbol{X}_p\\), are orthogonal, that is \\(\\boldsymbol{X}_i^\\top \\boldsymbol{X}_j=0\\) for \\(i\\neq j\\). (b). Let \\(\\hat y_i\\) and \\(\\hat\\epsilon_i\\) be the fitted values and the residuals, respectively. Show that \\(n\\sigma^2 = \\sum_{i=1}^n \\mathrm{Var}[\\hat y_i]+\\sum_{i=1}^n\\mathrm{Var}[\\hat\\epsilon_i]\\). (c). Suppose further that \\(\\boldsymbol\\epsilon\\sim N(\\boldsymbol 0,\\sigma^2 I_n)\\). Use the generalized likelihood ratio method to test the hypothesis \\[H_0: \\beta_1=\\beta_2=\\dots=\\beta_{p-1}=0\\ vs.\\ H_1:\\sum_{i=1}^{p-1} \\beta_i^2\\neq0.\\] If the coefficient of determination \\(R^2=0.58\\), \\(p = 5\\) and \\(n=15\\), is the null rejected at the significance level \\(\\alpha =0.05\\)? (\\(F_{0.95}(4,10)=3.48,F_{0.95}(5,10)=3.33,t_{0.95}(10)=1.81\\)) "],
["anova.html", "第 5 章 方差分析 5.1 引言 5.2 单因子方差分析 5.3 两因子方差分析 5.4 小结", " 第 5 章 方差分析 5.1 引言 在工业生产或者实验中经常要比较若干个因素对业务指标的影响。比如，为了比较药物A,B,C对治疗某疾病的疗效，我们将实验对象分成三组，分别记录服用三种药物的治疗效果，得到三组样本 \\[X_1,\\dots,X_{n_1};\\ Y_1,\\dots,Y_{n_2};\\ Z_1,\\dots,Z_{n_3}.\\] 通过这些实验数据，我们希望回答： 这三种药物对治疗该疾病有没有显著差异； 如果有差异，哪种药物治疗效果最好？ 这个例子中，药物称为因子，A,B,C称为该因子的水平。 由于这个实验只涉及单个因子——“药物”，我们称之为单因子实验。此外，如果比较不同的药物和性别对疗效的影响，这就是两因子实验。不难推广到多因子实验。 本章介绍方差分析方法(Analysis of Variance, ANOVA)，研究因子不同水平的差异性，不同因子交互作用的显著性。这些研究有助于搭配有利于指标的不同因子的水平。虽然本章叫做方差分析，但实际上关心的是不同总体的均值比较，而不是它们的方差。 5.2 单因子方差分析 单因子实验设计(one-way layout)在一个因子的不同水平下分别进行独立的观测，是两个独立样本比较方法的推广。 模型假设：考虑一个因子A，有\\(r\\)个水平\\(A_1,\\dots,A_r\\), \\(r\\ge 2\\). 设在水平\\(A_i\\)下重复进行了\\(n_i\\)次实验(\\(n_i\\ge2\\))，数据是\\(y_{i1},y_{i2},\\dots,y_{in_i}\\). 假设这些数据之间相互独立且\\(y_{ij}\\sim N(\\mu_i,\\sigma^2)\\)，其中\\(\\sigma\\)未知。我们关心的问题是\\(\\mu_i\\)是否全相等，即要检验 \\[H_0:\\mu_1=\\dots=\\mu_r\\ vs.\\ H_1:\\mu_i\\text{不全相等}.\\] 令\\(n=\\sum_{i=1}^r n_i\\)， \\[\\bar y = \\frac{1}{n}\\sum_{i=1}^r\\sum_{j=1}^{n_i}y_{ij},\\ \\bar y_{i\\cdot}=\\frac 1{n_i}\\sum_{j=1}^{n_i}y_{ij},i=1,\\dots,r.\\] 这里\\(\\bar y\\)表示所有观测的平均值，\\(\\bar y_{i\\cdot}\\)表示水平\\(A_i\\)下的观测平均值。令 \\[S_T^2=\\sum_{i=1}^r\\sum_{j=1}^{n_i}(y_{ij}-\\bar y )^2,\\ S_e^2=\\sum_{i=1}^r\\sum_{j=1}^{n_i}(y_{ij}-\\bar y_{i\\cdot} )^2,\\] \\[S_A^2 = \\sum_{i=1}^r n_i(\\bar y_{i\\cdot} -\\bar y)^2.\\] 其中，\\(S_T^2\\)刻画全部数据的波动程度，\\(S_e^2\\)刻画组内数据的波动程度，\\(S_A^2\\)刻画不同组均值的差异引起的波动程度。这三者满足以下三角分解： \\[S_T^2 = S_e^2+S_A^2.\\] 这是由于 \\[\\begin{align*} S_T^2&amp;=\\sum_{i=1}^r\\sum_{j=1}^{n_i}(y_{ij}-\\bar y_{i\\cdot}+\\bar y_{i\\cdot}-\\bar y )^2\\\\ &amp;=\\sum_{i=1}^r\\sum_{j=1}^{n_i}(y_{ij}-\\bar y_{i\\cdot})^2+\\sum_{i=1}^r\\sum_{j=1}^{n_i}(\\bar y_{i\\cdot}-\\bar y )^2+2\\sum_{i=1}^r(\\bar y_{i\\cdot}-\\bar y )\\sum_{j=1}^{n_i}(y_{ij}-\\bar y_{i\\cdot})\\\\ &amp;=S_e^2+S_A^2. \\end{align*}\\] 这表明，总的平方和等于组内平方和加上组间平方和。注意到\\(\\bar y_{i\\cdot}\\)是\\(\\mu_i\\)的无偏估计，如果\\(H_0\\)成立，\\(\\bar y_{i\\cdot}\\)应该接近，那么\\(S_A^2\\)相对\\(S_e^2\\)小得多。也就意味着两者的比值\\(S_A^2/S_e^2\\)大到一定程度就有理由拒绝\\(H_0\\). 为给出确切的拒绝域， 我们需要知道在\\(H_0\\)成立下，\\((S_A^2, S_e^2)\\)的分布。 定理 5.1 考虑上述模型假设，有\\(S_e^2/\\sigma^2\\sim \\chi^2(n-r)\\)且\\(S_e^2\\)与\\(S_A^2\\)独立。如果\\(H_0:\\mu_1=\\dots=\\mu_r\\)成立，则有 \\[\\frac{S_A^2}{\\sigma^2}\\sim \\chi^2(r-1),\\ F=\\frac{S_A^2/(r-1)}{S_e^2/(n-r)}\\sim F(r-1,n-r).\\] 证明. 由单个正态总体的抽样分布定理有， \\[V_i:=\\frac{1}{\\sigma^2} \\sum_{j=1}^{n_i}(y_{ij}-\\bar y_{i\\cdot} )^2\\sim \\chi^2(n_i-1).\\] 由于\\(y_{ij}\\)之间独立， 所以\\(V_i\\)相互独立。由卡方分布的可加性，我们有\\(S_e^2/\\sigma^2=\\sum_{i=1}^r V_i\\sim \\chi^2(n-r)\\). 此外，\\(\\{V_1,\\dots,V_r\\}\\)与\\((\\bar y_{1\\cdot},\\dots,\\bar y_{r\\cdot})\\)独立。注意到\\(S_A^2\\)为\\(\\bar y_{i\\cdot}\\)的函数，所以\\(S_e^2\\)与\\(S_A^2\\)独立。 当\\(\\mu_1=\\dots=\\mu_r=\\mu\\)成立时，\\(\\bar y_{i\\cdot}\\stackrel{iid}{\\sim} N(\\mu,\\sigma^2/n_i)\\). 令\\(x_i = \\sqrt{n_i}(\\bar y_{i\\cdot}-\\mu)/\\sigma\\stackrel{iid}\\sim N(0,1)\\). 注意到， \\[\\begin{align*} S_A^2&amp;=\\frac{\\sum_{i=1}^rn_i(\\bar y_{i\\cdot} -\\bar y)^2}{\\sigma^2}\\\\ &amp;=\\frac{\\sum_{i=1}^r(\\sqrt{n_i}\\bar y_{i\\cdot} -\\sqrt{n_i}\\sum_{j=1}^r \\frac{n_j}{n}\\bar y_{j\\cdot} )^2}{\\sigma^2}\\\\ &amp;=\\frac{\\sum_{i=1}^r[\\sqrt{n_i}(\\bar y_{i\\cdot}-\\mu) -\\sqrt{n_i}\\sum_{j=1}^r \\frac{n_j}{n}(\\bar y_{j\\cdot}-\\mu) ]^2}{\\sigma^2}\\\\ &amp;=\\sum_{i=1}^r(x_i-\\sqrt{n_i}\\sum_{j=1}^r \\frac{\\sqrt{n_j}}{n}x_j)^2\\\\ &amp;=\\sum_{i=1}^rx_i^2-(\\sum_{i=1}^r\\sqrt{n_i/n} x_i)^2\\\\ &amp;=||x_{1{:}r}||^2-(\\alpha^\\top x_{1{:}r})^2, \\end{align*}\\] 其中\\(\\alpha=(\\sqrt{n_1/n},\\dots,\\sqrt{n_r/n})^\\top\\). 注意到\\(||\\alpha||=1\\)，参考定理1.4的证明，可以构造一个正交矩阵\\(A\\)使得\\(A\\)的第一行为\\(\\alpha^\\top\\)。令\\(z_{1{:}r}=Ax_{1{:}r}\\sim N(0,I_r)\\)，此时，\\(||z_{1{:}r}||^2=||x_{1{:}r}||^2\\), \\(z_1=\\alpha^\\top x_{1{:}r}\\). 所以， \\[S_A^2=||z_{1{:}r}||^2-z_1^2=\\sum_{i=2}^r z_i^2\\sim \\chi^2(r-1).\\] 由于\\(S_e^2\\)与\\(S_A^2\\)独立，所以\\(F\\sim F(r-1,n-r).\\) 为此，我们采用F检验，拒绝域为\\(W=\\{F&gt;F_{1-\\alpha}(r-1,n-r)\\}\\). 这种方法为方差分析法，是R. A. Fisher在1923年提出来的。在实践中常用以下方差分析表格。 来源 自由度 平方和 均方和 \\(F\\)值 P值 因子A \\(r-1\\) \\(S_A^2=\\sum_{i=1}^r n_i(\\bar y_{i\\cdot} -\\bar y)^2\\) \\(S_A^2/(r-1)\\) \\(\\frac{S_A^2/(r-1)}{S_e^2/(n-r)}\\) 误差 \\(n-r\\) \\(S_e^2=\\sum_{i=1}^r\\sum_{j=1}^{n_i}(y_{ij}-\\bar y_{i\\cdot} )^2\\) \\(S_e^2/(n-r)\\) 总和 \\(n-1\\) \\(S_T^2=\\sum_{i=1}^I\\sum_{j=1}^{n_i}(y_{ij}-\\bar y )^2\\) \\(S_T^2/(n-1)\\) 例 5.1 小白鼠在接种了三种不同类型的伤寒杆菌后的存活天数如下。判断小白鼠被注射三种菌型后的平均存活天数有没有显著差异？ 菌型 存 活 天 数 1 2 4 3 2 4 7 7 2 2 5 4 2 5 6 8 5 10 7 12 12 6 6 3 7 11 6 6 7 9 5 5 10 6 3 10 方差分析R的关键命令为aov(model,data....)与lm类似，详细如下： library(ggplot2) mouse &lt;- data.frame( X = c(2, 4, 3, 2, 4, 7, 7, 2, 2, 5, 4, 5, 6, 8, 5, 10, 7, 12, 12, 6, 6, 7, 11, 6, 6, 7, 9, 5, 5, 10, 6, 3, 10), A = factor(rep(1:3,c(11,10,12))) ) ggplot(mouse,aes(A,X,fill=A)) + geom_boxplot() 图 5.1: 箱线图 mouse.aov &lt;- aov(X~A, data=mouse) summary(mouse.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## A 2 94.3 47.1 8.48 0.0012 ** ## Residuals 30 166.7 5.6 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 从中发现，\\(p=0.0012\\)，在显著水平\\(\\alpha=0.05\\)下拒绝\\(H_0\\)，即认为注射三种菌型后的平均存活天数有显著差异。 5.3 两因子方差分析 两因子实验设计(two-way layout)研究两个因子在不同水平下的交互作用。 模型假设：考虑因子A有\\(r\\)个水平：\\(A_1,\\dots,A_r\\), \\(r\\ge 2\\)，因子B有\\(s\\)个水平：\\(B_1,\\dots,B_s\\), \\(s\\ge 2\\)。 设在水平\\((A_i,B_j)\\)下重复进行了\\(n_{ij}\\)次实验(\\(n_{ij}\\ge2\\))，数据是\\(y_{ij1},y_{ij2},\\dots,y_{ijn_{ij}}\\). 假设这些数据之间相互独立且\\(y_{ijk}\\sim N(\\mu_{ij} ,\\sigma^2)\\)，其中\\(\\sigma\\)未知。为分析各个因子对指标的影响，令 \\[\\mu = \\frac{1}{rs}\\sum_{i=1}^r\\sum_{j=1}^s \\mu_{ij},\\] \\[\\alpha_i = \\frac 1 s\\sum_{j=1}^s \\mu_{ij}-\\mu,\\ i=1,\\dots,r,\\] \\[\\beta_j = \\frac 1 r \\sum_{i=1}^r \\mu_{ij}-\\mu,\\ j=1,\\dots,s,\\] \\[\\lambda_{ij} = \\mu_{ij}-\\mu-\\alpha_i-\\beta_j.\\] 于是，模型可以表示为 \\[y_{ijk} = \\mu+ \\alpha_i+\\beta_j +\\lambda_{ij}+\\epsilon_{ijk},\\ \\epsilon_{ijk}\\stackrel{iid}{\\sim}N(0,\\sigma^2),\\] 其中\\(\\alpha_i\\)表示因子A的第\\(i\\)个水平\\(A_i\\)的“主效应”，\\(\\beta_j\\)表示因子B的第\\(j\\)个水平\\(B_j\\)的“主效应”，\\(\\lambda_{ij}\\)表示\\(A_i\\)和\\(B_j\\)下的交互作用的效应。 我们关心因子A或者因子B或者它们的交互作用（记为\\(A\\times B\\)）对指标有没有影响。对应的检验问题为 \\[H_0:\\alpha_1=\\dots=\\alpha_r=0\\ vs.\\ H_1:\\alpha_i\\text{不全为0},\\] \\[H_0:\\beta_1=\\dots=\\beta_s=0\\ vs.\\ H_1:\\beta_j\\text{不全为0},\\] \\[H_0:\\lambda_{11}=\\dots=\\lambda_{rs}=0\\ vs.\\ H_1:\\lambda_{ij}\\text{不全为0}.\\] 记\\(n_{i\\cdot} = \\sum_{j=1}^s n_{ij},\\ n_{\\cdot j} =\\sum_{i=1}^r n_{ij},\\ n = \\sum_{i=1}^r\\sum_{j=1}^s n_{ij}\\). 类似地，我们有相应的方差分析表： 来源 自由度 平方和 \\(F\\)值 P值 A \\(r-1\\) \\(S_A^2=\\sum_{i=1}^r n_{i\\cdot}(\\bar y_{i\\cdot\\cdot} -\\bar y)^2\\) \\(\\frac{S_A^2/(r-1)}{S_e^2(n-rs)}\\) B \\(s-1\\) \\(S_B^2=\\sum_{j=1}^s n_{\\cdot j}(\\bar y_{\\cdot j\\cdot} -\\bar y)^2\\) \\(\\frac{S_B^2/(s-1)}{S_e^2/(n-rs)}\\) \\(A\\times B\\) \\((r-1)(s-1)\\) \\(S_{AB}^2=\\sum_{i=1}^r\\sum_{j=1}^s n_{ij}(\\bar y_{ij\\cdot} -\\bar y)^2\\) \\(\\frac{S_{AB}^2/[(r-1)(s-1)]}{S_e^2/(n-rs)}\\) 误差 \\(n-rs\\) \\(S_e^2=\\sum_{i=1}^r\\sum_{j=1}^{s}\\sum_{k=1}^{n_{ij}}(y_{ijk}-\\bar y_{ij\\cdot})^2\\) 总和 \\(n-1\\) \\(S_T^2=\\sum_{i=1}^r\\sum_{j=1}^{s}\\sum_{k=1}^{n_{ij}}(y_{ijk}-\\bar y )^2\\) 例 5.2 研究树种与地理位置对松树生长的影响。以下有4个地区\\(B_1-B_4\\)，3种同龄松树\\(A_1-A_3\\)的直径测量数据（单位：厘米），请对实验结果做分析。 \\(A_1\\) \\(A_2\\) \\(A_3\\) \\(B_1\\) 23, 25, 21, 14, 15 28, 30, 19, 17, 22 18, 15, 23, 18, 10 \\(B_2\\) 20, 17, 11, 26, 21 26, 24, 21, 25, 26 21, 25, 12, 12, 22 \\(B_3\\) 16, 19, 13, 16, 24 19, 18, 19, 20, 25 19, 23, 22, 14, 13 \\(B_4\\) 20, 21, 18, 27, 24 26, 26, 28, 29, 23 22, 13, 12, 22, 19 tree &lt;- data.frame( Y = c(23, 25, 21, 14, 15, 28, 30, 19, 17, 22, 18, 15, 23, 18, 10, 20, 17, 11, 26, 21, 26, 24, 21, 25, 26, 21, 25, 12, 12, 22, 16, 19, 13, 16, 24, 19, 18, 19, 20, 25, 19, 23, 22, 14, 13, 20, 21, 18, 27, 24, 26, 26, 28, 29, 23, 22, 13, 12, 22, 19), A = gl(3,5,60,labels = paste0(&quot;A&quot;,1:3)), B = gl(4,15,60,labels = paste0(&quot;B&quot;,1:4)) ) tree.aov &lt;- aov(Y~A*B,tree) summary(tree.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## A 2 353 176.3 8.96 0.00049 *** ## B 3 88 29.2 1.48 0.23108 ## A:B 6 72 12.0 0.61 0.72289 ## Residuals 48 944 19.7 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Warning: package &#39;tidyverse&#39; was built under R version ## 3.5.3 ## Warning: package &#39;tibble&#39; was built under R version ## 3.5.3 ## Warning: package &#39;tidyr&#39; was built under R version ## 3.5.3 ## Warning: package &#39;readr&#39; was built under R version ## 3.5.3 ## Warning: package &#39;purrr&#39; was built under R version ## 3.5.3 ## Warning: package &#39;dplyr&#39; was built under R version ## 3.5.3 ## Warning: package &#39;stringr&#39; was built under R version ## 3.5.3 ## Warning: package &#39;forcats&#39; was built under R version ## 3.5.3 ## Warning: package &#39;kableExtra&#39; was built under R version ## 3.5.3 表 5.1: 三种树种的平均直径 A mean variance A1 19.55 20.37 A2 23.55 15.63 A3 17.75 22.09 由此可得，在显著水平\\(\\alpha=0.05\\)下，树种效应\\(A\\)是显著的，地理位置效应\\(B\\)及相互效应\\(A\\times B\\)并不显著。由于树种效应是显著的，所以选择什么树种对树的生长很重要，计算树种因子的平均值，如上表所示。从中可以看出，第二种树种对生长有利。同样地，我们可以计算地理位置因子的平均值，只不过该因子对生长有没有显著的影响。 5.4 小结 本章主要介绍了方差分析法来研究因子是否显著影响指标。如果发现某因子影响显著，我们可以进一步分析该因子的哪些水平存在差异性，哪些有利于指标。此时就要两两比较不同水平的差异性，如第三章提到的两样本t检验方法。 此外，本章考虑的是全面实验的情形，即各个因子的所有水平组合都安排实验，得到相应的观测数据。然而，当因子比较多，水平数量比较多时，这种全面实验就不合适了，因此需要合理设计实验，挑选出一些有代表性的组合进行实验，这属于实验设计的内容。感兴趣可以参考陈家鼎等编著的教材的第五章。 "],
["prob.html", "第 6 章 概率论基础 6.1 随机现象与随机试验 6.2 概率的定义 6.3 条件概率与独立性", " 第 6 章 概率论基础 6.1 随机现象与随机试验 6.1.1 随机试验、随机现象和随机事件 随机试验特点：可重复性、不可预知性 随机试验观测到的现象为随机现象 概率论与数理统计研究的对象 概率论研究随机现象的统计规律 数理统计研究随机现象的数据收集与分析 随机试验的某些可能结果组成的集合称为，简称事件 号 随机试验 事件A 事件B (1) 观测一部手机的通话时间 通话时间2h 通话时间2h (2) 观测一颗骰(tóu)子的点数 点数为奇数 点数为偶数 (3) 某新型药的治疗效果 治疗有效 治疗无效 6.1.2 样本空间与随机事件 样本空间：随机试验所有可能结果的集合称为样本空间。常用\\(\\Omega\\)表示。 样本点：样本空间的元素称为样本点，常用\\(\\omega\\)表示。 号 随机试验 样本空间 (1) 观测一部手机的通话时间 \\(\\Omega=[0,\\infty)\\) (2) 观测一颗骰(tóu)子的点数 \\(\\Omega=\\{1,2,3,4,5,6\\}\\) (3) 某新型药的治疗效果 \\(\\Omega=\\{\\text{治疗有效}, \\text{治疗无效}\\}\\) 有限样本空间：试验(2)和试验(3) 无限样本空间：试验(1) 随机事件是某些样本点组成的集合。 在一次试验中，当试验结果属于事件A时，称这次试验中。否则称。 件类型 记号 必然事件 \\(A=\\Omega\\) 不可能事件 \\(A=\\emptyset\\) 基本事件/简单事件 \\(A = \\{\\omega\\}\\), \\(\\omega\\in \\Omega\\) 复合事件 \\(A\\subseteq \\Omega\\) 例 6.1 在一批含有20件正品，5件次品的产品中随机地抽取2件，可能结果如下： A＝{2件全是正品} B＝{只有1件是正品} C＝{2件全是次品} 在不计次序的假定下，A、B、C是基本事件。 如果考虑次序，B不再是基本事件，它可分解为\\(B_1\\)和\\(B_2\\)两个基本事件。 \\(B_1\\)＝{第1次抽到正品，第2次是次品} \\(B_2\\)＝{第1次抽到次品，第2次是正品} 事件的关系： 包含: 如果事件A发生，事件B一定发生。则称事件B包含事件A。记为\\(A\\subset B\\)。显然\\(A\\subset\\Omega\\)。 相等: \\(A=B\\Leftrightarrow A\\subset B\\text{且}B\\subset A\\) 互斥: \\(A\\cap B = \\varnothing\\) 对立：\\(A\\cap B = \\varnothing\\)且\\(A\\cup B=\\Omega\\)，记\\(A = \\bar{B}\\)或者\\(B = \\bar{A}\\) 例 6.2 考虑投骰子的例子：\\(A=\\){出现偶数点}，\\(B=\\){出现奇数点}，\\(C=\\){出现点数1和3}，\\(D=\\){出现点数5和6}, \\(E=\\){出现点数大于4} 事件A和B的关系? 事件B和C的关系? 事件C和D的关系? 事件D和E的关系? 事件的运算：交、并、差 \\(A\\cap B\\)或者AB，事件A和B同时发生 \\(A\\cup B\\)，事件A和B至少一个发生。若\\(A\\)和\\(B\\)互斥, 则\\(A\\cup B\\)可表示为\\(A+B\\)。 \\(A\\backslash B\\)，或表示为\\(A-B\\)，事件A发生但事件B不发生 多个事件的交\\(\\bigcap_{i=1}^n A_i\\)，多个事件的并 \\(\\bigcup_{i=1}^n A_i\\) 韦恩图 例 6.3 设\\(A\\)、B、C为任意三个事件，写出下列事件的表达式： AC都发生B不发生。 恰有二个事件发生。 至少有一个事件发生。 三个事件同时发生。 三个事件发生。 三个事件发生。 事件的运算法则: 对于任意三个事件A,B,C，满足下列运算： \\(AB=BA\\), \\(A\\cup B=B\\cup A\\) \\((AB)C=A(BC)\\), \\((A\\cup B)\\cup C=A\\cup (B\\cup C)\\) \\(A(B\\cup C)=AB\\cup AC\\), \\(A\\cup (B\\cap C)=(A\\cup B)\\cap (A\\cup C)\\) \\(\\overline{A\\cup B}=\\bar{A}\\cap \\bar{B}\\), \\(\\overline{A\\cap B}=\\bar{A}\\cup \\bar{B}\\) \\[\\overline{\\bigcup_{i=1}^n A_i}=\\bigcap_{i=1}^n \\bar{A}_i,\\quad \\overline{\\bigcap_{i=1}^n A_i}=\\bigcup_{i=1}^n \\bar{A}_i\\] 6.2 概率的定义 频率的定义：设事件\\(A\\)在\\(n\\)次试验中出现了\\(r\\)次，则比值 \\(r/n\\)称为事件\\(A\\)在\\(n\\)次试验中出现的频率。 概率的统计定义：在同一组条件下所作的大量重复试验中，事件\\(A\\)出现的频率总是在区间\\([0,1]\\)上的一个确定的常数\\(p\\)附近摆动，并且稳定于\\(p\\)（频率的稳定值），则\\(p\\)称为事件\\(A\\)的概率，记作\\(P(A)\\)。 注意：\\(P(\\)这里面只能为事件！\\()\\)。如，\\(P(\\)骰子的点数\\()=1/6\\)是错误的。 确定概率的古典方法。古典概型的随机试验要求满足下两条件： 有限性。只有有限多个不同的基本事件。 等可能性。每个基本事件出现的可能性相等。 在古典概型中，如果基本事件（样本点）的总数为\\(n\\)，事件\\(A\\)所包含的基本事件（样本点）个数为\\(r(r\\le n)\\)，则定义事件\\(A\\)的概率\\(P(A)\\)为\\(r/n\\)。即 \\[P(A)=\\frac{r}{n}=\\frac{A\\mathrm{中包含的基本事件的个数}}{\\mathrm{基本事件的总数}}.\\] 简单例子：抛均匀的硬币、骰子 思考： 古典概型的取值范围？ 例 6.4 (抽球类型) 袋中有\\(a\\)个黄球，\\(b\\)个白球，从中接连任意取出\\(k\\)个球\\((k\\leq a+b)\\)，且每次取出的球不再放回去，求第\\(k\\)次取出的球是黄球的概率？ 解. 设事件A={第\\(k\\)次取出的球是黄球}。 解法一：考虑第1次到第\\(k\\)次的取球结果。 \\[P(A)=\\frac{C_a^1A_{a+b-1}^{k-1}}{A_{a+b}^{k}}=\\frac{a}{a+b}.\\] 解法二：只考虑第\\(k\\)次的取球结果。 \\[P(A)=\\frac{C_a^1}{C_{a+b}^{1}}=\\frac{a}{a+b}\\] 结论：抽签与顺序无关（如同有放回的情况）！基本事件是相对的！ 例 6.5 (m个质点在n个格子中的分布问题) 设有\\(m\\)个不同质点，每个质点都以概率\\(1/n\\)落入\\(n\\)个格子(\\(n\\ge m\\))的每一个之中，求下列事件的概率： A: 指定\\(m\\)个格子中各有一个质点； B: 任意\\(m\\)个格子中各有一个质点； C: 指定的一个格子中恰有\\(k(k\\le m)\\)个质点。 解. \\[\\begin{aligned} P(A) &amp;= \\frac{m!}{n^m}\\\\ P(B) &amp;= \\frac{C_n^mm!}{n^m}=\\frac{A_n^m}{n^m}\\\\ P(C) &amp;= \\frac{C_m^k(n-1)^{m-k}}{n^m} \\end{aligned}\\] 例 6.6 (同一天生日问题) 某班级有\\(n\\)个人，问至少有两个人的生日在同一天的概率为多大？（假设一年有365天） 解. 记\\(A=\\){\\(n\\)个人中至少有两个人的生日相同}。\\(\\bar{A}=\\){\\(n\\)个人中的生日全不相同}, 易知 \\[P(\\bar{A})=\\frac{A_{365}^n}{365^n}=\\frac{365!}{365^n(365-n)!}.\\] 因此， \\[P(A)=1-P(\\bar{A})=1-\\frac{365!}{365^n(365-n)!}.\\] 上述只是对\\(n\\le 365\\)成立。如果\\(n&gt;365\\)，则显然\\(P(A)=1\\). 联系生日攻击问题 概率的几何定义：平面上有可测的区域\\(G\\)和\\(g\\)，向\\(G\\)中随机投掷一点\\(M\\)，设\\(M\\)必落在\\(G\\)内。如\\(M\\)落在\\(g\\)内的概率只与\\(g\\)的面积成正比，而与\\(g\\)的位置和形状无关。这样的随机实验，称为几何概型。点\\(M\\)落入\\(G\\)内的部分区域\\(g\\)的概率为： \\[P = \\frac{g\\text{的面积}}{G\\text{的面积}}.\\] 注意：随机投点是指\\(M\\)落入\\(G\\)内任一处均是等可能的。 思考： 几何概型的取值范围？ 例 6.7 (会面问题) 已知甲乙两船将在同一天的0点到24点之间随机地到达码头，该码头只有一个泊位。若甲先到达，需停靠6小时后才离开码头。若乙先到达，则要停靠8小时后才离开码头。问这两船中有船需等候泊位空出的概率? 解. 设甲船到达码头的时刻是\\(x\\)，乙船到达码头的时刻是\\(y\\)，显然\\(0\\le x,y\\le 24\\)。若这两船中有船需等候泊位空出，则\\(y-x&lt; 6\\)且\\(x-y&lt; 8\\). \\[P(A) = \\frac{24^2-(16^2+18^2)/2}{24^2}\\approx 0.4965.\\] 例 6.8 (蒲丰投针实验) 蒲丰是几何概率的开创者，并以蒲丰投针问题闻名于世，发表在其1777年的论著《或然性算术试验》中。 由于通过他的投针试验法可以利用很多次随机投针试验算出\\(\\pi\\)的近似值，所以特别引人瞩目，这也是最早的几何概 率问题。并且蒲丰本人对这个实验给予证明。1850年，瑞士数学家沃尔夫在苏黎世，用一根长\\(36mm\\)的针，平行线间距为\\(45mm\\)，投掷\\(5000\\)次，得\\(\\pi\\approx3.1596\\)。1864年，英国人福克投掷了\\(1100\\)次，求得\\(\\pi\\approx3.1419\\)。1901年，意大利人拉泽里尼投掷了\\(3408\\)次，得到了准确到6位小数的\\(\\pi\\)值。 George-Louis Leclerc de Buffon (1707.9.7-1788.4.16)，法国数学家、自然科学家。 平面上画着一些平行线，它们之间的距离等于\\(a\\),向此平面任投长度为\\(\\ell(\\ell&lt;a)\\) 的针，试求此针与任一平行线相交的概率。 解. 设\\(x\\)表示针的中点到最近的一条平行线的距离，\\(\\theta\\)表示针与平行线的交角。显然\\(0\\le x\\le a/2\\), \\(0\\le \\theta\\le \\pi\\). 为使针与平行线相交，必须\\(x\\le \\frac{\\ell}{2}\\sin\\theta\\). 所求的概率为： \\[\\frac{\\frac \\ell 2\\int_0^\\pi\\sin\\theta d\\theta }{\\frac 1 2\\pi a}=\\frac{2\\ell}{\\pi a}.\\] 若取\\(\\ell = a/2\\)，则\\(\\pi = 1/P.\\) \\(\\blacktriangleright\\)模拟动画 如果是三角形呢？知道边长分别为\\(\\ell_1,\\ell_2,\\ell_3\\). 统计界的贝叶斯学派认为：一个事件的概率是人们根据经验对该事件发生的可能性所给出的个人信念。 天气预报说“明天下雨的概率为90%” 医生说“手术成功可能性为90%” 我说“考试及格的可能性为99%” 注意区别“主观概率”与“主观臆造”。 前面学了三种概率定义，各有其局限性。 统计概率：要求作大量重复试验。很难观测到稳定值 古典概率：试验结果要求有限、互不相容、等可能。不均匀的硬币 几何概率：落入区域G内任一点是等可能的。高手投飞镖 如何刻画更一般的情况？1900年数学家希尔伯特(Hilbert, 1862–1943)提出要建立概率的公理化定义来解决这个问题，即以最少的几条本质特征出发去刻画概率的概念。1933年苏联数学家柯尔莫戈夫(Kolmogorov, 1903–1987)首次提出了概率的公理化定义，这个定义既概括了历史上几种概率的共同特征，又避免了各自的局限性和含混之处，不管什么随机现象，只要满足该定义的三条公理，才能说它是概率。 6.2 概率的公理化定义 样本空间\\(\\Omega\\)，事件域\\(\\mathcal{F}\\)，概率测度\\(P:\\mathcal{F}\\to [0,1]\\)。由三元组\\((\\Omega,\\mathcal{F},P)\\)定义一个概率空间。 事件域\\(\\mathcal{F}\\)是由样本空间的一些子集构成的集合，并满足以下条件： \\(\\Omega\\in \\mathcal{F}\\); 如果\\(A\\in\\mathcal{F}\\), 则\\(\\bar{A}\\in \\mathcal{F}\\); 如果\\(A_n\\in\\mathcal{F}\\), \\(n=1,\\dots,\\infty\\), 则\\(\\bigcup_{n=1}^\\infty A_n\\in \\mathcal{F}\\). 试证明：\\(\\varnothing \\in \\mathcal{F}\\). 如果\\(A_n\\in\\mathcal{F}\\), \\(\\bigcap_{n=1}^\\infty A_n\\in \\mathcal{F}\\). 概率的公理化定义：样本空间\\(\\Omega\\)，事件域\\(\\mathcal{F}\\)，概率测度\\(P:\\mathcal{F}\\to [0,1]\\)。由三元组\\((\\Omega,\\mathcal{F},P)\\)定义一个概率空间。 概率测度\\(P(A)\\)度量事件\\(A\\in \\mathcal{F}\\)发生的概率，满足下面三个公理。 公理1(非负性)： \\(\\forall A\\in \\mathcal{F}\\), \\(0\\le P(A)\\); 公理2(规范性)： \\(P(\\Omega)=1\\); 公理3(可列可加性)： 对可列个两两互斥的事件\\(A_1,A_2,\\dots,A_n,\\dots\\)有 \\[P\\left(\\bigcup_{i=1}^\\infty A_i\\right)=\\sum_{i=1}^{\\infty}P(A_i).\\] 思考：古典概率和几何概率是否符合该公理化定义？ 概率的性质： \\(P(\\varnothing)=0\\) 如果\\(A,B\\)互斥，则\\(P(A+B)=P(A)+P(B)\\). 单调性：如果\\(A\\subset B\\),则\\(P(A)\\le P(B)\\). \\(P(\\bar{A})=1-P(A)\\) \\(P(A\\backslash B)=P(A)-P(AB)\\) \\(P(A\\cup B)=P(A)+P(B)-P(AB)\\) 上连续性：如果\\(A_1\\supset A_2\\supset \\cdots\\supset A_n\\cdots\\)，则 \\[P\\left(\\bigcap_{i=1}^\\infty A_i\\right)=\\lim_{i\\to\\infty}P(A_i).\\] 下连续性：如果\\(A_1\\subset A_2\\subset \\cdots\\subset A_n\\cdots\\)，则 \\[P\\left(\\bigcup_{i=1}^\\infty A_i\\right)=\\lim_{i\\to\\infty}P(A_i).\\] 半可加性： 对任意两个事件\\(A\\), \\(B\\), 有 \\[P(A\\cup B)\\le P(A)+P(B).\\] 对任意\\(n\\)个事件\\(A_1,A_2,\\dots,A_n\\), 有 \\[P(\\cup_{i=1}^nA_i)\\le \\sum_{i=1}^{n}P(A_i).\\] 例 6.9 证明下列命题： 若\\(A_1\\)与\\(A_2\\)同时发生时\\(A\\)发生，则有\\(P(A)\\ge P(A_1)+P(A_2)-1\\). 证明：因为\\(A_1A_2\\subset A\\)，则\\(P(A)\\ge P(A_1A_2)\\). 又 \\[\\begin{aligned} P(A_1A_2)&amp;=P(A_1)+P(A_2)-P(A_1\\cup A_2)\\notag\\\\&amp;\\ge P(A_1)+P(A_2)-1.\\label{eq:1} \\end{aligned}\\] 若\\(A_1A_2A_3\\subset A\\)，则有\\(P(A)\\ge P(A_1)+P(A_2)+P(A_3)-2\\). 证明：由第一题结论得到\\(P(A)\\ge P(A_1A_2)+P(A_3)-1\\). 再由??可得所需结果。 思考：若\\(\\bigcap_{i=1}^nA_i\\subset A\\)，则\\(P(A)\\ge\\sum_{i=1}^nP(A_i)-n+1\\). 6.3 条件概率与独立性 条件概率是指在某事件\\(B\\)发生的前提下，求另一事件\\(A\\)的概率，记为\\(P(A|B)\\)。 例 6.10 \\(A=\\){抽到号码为偶数的橙色球}, \\(B=\\){抽到号码小于4的球}. \\[\\begin{aligned} P(A)&amp;=\\frac{4}{12}\\\\ P(B)&amp;=\\frac 6 {12}\\\\ P(AB)&amp;=\\frac{1}{12}\\\\ P(A|B)&amp;= \\frac 1 6 = \\frac{1/12}{6/12} = \\frac{P(AB)}{P(B)}. \\end{aligned}\\] 不妨考虑更一般的古典概型和几何概型下的条件概率。 定义 6.1 对于事件\\(A,B\\), 若\\(P(B)&gt;0\\), 则称 \\(P(A|B)=\\frac{P(AB)}{P(B)}\\) 为事件\\(A\\)在事件\\(B\\)发生下的条件概率。 条件概率也是概率。 条件概率满足所有概率性质。验证三个公理 条件概率空间\\((\\Omega,\\mathcal{F},P(\\cdot|B))\\) 若\\(A\\subset B\\), 则\\(P(A|B)\\ge P(A)\\). 若\\(B\\subset A\\), 则\\(P(A|B)=1\\). 若\\(AB=\\varnothing\\), 则\\(P(A|B)=0\\). 乘法公式：如果\\(P(B)&gt;0\\), 则有 \\[P(AB)=P(B)P(A|B).\\] 更一般地，对于\\(n\\)个事件\\(A_1,\\dots,A_n\\), 如果\\(P(A_1\\cdots A_{n-1})&gt;0\\), 则有 \\[P(A_1\\cdots A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\\cdots P(A_n|A_1\\cdots A_{n-1}).\\] 例 6.11 为安全起见，工厂同时装有两套报警系统1，2。已知每套系统单独使用时能正确报警的概率分别为0.92和0.93，又已知第一套系统失灵时第二套系统仍能正常工作的概率为0.85。试求该工厂在同时启用两套报警系统时，能正确报警的概率是多少？ 解. 设事件\\(A_i\\)={第\\(i\\)套系统能正常工作}，\\(i=1,2\\). 即求\\(P(A_1\\cup A_2)\\). 依题意有，\\(P(A_1)=0.92\\), \\(P(A_2)=0.93\\), \\(P(A_2|\\bar{A}_1)=0.85\\). 因此， \\[\\begin{aligned} P(A_1\\cup A_2) &amp;=P(A_1)+P(A_2\\bar{A}_1)\\\\ &amp;=P(A_1)+P(A_2|\\bar{A}_1)P(\\bar{A}_1)\\\\ &amp;=P(A_1)+P(A_2|\\bar{A}_1)(1-P(A_1))\\\\ &amp;=0.988. \\end{aligned}\\] 例 6.12 对某种产品要依次进行三项破坏性试验。已知产品不能通过第一项试验的概率是0.3；通过第一项而通不过第二项试验的概率是0.2；通过了前两项试验却不能通过最后一项试验的概率是0.1。试求产品未能通过破坏性试验的概率？ 例 6.13 设事件\\(A_i\\)={通过第\\(i\\)项试验}，\\(i=1,2,3\\). 即求\\[P(\\overline{A_1 A_2A_3})=1-P(A_1A_2A_3).\\] 依题意有，\\(P(\\bar{A}_1)=0.3\\), \\(P(\\bar{A}_2|A_1)=0.2\\), \\(P(\\bar{A}_3|A_1A_2)=0.1\\). 由乘法公式得， \\[\\begin{aligned} P(A_1A_2A_3) &amp;= P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\\\\ &amp;=(1-0.3)\\times(1-0.2)\\times(1-0.1)\\\\ &amp;=0.504. \\end{aligned}\\] 所以，\\(P(\\overline{A_1 A_2A_3})=1-0.504=0.496\\). 例 6.14 一个人依次进行四次考试，他第一次考试及格的概率为\\(p(0&lt;p&lt;1)\\)，**又若他前一次考试及格，则本次考试的及格率为\\(p\\)，若前一次考试不及格，则本次考试的及格率为\\(p/2\\)，如果他至少要有三次考试及格，才能认为考试合格，问他能考试合格的概率有多大? 解. 设\\(A_i=\\){第\\(i\\)次考试及格}, \\(i=1,2,3,4\\). 则事件\\(B=\\){考试合格}可表示为 \\[B=A_1A_2A_3+\\bar{A}_1A_2A_3A_4+A_1\\bar{A}_2A_3A_4+A_1A_2\\bar{A}_3A_4.\\] \\[\\begin{aligned} P(A_1A_2A_3) &amp;= P(A_1)P(A_2|A_1)P(A_3|A_1A_2)=p^3\\\\ P(\\bar{A}_1A_2A_3A_4) &amp;= P(\\bar{A}_1)P(A_2|\\bar{A}_1)P(A_3|A_2\\bar{A}_1)P(A_4|A_3A_2\\bar{A}_1)=(1-p)p^3/2\\\\ P(A_1\\bar{A}_2A_3A_4)&amp;=P(A_1)P(\\bar{A}_2|A_1)P(A_3|A_1\\bar{A}_2)P(A_4|A_3\\bar{A}_2A_1)=(1-p)p^3/2\\\\ P(A_1A_2\\bar{A}_3A_4)&amp;=P(A_1)P(A_2|A_1)P(\\bar{A}_3|A_1A_2)P(A_4|\\bar{A}_3A_2A_1)=(1-p)p^3/2.\\end{aligned}\\] 因此，\\(P(B)=3(1-p)p^3/2+p^3=(5-3p)p^3/2\\).** 例 6.15 (敏感性问题调查) 考虑以下问题： 问题\\(A\\): 你的生日是否在7月1日之前？ 问题\\(B\\): 你是否逃过课？ 问卷调查 全概率公式(原因推结果) \\(B_1,B_2,\\dots,B_n\\)为一组两两互斥的事件，且 \\(B_1+B_2+\\dots+B_n=\\Omega\\); \\(P(B_i)&gt;0\\), \\(i=1,\\dots,n\\), 我们称这些\\(B_i\\)为样本空间的一个划分。 对任意事件\\(A\\)有 \\[P(A)=\\sum_{i=1}^nP(A|B_i)P(B_i).\\] 注： 该定理可以推广到可列多个的情况 \\(B_i\\)视为导致事件\\(A\\)(结果)发生的&quot;原因&quot; 例 6.16 (敏感性问题调查（续）) - 问题\\(A\\): 你的生日是否在7月1日之前？ 问题\\(B\\): 你是否在考试中做过弊？ \\(n\\)个人参与调查(\\(n\\)充分大)，通过抛硬币决定回答哪个问题。若其中\\(k\\)个人回答“是”。作弊的比例大概为： \\[p\\approx 2k/n-0.5.\\] 例 6.17 一商店出售的某型号的晶体管是甲、乙、丙三家工厂生产的，其中乙厂产品占总数的\\(50\\%\\)，另两家工厂的产品各占\\(25\\%\\)。已知甲、乙、丙各厂产品合格率分别为0.9、0.8、0.7，试求随意取出一只晶体管是合格品的概率（此货合格率）。 解. 设\\(A=\\){取出一只晶体管是合格品}，\\(B_1=\\){取出一只晶体管来自甲产}，\\(B_2=\\){取出一只晶体管来自已产}，\\(B_3=\\){取出一只晶体管来自丙产}。由全概率公式得： \\[P(A)=\\sum_{i=1}^3P(A|B_i)P(B_i)=0.9\\times 25\\%+0.8\\times 50\\%+0.7\\times 25\\%=0.8.\\] 例 6.18 设甲袋中有\\(m-1\\)只白球和1只黑球，乙袋中有\\(m\\)只白球，每次从甲、乙两袋中分别取出一只球，经交换后放回袋中，求经\\(n\\)次交换后，黑球在甲袋中的概率，并讨论\\(n\\to\\infty\\)时的情形。 解. 设\\(A_i=\\){通过\\(i\\)次交换后黑球在甲袋中}，\\(p_i=P(A_i)\\). 则 \\(p_1=(m-1)/m\\). 依题意， \\(P(A_i|A_{i-1})=(m-1)/m\\)，\\(P(A_i|\\bar{A}_{i-1})=1/m\\). 由全概率公式得， \\[\\begin{aligned} p_i&amp;=P(A_i|A_{i-1})P(A_{i-1})+P(A_i|\\bar{A}_{i-1})P(\\bar{A}_{i-1})\\\\&amp;=\\frac{m-1}{m}p_{i-1}+\\frac{1}{m}(1-p_{i-1})\\\\ &amp;=\\frac{m-2}{m}p_{i-1}+\\frac{1}{m}.\\end{aligned}\\] 所以， \\[p_n = \\left(\\frac{m-2}{m}\\right)^{n-1}p_1+\\frac{1}{m}\\sum_{i=0}^{n-2}\\left(\\frac{m-2}{m}\\right)^i=\\frac{1}{2}+\\frac{1}{2}\\left(\\frac{m-2}{m}\\right)^n\\to 1/2.\\] 例 6.19 (水果糖问题) 两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。**请问这颗水果糖来自哪个碗的可能性大？ image 它是贝叶斯定理的应用。英国数学家托马斯贝叶斯(Thomas Bayes)在1793年发表的一篇论文中，首先提出了这个定理。 贝叶斯(Bayes)公式(结果推原因) 设\\(B_1,B_2,\\dots,B_n\\)为一组两两互斥的事件，且 \\(B_1+B_2+\\dots+B_n=\\Omega\\); \\(P(B_i)&gt;0\\), \\(i=1,\\dots,n\\), 则对任意一个具有正概率的事件\\(A\\)有 \\[\\bf{P(B_k|A)=P(B_k)\\frac{P(A|B_k)}{P(A)}=\\frac{P(A|B_k)P(B_k)}{\\sum_{i=1}^nP(A|B_i)P(B_i)}}.\\] 几点说明： 该定理可以推广到可列多个的情况。 贝叶斯公式广泛用于统计推断：通过观测到的实验数据来推测模型参数，即由结果推断成因。 \\(B_k\\)可视为“因”，\\(A\\)视为“果”，\\(P(B_k)\\)称为&quot;先验概率&quot;，即在事件A发生之前，我们对事件\\(B_k\\)发生概率的一个判断；\\(P(B_k|A)\\)称为&quot;后验概率&quot;，即在事件A发生后，我们对事件\\(B_k\\)的重新评估。**** 例 6.20 (水果糖问题（续）) 两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。**求这颗水果糖来自一号碗的概率？ image 设\\(H_1\\)表示一号碗，\\(H_2\\)表示二号碗。则\\(P(H_1)=P(H_2)\\)，也就是说，再取出水果糖之前，这两个碗被选中的概率相同。因此，\\(P(H_1)=0.5\\)，我们把这个概率叫做“先验概率&quot;，即没有做实验之前，来自一号碗的概率是0.5。再假定，\\(E\\)表示水果糖，所以问题就变成了在已知\\(E\\)的情况下，来自一号碗的概率有多少？即求\\(P(H_1|E)\\)。我们把这个概率叫做”后验概率&quot;，即在事件\\(E\\)发生之后，对\\(P(H_1)\\)的修正。由贝叶斯公式可求\\[P(H_1|E)=\\frac{P(H_1)P(E|H_1)}{P(H_1)P(E|H_1)+P(H_2)P(E|H_2)}=\\frac{0.5\\times(3/4)}{0.5\\times(3/4)+0.5\\times(1/2)}=0.6,\\]也就是说，取出水果糖之后，\\(H_1\\)事件的可能性得到了增强。 例 6.21 (三门问题(Monty Hall problem)) 参赛者会看见三扇关闭了的门，其中一扇的后面有一辆汽车，选中后面有车的那扇门可赢得该汽车，另外两扇门后面则各藏有一只山羊。当参赛者选定了一扇门，但未去开启它的时候，节目主持人开启剩下两扇门的其中一扇，露出其中一只山羊。主持人其后会问参赛者要不要换另一扇仍然关上的门。问题是：换另一扇门会否增加参赛者赢得汽车的机会率？ 解. 设\\(A=\\){主持人选中山羊}，\\(B=\\){参赛者第一次选中山羊}，\\(C=\\){参赛者第二次选中汽车}。所求为\\(P(C|A)\\). 若参赛者换另一扇门，则\\(P(C|A)=P(B|A)\\)，否则 \\(P(C|A)=P(\\bar{B}|A)=1-P(B|A)\\). 已知\\(P(B)=2/3\\), \\(\\bf{P(A|\\bar{B})=1}\\). 由贝叶斯公式得 \\[P(B|A)=\\frac{P(A|B)P(B)}{P(A|B)P(B)+\\bf{P(A|\\bar{B})}P(\\bar{B})}=\\frac{\\bf{P(A|B)}(2/3)}{\\bf{P(A|B)}(2/3)+1/3}.\\] 假定1：主持人知道汽车的位置。如果参赛者第一次没选中汽车，主持人必定开启藏山羊的门。即\\(P(A|B)=1\\). 所以，换门中奖概率\\(P(B|A)=2/3\\). 假定2：主持人随机打开一扇门。则\\(P(A|B)=1/2\\). 所以，换门中奖概率\\(P(B|A)=1/2\\). 例 6.22 两台机床加工同样的零件，第一台出现废品的概率为\\(0.05\\)，第二台出现废品的概率为\\(0.02\\)，加工的零件混放在一起，若第一台车床与第二台车床加工的零件数为\\(5:4\\)。求 任意地从这些零件中取出一个合格品的概率； 若已知取出的一个零件为合格品，那么，它是由哪一台机床生产的可能性较大。 解. 设\\(A=\\){取出的零件合格}，\\(B=\\){取出的零件来自第一台车床}。依题意有，\\(P(B)=5/9\\), \\(P(A|B)=0.95\\), \\(P(A|\\bar{B})=0.98\\). 由全概率公式得 \\[P(A) = P(A|B)P(B)+P(A|\\bar{B})P(\\bar{B})=0.95\\times \\frac{5}{9}+0.98\\times \\frac{4}{9}=\\frac{867}{900}.\\] 由贝叶斯公式得，第一台车床生产的可能性 \\[P(B|A)=\\frac{P(A|B)P(B)}{P(A)}=\\frac{475}{867}.\\] 第二台车床生产的可能性为\\(P(\\bar{B}|A)=1-P(B|A)=\\frac{392}{867}.\\) 例 6.23 某实验室在器皿中繁殖成\\(k\\)个细菌的概率为 \\[p_k = \\frac{\\lambda^k}{k!}e^{-\\lambda}, \\lambda&gt;0, k=1,2,\\dots.\\] 并设所繁殖的每个细菌为甲类菌或乙类菌的概率相等，求下列事件的概率： 器皿中所繁殖的全部是甲类菌的概率； 已知所繁殖的全部是甲类菌，求细菌个数为2的概率； 求所繁殖的细菌中有\\(i\\)个甲类菌的概率。 解. (1) 设\\(A=\\){所繁殖的全部是甲类菌}，\\(B_i=\\){器皿中繁殖成\\(i\\)个细菌}。由全概率公式得 \\[P(A)=\\sum_{k=1}^\\infty P(A|B_k)P(B_k)=\\sum_{k=1}^\\infty (1/2)^kp_k=e^{-\\lambda}\\sum_{k=1}^\\infty\\frac{(\\lambda/2)^k}{k!}=e^{-\\lambda}(e^{\\lambda/2}-1).\\] 由贝叶斯公式得 \\[P(B_2|A)=\\frac{P(A|B_2)P(B_2)}{P(A)}=\\frac{(1/2)^2p_2}{e^{-\\lambda}(e^{\\lambda/2}-1)}=\\frac{\\lambda^2}{8(e^{\\lambda/2}-1)}.\\] 设\\(A_i=\\){所繁殖的细菌中有\\(i\\)个甲类菌}，\\(B_i=\\){器皿中繁殖成\\(i\\)个细菌}。则有对于\\(k\\ge i\\), \\(P(A_i|B_k)=C_k^i/2^k\\). 由全概率公式得 \\[\\begin{aligned} P(A_i)&amp;=\\sum_{k=i}^\\infty P(A_i|B_k)P(B_k)=\\sum_{k=i}^\\infty C_k^i2^{-k}p_k\\\\&amp;=e^{-\\lambda}\\sum_{k=i}^\\infty\\frac{(\\lambda/2)^k}{i!(k-i)!} =e^{-\\lambda}\\frac{(\\lambda/2)^i}{i!}\\sum_{k=i}^\\infty\\frac{(\\lambda/2)^{k-i}}{(k-i)!} \\\\&amp;=\\frac{\\lambda^i}{2^ii!}e^{-\\lambda/2}. \\end{aligned}\\] 定义 6.2 若两事件\\(A,B\\)满足\\(P(AB)=P(A)P(B)\\)，则称事件\\(A,B\\)(或\\(B,A\\))相互独立。简称独立。 注：必然事件及不可能事件与任何事件均是独立的。 如果\\(P(A)&gt;0\\), 则\\(P(B|A)=P(B)\\Leftrightarrow\\)事件\\(A,B\\)相互独立。 如果\\(P(B)&gt;0\\), 则\\(P(A|B)=P(A)\\Leftrightarrow\\)事件\\(A,B\\)相互独立。 若事件对\\(A,B\\)；\\(A,\\bar{B}\\)；\\(\\bar{A},B\\)；\\(\\bar{A},\\bar{B}\\)中有一对相互独立则其它三对亦相互独立。换言之，这四对事件要么都相互独立；要么都不独立。 定义 6.3 (有限个事件独立) 设\\(A_1,A_2,\\dots,A_n\\)为\\(n\\)个事件，\\(\\mathcal{I}\\subset\\{1,2,\\dots,n\\}\\)为一个集合。 若对所有的非空集合\\(\\mathcal{I}\\)均满足， \\[P\\left(\\bigcap_{i\\in \\mathcal{I}}A_i\\right)=\\prod_{i\\in \\mathcal{I}}P(A_i),\\] 则称\\(A_1,A_2,\\dots,A_n\\)相互独立。 设\\(n\\)个事件\\(A_1,A_2,\\dots,A_n\\)相互独立，那么，把其中任意\\(m(1\\le m\\le n)\\)个事件相应换成它们的对立事件，则所得的\\(n\\)个事件仍然相互独立。 注意：\\(A_1,A_2,\\dots,A_n\\)相互独立可以推出它们两两独立，反之不能！反例？ 例 6.24 (一个反例) 同时抛掷两个四面体，每个四面体的四个面分别标有1、2、3、4。定义事件 \\(A=\\){第一个四面体出现偶数}；\\(B=\\){第二个四面体出现奇数}；\\(C=\\){两个四面体同时出现奇数或同时出现偶数}。 不难发现， \\[\\begin{aligned} P(A)&amp;=P(B)=P(C)=1/2\\\\ P(AB)&amp;=P(AC)=P(BC)=1/4\\\\ P(ABC)&amp;=0. \\end{aligned}\\] 因此，\\(A\\)与\\(B\\)相互独立，\\(A\\)与\\(C\\)相互独立，\\(B\\)与\\(C\\)相互独立，但\\(A,B,C\\)不相互独立。 例 6.25 设某型号的高射炮发射一发炮弹击中飞机的概率为\\(0.6\\)，现在用此型号的炮若干门同时各发射一发炮弹，问至少需要设置几门高射炮才能以不小于\\(0.99\\)的概率击中来犯的敌机（可以认为各门高射炮的射击相互独立）？ 解. \\(A_i=\\){第\\(i\\)门炮击中敌机}。\\(A_1,\\dots,A_n\\)求\\(p_n=P(\\bigcup_{i=1}^n A_i)\\). 注意到\\(A_1,\\dots,A_n\\)相互独立，则 \\[\\begin{aligned} 1-p_n = P(\\bigcap_{i=1}^n \\bar{A}_i)=\\prod_{i=1}^{n}P(\\bar{A}_i)=0.4^n. \\end{aligned}\\] 因此，\\(p_n= 1-0.4^n\\ge 0.99\\). 即\\(n\\ge \\log_{0.4}0.01\\approx 5.026\\). 所以，至少需要6门大炮。 例 6.26 (系统可靠性) 一个元件能正常工作的概率称为这个元件的可靠性；由元件组成的系统能正常工作的概率称为系统的可靠性。设构成系统的每个元件的可靠性均为\\(r(0&lt;r&lt;1)\\)，且各元件能否正常工作是相互独立的。设以\\(2n(n&gt;1)\\)个元件按下图所示的两种联接方式构成两个系统，试求它们的可靠性，并比较两个可靠性的大小。 解. \\(A_i=\\){元件\\(A_i\\)能正常工作}，设\\(B_i=\\){元件\\(B_i\\)能正常工作}。依题得，** \\(P(A_i)=P(B_i)=r\\). \\(A_i,B_i\\), \\(i=1,\\dots,n\\)相互独立。 \\[\\begin{aligned} P((A_1A_2\\dots A_n)\\cup (B_1B_2\\dots B_n))&amp;=1-P(\\overline{A_1A_2\\dots A_n}\\cap \\overline{B_1B_2\\dots B_n})\\\\ &amp;=1-P(\\overline{A_1A_2\\dots A_n})P(\\overline{B_1B_2\\dots B_n}) \\\\&amp;=1-(1-P(A_1A_2\\dots A_n))(1-P(B_1B_2\\dots B_n)) \\\\&amp;=1-(1-r^n)^2=r^n(2-r^n). \\end{aligned}\\] 设\\(A_i=\\){元件\\(A_i\\)能正常工作}，设\\(B_i=\\){元件\\(B_i\\)能正常工作}。依题得，** \\(P(A_i)=P(B_i)=r\\). \\(A_i,B_i\\), \\(i=1,\\dots,n\\)相互独立。 \\[\\begin{aligned} P\\left(\\bigcap_{i=1}^n(A_i\\cup B_i)\\right)&amp;=\\prod_{i=1}^{n}P(A_i\\cup B_i) =\\prod_{i=1}^{n}(1-P(\\bar{A}_i\\cap \\bar{B}_i)) \\\\&amp;=(1-(1-r)^2)^n=r^n(2-r)^n. \\end{aligned}\\] 只需比较\\(2-r^n\\)与\\((2-r)^n\\)的大小关系。所以，系统II比系统I更可靠。 定义 6.4 (独立试验模型) 设有随机试验\\(E_1,E_2,\\dots,E_n\\). 如果对于\\(E_i\\)的任意结果（事件）\\(A_i\\), \\(i=1,\\dots,n\\), 都有 \\[P(A_1A_2\\cdots A_n)=P(A_1)P(A_2)\\cdots P(A_n),\\] 则称随机试验\\(E_1,E_2,\\dots,E_n\\)相互独立。 特别地，如果这\\(n\\)个独立试验的条件相同，可能出现的结果也相同，则称为\\(n\\)重独立试验。若\\(n\\)重独立试验中的每个试验都只有两种可能的结果，则称为\\(n\\)重伯努利(Bernoulli)试验。 雅各布\\(\\cdot\\)伯努利(Jacob Bernoulli) 伯努利家族3代人中产生了8位科学家。伯努利家族代表人物之一，数学家。被公认的概率论的先驱之一。他是最早使用“积分”这个术语的人，也是较早使用极坐标系的数学家之一。还较早阐明拉随着试验次数的增加，频率稳定在概率附近。他研究了悬链线，还确定了等时曲线的方程。 1994年第22届国际数学家大会在瑞士的苏黎世召开，瑞士邮政发行的纪念邮票的邮票图案是雅各布\\(\\cdot\\)伯努利的头像，以他名字命名的大数定律及大数定律的几何示意图(即当试验次数无限增大时，事件出现的频率稳定于其出现的概率)。 雅可比\\(\\cdot\\)伯努利在数学方面取得了许多重大成果. 例如：他曾对微积分的发展作出了重要贡献；为常微分方程的积分法奠定理论基础；在研究曲线问题方面，他提出了一系列新概念；他创立了变分法；他还是概率论的早期研究者和奠基人。 洛必达法则是雅各布·伯努利的弟弟约翰\\(\\cdot\\)伯努利(Johann Bernoulli, 1667.7.27–1748.1.1)提出的。 Jacob Bernoulli 1654.12.27–1705.8.16 二项概率公式: 设一次试验中，事件\\(A\\)出现的概率为\\(P(A)=p, (0&lt;p&lt;1)\\)，则在\\(n\\)重伯努利试验中，事件\\(A\\)恰好出现\\(k\\)次的概率为 \\[b(k;n,p)=C_n^k p^k (1-p)^{n-k},\\ k=0,\\dots,n.\\] 注：\\(\\sum_{k=0}^n b(k;n,p) =\\sum_{k=0}^n C_n^k p^k (1-p)^{n-k} = 1\\). 例 6.27 某彩票每周开奖一次，中奖率为十万分之一。若你坚持每周买一张彩票，尽管你坚持十年（每年52周）之久，你未中奖的概率为\\[b(0;520,10^{-5})=(1-10^{-5})^{520}\\approx 0.9948.\\] 十年中你从未中奖很正常嘛！ 例 6.28 设某种药物对某种疾病的治愈率为\\(0.8\\)，现有\\(10\\)名患这种病的病人同时服用此药，求其中至少有\\(6\\)人被治愈的概率。 解. 将每名患者用药治疗作为一次试验，那么\\(10\\)名患者用药就是\\(10\\)重伯努利试验，由二项概率公式得，至少有\\(6\\)人治愈的概率为 \\[P=\\sum_{k=6}^{10} b(k;10,0.8)=\\sum_{k=6}^{10} C_{10}^k 0.8^k0.2^{10-k}\\approx 0.97.\\] 解. 某车间有10台同类型的机床，每台机床配备的电动机功率为10千瓦，已知每台机床工作时，平均每小时实际开动12分钟，且各机床开动与否是相互独立的，若供电部门只提供50千瓦的电力给这10台机床，问这10台机床能够正常工作的概率为多大？ 解. 每台机床在同一时刻是否开动看成一次试验，10台机床在同一时刻开动的台数，可以看成10重贝努利试验，\\(p=12/60=0.2\\)。10台机床要正常工作，必须同一时刻开动的机床数不得超过\\(50/10=5\\)台，即\\(k\\le 5\\)。故所求概率为： \\[P=\\sum_{k=0}^{5} b(k;10,0.2)=\\sum_{k=0}^{5} C_{10}^k 0.2^k0.8^{10-k}= 0.9936.\\] "],
["exam.html", "第 7 章 综合练习 7.1 2018秋季试卷 7.2 2018秋季试卷答案 7.3 2019春季试卷", " 第 7 章 综合练习 7.1 2018秋季试卷 Part I: Each problem is worth 3 points. Let \\(X_1,X_2,\\dots,X_6\\) be a simple random sample taken from \\(N(0,2^2)\\). Denote \\[Y = (X_1+X_2)^2+(X_3+X_4)^2+(X_5+X_6)^2.\\] If \\(kY\\sim \\chi^2(3)\\), then \\(k=\\)___? Let \\(X_1,X_2,X_3\\) be a simple random sample taken from \\(N(\\mu,\\sigma^2)\\). If \\(\\hat\\mu = \\frac{1}{2} X_1+cX_2+\\frac{1}{6}X_3\\) is an unibased estimate of \\(\\mu\\), then \\(c=\\)___? Let \\(X_1,X_2,X_3\\) be a simple random sample taken from \\(B(1,p)\\). For testing the hypothesis \\(H_0:p=1/2\\ vs.\\ H_1:p=3/4\\), we use a rejection region: \\[W=\\{(x_1,x_2,x_3):x_1+x_2+x_3\\ge 2\\}.\\] The power of the test is ___? Let \\(X_1,\\dots,X_n\\) be a simple random sample taken from \\(N(\\mu,1)\\), and let \\(S_n^2=\\frac 1n\\sum_{i=1}^n(X_i-\\bar X)^2\\) be the sample variance. Then \\(Var[S_n^2]=\\)___? If the usual \\(95\\%\\) confidence interval for the mean of normal population was \\([0.12,0.22]\\), the method of moments estimate of the mean would be ___? Part II: Multiple Choice Problems (one or more than one items may be true). Each problem is worth 3 points. The parameters \\(\\theta,\\lambda,\\alpha,\\beta\\) are unknown in the following densities. Which of the following probability distributions belong to the exponential family? ( ) A. \\(f(x;\\theta,\\lambda) = \\frac \\theta\\lambda\\left(\\frac{x}{\\lambda}\\right)^{\\theta-1}e^{-(x/\\lambda)^\\theta}1\\{x&gt; 0\\}\\) B. \\(f(x;\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1\\{0&lt;x&lt;1\\}\\), where \\(\\Gamma(\\cdot)\\) is the gamma function. C. \\(f(x;\\lambda) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x}1\\{x&gt; 0\\}\\) D. \\(f(x;\\theta) = \\frac{2}{\\sqrt{2\\pi}}e^{-\\frac{(x-\\theta)^2}{2}}1\\{x\\ge \\theta\\}\\) Let \\(X_1,\\dots,X_n\\) be the simple random sample taken from the normal distribution \\(N(\\mu,\\sigma^2)\\), where \\(\\mu,\\sigma^2\\) are unknown parameters. Which of the following are sufficient statistics for \\(\\theta=(\\mu,\\sigma^2)\\)? ( ) A. \\(T_1 = (X_1,\\dots,X_n)\\) B. \\(T_2 = (\\sum_{i=1}^n X_i,\\sum_{i=1}^n X_i^2)\\) C. \\(T_3 = (\\sum_{i=1}^n |X_i|,\\sum_{i=1}^n X_i^2)\\) D. \\(T_4 = \\frac{1}{n}\\sum_{i=1}^n X_i\\) Which of the following statements are true? ( ) A. If the \\(p\\)-value is 0.05, the corresponding test will be rejected at the significance level 0.03. B. If a test rejects at significance level 0.05, then the \\(p\\)-value is less than or equal to 0.05. C. If the significance level of a test is decreased, the power of the test would be expected to decrease. D. A type II error occurs when the test statistic falls in the rejection region of the test and the null is true. Let \\(\\hat\\beta_0,\\hat\\beta_1\\) be the least squares etstimators for the simple linear model \\(y_i = \\beta_0+\\beta_1x_i+\\epsilon_i,\\ i=1,\\dots,n\\), where \\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\). Which of the following statements are true? ( ) A. \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are independent. B. \\(\\hat\\beta_0-\\hat\\beta_1\\) is normally distributed. C. The more spread out the \\(x_i\\) are the better we can estimate the slope \\(\\beta_1\\). D. \\(\\bar y = \\hat\\beta_0+\\hat\\beta_1 \\bar x\\), where \\(\\bar x = \\frac 1 n\\sum_{i=1}^n x_i,\\ \\bar y = \\frac 1 n\\sum_{i=1}^n y_i\\). Let \\(X_1,\\dots,X_n\\) be a simple random sample taken from \\(N(2,3^2)\\), and let \\(\\bar X\\) be the sample mean. Which of the following are true? ( ) A. \\(\\frac{\\bar X -2}{3/\\sqrt{n}}\\sim t(n)\\) B. \\(\\frac 1 9\\sum_{i=1}^n (X_i-2)^2\\sim F(n,1)\\) C. \\(\\frac{\\bar X-2}{\\sqrt{3}/\\sqrt{n}}\\sim N(0,1)\\) D. \\(\\frac 1 9\\sum_{i=1}^n(X_i-2)^2\\sim \\chi^2(n)\\) Part III. (15 points) Let \\(X_1,\\dots,X_n\\) be a simple random sample taken from the density \\[f(x;\\theta)=\\frac{2x}{\\theta^2},\\quad 0\\le x\\le \\theta.\\] Find an expression for \\(\\hat\\theta_L\\), the maximum likelihood estimator (MLE) for \\(\\theta\\). Find an expression for \\(\\hat\\theta_M\\), the method of moments estimator for \\(\\theta\\). For the two estimators \\(\\hat\\theta_L\\) and \\(\\hat\\theta_M\\), which one is more efficient in terms of mean squared error (MSE)? Part IV. (10 points) Let \\(X_1,\\dots,X_n\\) be a simple random sample taken from an exponential distribution \\(Exp(\\lambda)\\), whose density is given by \\[f(x;\\lambda) = \\lambda e^{-\\lambda x}1\\{x\\ge 0\\},\\ \\lambda&gt;0.\\] Derive a likelihood ratio test of the hypothesis \\[H_0:\\lambda=1\\ vs.\\ H_1:\\lambda=2.\\] What is the definition of uniformly most powerful (UMP)? Is the test UMP against the alternative \\(H_1:\\lambda&gt;1\\)? Part V. (10 points) A medical researcher believes that women typically have lower serum cholesterol (血清胆固醇) than men. To test this hypothesis, he took a sample of 476 men between the ages of nineteen and forty-four and found their mean serum cholesterol to be 189.0 mg/dl with a sample standard deviation of 34.2. A group of 592 women in the same age range averaged 177.2 mg/dl and had a sample standard deviation of 33.3. Is the lower average for the women statistically significant? Set the significant level \\(\\alpha\\) =0.05. What assumptions are made when conducting the test? (\\(u_{0.95}=1.644854\\), \\(t_{0.95}(1066)=1.646284\\), \\(t_{0.95}(1068)=1.646282\\), \\(u_{0.975}=1.959964\\), \\(t_{0.975}(1066)=1.962192\\), \\(t_{0.975}(1068)=1.962188\\)) Part VI. (10 points) Let \\(X_1,\\dots,X_n\\) be a simple random sample taken from the uniform distribution \\(U(\\theta,0)\\), where \\(\\theta&lt;0\\). (a). Derive a \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\theta\\). (b). There is a duality between confidence intervals and hypothesis tests. Use the result in part (a) to derive a test at significant level \\(\\alpha\\) of the hypothesis \\[H_0: \\theta = \\theta_0\\ vs.\\ H_1:\\theta \\neq \\theta_0,\\] where \\(\\theta_0&lt;0\\) is fixed. Part VII. (10 points) Consider the linear model \\[y_i=\\beta_0+\\beta_1x_i+\\epsilon_i,\\ \\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2),\\ i=1,\\dots,n.\\] Suppose that all the fixed \\(x_i\\) are not equal and \\(n\\ge 3\\). (a). Derive a maximum likelihood estimator (MLE) \\(\\hat\\sigma_L^2\\) for \\(\\sigma^2\\). (b). Let \\(T_k=k\\hat\\sigma_L^2\\) be an estimate of \\(\\sigma^2\\). Find a \\(k\\in \\mathbb{R}\\) such that \\(T_k\\) is an unbiased estimate of \\(\\sigma^2\\). Show that the unbiased estimate is not the optimal choice by taking account of mean squared error (MSE), and the most efficient \\(T_k\\) takes place at \\(k=1\\), i.e., the MLE \\(\\hat\\sigma_L^2\\). Part VIII. (15 points) Consider the multiple linear regression model \\[y_i = \\beta_0+\\beta_1 x_{i1}+\\beta_2x_{i2}+\\dots+\\beta_{p-1}x_{i,p-1} +\\epsilon_i,\\] where \\(i=1,\\dots,n\\) and \\(n&gt;p\\ge 2\\). (a). Find the least squares estimates (LSE) of \\(\\beta_0,\\dots,\\beta_{p-1}\\) via the matrix formalism. What assumptions are required for ensuring a unique solution of the LSE? (b). Show that the the residuals sum to zero. Are the standard assumptions of \\(E[\\epsilon_i]=0\\) for \\(i=1,\\dots,n\\) required to establish the statement? (c). Suppose that \\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\), where \\(\\sigma&gt;0\\) is an unknown parameter. Define \\(\\alpha = \\sum_{i=1}^{p-1} \\beta_i^2\\). Use the generalized likelihood ratio method to test the hypothesis \\[H_0: \\alpha = 0\\ vs.\\ H_1:\\alpha&gt;0.\\] If the coefficient of determination \\(R^2=0.95\\), \\(p = 3\\) and \\(n=13\\), is the null rejected at the significant level \\(\\alpha =0.05\\)? (\\(F_{0.95}(2,10)=4.10,F_{0.95}(3,10)=3.71,t_{0.95}(10)=1.81\\)) 7.2 2018秋季试卷答案 Part I: 1/8 1/3 27/32 \\(2(n-1)/n^2\\) 0.17 Part II: BC AB BC BCD D Part III: The likelihood function is \\[L(\\theta) = \\prod_{i=1}^n f(x_i;\\theta) = \\frac{2^n}{\\theta^n}\\left(\\prod_{i=1}^n x_i\\right) 1\\{x_{(n)}\\le \\theta\\}.\\] To maximize \\(L(\\theta)\\), we need to choose \\(\\theta\\ge x_{(n)}\\) so that \\(L(\\theta) = A\\theta^{-n}\\), where \\(A=2^n\\prod_{i=1}^n x_i\\) does not depend on \\(\\theta\\). So the MLE is \\(\\hat\\theta_L = X_{(n)}\\). First, compute the first order moment: \\[E[X] = \\int_0^\\theta xf(x;\\theta)dx = \\int_0^\\theta \\frac{2x^2}{\\theta^2}dx=\\frac{2\\theta}{3}.\\] This implies that \\(\\theta = 3E[X]/2\\). The method of moments estimator \\(\\hat\\theta_M=3\\bar X/2\\). The density for \\(X_{(n)}\\) is given by \\[f_{X_{(n)}}(x;\\theta) = nF^{n-1}(x)f(x;\\theta)=n\\frac{x^{2(n-1)}}{\\theta^{2(n-1)}}\\frac{2x}{\\theta^2}=\\frac{2nx^{2n-1}}{\\theta^{2n}},\\quad 0\\le x\\le \\theta.\\] The first and second order moments for \\(X_{(n)}\\) are \\[E[X_{(n)}] = \\int_0^\\theta \\frac{2nx^{2n}}{\\theta^{2n}}dx = \\frac{2n\\theta}{2n+1},\\] \\[E[X_{(n)}^2] = \\int_0^\\theta \\frac{2nx^{2n+1}}{\\theta^{2n}}dx = \\frac{n\\theta^2}{n+1}.\\] The MSE for \\(\\hat\\theta_L\\) is given by \\[\\begin{align*} MSE(\\hat\\theta_L)&amp;=E[(\\hat\\theta_L-\\theta)^2]=E[X_{(n)}^2]-2\\theta E[X_{(n)}]+\\theta^2\\\\ &amp;=\\frac{n\\theta^2}{n+1}-\\frac{4n\\theta^2}{2n+1}+\\theta^2\\\\ &amp;=\\frac{\\theta^2}{(n+1)(2n+1)}. \\end{align*}\\] The second order moment for \\(X\\) is \\[E[X^2] = \\int_{0}^\\theta \\frac{2x^3}{\\theta^2}dx=\\frac{\\theta^2}{2}.\\] The MSE for \\(\\hat\\theta_M\\) is given by \\[\\begin{align*} MSE(\\hat\\theta_M)&amp;=Var[\\hat\\theta_M]=\\frac{9Var[X]}{4n}\\\\ &amp;=\\frac{9}{4n}(E[X^2]-E[X]^2)\\\\ &amp;=\\frac{9}{4n}\\left(\\frac{\\theta^2}{2}-\\frac{4\\theta^2}{9}\\right)= \\frac{\\theta^2}{8n}. \\end{align*}\\] It is easy to see that when \\(n\\ge 3\\), \\(MSE(\\hat\\theta_L)&lt;MSE(\\hat\\theta_M)\\); otherwise, \\(MSE(\\hat\\theta_L)&gt;MSE(\\hat\\theta_M)\\). Part IV: The likelihood function is \\[L(\\lambda)=\\prod_{i=1}^n (\\lambda e^{-\\lambda x_i}) = \\lambda^ne^{-\\lambda n\\bar x}.\\] The likelihood ratio is given by \\[\\lambda(\\vec x)= \\frac{L(2)}{L(1)}=\\frac{2^ne^{-2 n\\bar x}}{e^{- n\\bar x}}=2^ne^{-n\\bar x}.\\] Choose the test statistic \\(T(\\vec x) = 2n\\bar x\\). When \\(\\lambda=1\\), \\(T(\\vec X)\\sim \\chi^2(2n)\\). Also, \\(\\lambda(\\vec x) = 2^ne^{-T(\\vec x)/2}.\\) The rejection region is of the form \\(W=\\{T(\\vec x)&lt;C\\}\\). We thus have \\(C=\\chi_{\\alpha}^2(2n)\\). A rejection region \\(W\\) is said to be UMP if for any rejection region \\(W&#39;\\) with the type I error probability no more than \\(\\alpha\\), the power of the test associated with \\(W&#39;\\) is no larger than that of the rejection region \\(W\\). Consider the test of the hypothesis \\[H_0:\\lambda=1\\ vs.\\ H_1:\\lambda=\\lambda_0&gt;1.\\] Following the same procedure above, the likelihood ratio test gives the same rejection region W. So the test derived before is also UMP for the alternative \\(H_1:\\lambda&gt;1\\) by using the N-P lemma. Part V: Let \\(X_i\\) be the serum cholesterol for men, \\(i=1,\\dots,n=476\\), let \\(Y_j\\) be the serum cholesterol for women, \\(j=1,\\dots,m=592\\). We now have \\(\\bar x = 189.0\\), \\(s_{1n}=34.2\\), \\(\\bar y = 177.2\\), \\(s_{2m}=33.3\\). Suppose that \\(X_i\\stackrel{iid}{\\sim} N(\\mu_1,\\sigma^2)\\) and \\(Y_i\\stackrel{iid}{\\sim} N(\\mu_2,\\sigma^2)\\). We are testing \\[H_0:\\mu_1\\le \\mu_2,\\ vs.\\ H_1:\\mu_1&gt;\\mu_2.\\] We use the t-test. The test statistic is \\[T = \\frac{\\bar X-\\bar Y}{S_w\\sqrt{\\frac 1 n+\\frac 1 m}},\\] where \\(S_w^2 = (nS_{1n}^2+mS_{2m}^2)/(n+m-2)\\). The rejection region is given by \\(W = \\{T&gt;t_{1-\\alpha}(n+m-2)\\}\\). The observed test statistic is \\[t=\\frac{189.0-177.2}{33.74\\sqrt{\\frac 1 {476}+\\frac 1 {592}}}=5.68&gt;t_{0.95}(1066)=1.65.\\] We therefore reject the null. The lower average for the women is statistically significant. The assumptions are normally distributed for both groups the two grouds are independent their variances are the same Part VI: Let \\(G = X_{(1)}/\\theta\\). The CDF for \\(G\\) is given by \\[\\begin{align*} F_G(x) &amp;= P(G\\le x) = P(X_{(1)}/\\theta\\le x) \\\\ &amp;= P(X_{(1)}\\ge \\theta x) \\\\ &amp;= \\prod_{i=1}^nP(X_i\\ge \\theta x) \\\\ &amp;= x^n,\\ 0&lt; x&lt; 1. \\end{align*}\\] Let \\(a,b\\in \\mathbb{R}\\) such that \\(P(a\\le G\\le b)=1-\\alpha\\). Then the CI for \\(\\theta\\) is \\[CI=\\left[\\frac{X_{(1)}}{a},\\frac{X_{(1)}}{b}\\right].\\] For simplicity, we take \\(a,b\\) such that \\(P(G\\le a) = P(G\\ge b) = \\alpha/2\\). This implies \\(a = (\\alpha/2)^{1/n},\\ b= (1-\\alpha/2)^{1/n}\\). Or you can take \\(P(G\\le a) = \\alpha,P(G\\le b)=1\\) so that \\(a=\\alpha^{1/n}, b=1\\). You can also other statistics, such as \\(G=-2\\log(\\sum_{i=1}^n X_i/\\theta)\\). The answer is not unique. Form part (a), we have \\[P_{\\theta}\\left(\\theta\\in CI\\right) = 1-\\alpha\\ \\forall\\theta&lt;0.\\] We therefore choose the rejection region \\[W = \\{\\theta_0\\notin CI\\}.\\] It is easy to see that \\(P_{\\theta_0}(\\theta_0\\notin CI) = \\alpha\\). Part VII: It is easy to see that \\[\\hat\\sigma_L^2=\\frac{S_e^2}{n},\\] where \\(S_e^2 = \\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)^2\\), and \\(\\hat\\beta_0,\\hat\\beta_1\\) are the LSE for \\(\\beta_0,\\beta_1\\). It is known that \\(S_e^2/\\sigma^2\\sim \\chi^2(n-2)\\). This gives \\(E[S_e^2]=(n-2)\\sigma^2\\) and \\(Var[S_e^2] = 2(n-2)\\sigma^4\\). As a result, \\(E[T_k] = kE[S_e^2/n]=\\frac{k(n-2)}{n}\\sigma^2\\). If \\(T_k\\) is unbiased, then \\(k = n/(n-2)\\). On the other hand, \\[Var[T_k] = \\frac{k^2}{n^2}Var[S_e^2] = \\frac{2(n-2)k^2}{n^2}\\sigma^4.\\] The MSE of \\(T_k\\) is given by \\[\\begin{align*} M(k) &amp;= E[(T_k-\\sigma^2)^2] = (E[T_k]-\\sigma^2)^2+Var[T_k]\\\\ &amp;=\\frac{(n-2)(k-1)^2+2}{n}\\sigma^4 \\end{align*}\\] whose minimum takes place at \\(k=1\\). Part VIII: (a). \\(Y=X\\beta\\), the LSE is \\(\\hat\\beta = (X^\\top X)^{-1} X^\\top Y\\). It is requried that that \\(\\text{rank} (X) = p\\). (b). \\(\\hat\\epsilon = Y-X\\hat \\beta = Y-X(X^\\top X)^{-1} X^\\top Y=(I_n-P)Y\\) \\[\\hat \\epsilon^\\top X = Y^\\top (I_n-P)X = 0.\\] As a result, we have \\(\\hat \\epsilon^\\top 1 = \\sum_{i=1}^n \\hat\\epsilon_i=0\\). We do not require any assumption on \\(\\epsilon_i\\). (C). The test statistic is \\[\\begin{align*} F &amp;=\\frac{S_R^2/(p-1)}{S_e^2/(n-p)}=\\frac{R^2/(p-1)}{(1-R^2)/(n-p)}\\\\ &amp;=\\frac{0.95/2}{(1-0.95)/10}=95&gt;F_{0.95}(2,10)=4.1. \\end{align*}\\] We therefore reject the null. 7.3 2019春季试卷 Part I: Each problem is worth 3 points. Let \\(X_1,\\dots,X_{10}\\) be i.i.d. sample of \\(X\\sim \\text{Exp}(1)\\). If \\(2\\sum_{i=1}^{10} X_i\\sim \\chi^2(k)\\), then the value of \\(k\\) is __________ What is the definition of Type I error?_______________________________ Let \\(T\\sim t(10)\\). It is known that \\(P(T\\le 1.8)=0.95\\). Then \\(F_{0.9}(1,10)=\\)___________ If the \\(95\\%\\) confidence interval for the mean of a normally distributed population with known variance is \\([1.2, 1.4]\\) based on a sample of size \\(100\\), how much larger a sample do you think you would need to halve the length of the confidence interval （该置信区间长度减半需要增加多少样本）? ____________ Show one advantage of the maximum likelihood method compared to the method of moments. ___________________________ Part II: Multiple-Choice Problems (ONLY one of the items is true). Each problem is worth 3 points. Let \\(X_1,\\dots,X_n\\) be i.i.d. sample of \\(X\\sim N(\\mu,\\sigma^2)\\), where \\(\\mu,\\sigma\\) are unknown parameters. Which one of the following is NOT a statistic. ( ) A. \\(X_1+X_2+\\dots+X_n\\) B. \\(X_{(1)} = \\min\\{X_1,X_2,\\dots,X_n\\}\\) C. \\(\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}\\) D. \\(g(\\bar X)\\), where \\(g(\\cdot)\\) is a given function over \\(\\mathbb{R}\\). Consider the problem of testing \\[H_0:\\mu=0\\ vs.\\ H_1:\\mu&gt;0.\\] The power functions of four rejection regions are plotted below. Which one might be the uniformly most powerful (UMP) rejection region at the significance level \\(\\alpha = 0.05\\)? ( ) Let \\(X_1,\\dots,X_n\\) \\((n\\ge 3)\\) be a smple of a Weibull population with denstiy \\[f(x;k,\\lambda)=\\frac k\\lambda\\left(\\frac{x}{\\lambda}\\right)^{k-1}e^{-(x/\\lambda)^k}1\\{x&gt; 0\\},\\] where \\(k&gt;0,\\lambda&gt;0\\) are unknown parameters. Which of following is a sufficient statistic for \\(\\theta=(k,\\lambda)\\)? ( ) A. \\(T_1 = (X_1,\\dots, X_n)\\) B. \\(T_2 = \\prod_{i=1}^n X_i\\) C. \\(T_3 = \\sum_{i=1}^n X_i^k\\) D. \\(T_4 = (\\sum_{i=1}^n X_i^k, \\prod_{i=1}^n X_i)\\) Let \\(\\hat\\beta_0,\\hat\\beta_1,\\hat\\beta_2\\) be the least squares estimators for the multiple linear model \\[y_i = \\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\epsilon_i,\\] where \\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\), \\(i=1,\\dots,n\\). Which of the following statements is NOT true? ( ) A. The estimators \\(\\hat\\beta_0,\\hat\\beta_1,\\hat\\beta_2\\) are normally distributed. B. The estimators \\(\\hat\\beta_0,\\hat\\beta_1,\\hat\\beta_2\\) are independent. C. \\(\\mathbb{E}[\\hat\\beta_i] = \\beta_i,\\ i=0,1,2\\). D. \\(\\hat\\beta_0-\\hat\\beta_1\\) is normally distributed. Consider the multiple linear model \\(Y = X\\beta +\\epsilon\\), where \\(X\\) is the \\(n\\times p\\) design matrix, \\(\\beta\\) is a vector of \\(p\\) parameters, and the error \\(\\epsilon\\sim N(0,\\sigma^2 I_n)\\). Let \\(\\hat\\beta\\) be the least squares estimate of \\(\\beta\\), \\(\\hat Y = X\\hat\\beta\\), \\(\\hat\\epsilon = Y-\\hat Y\\), and \\(S_e^2 = ||\\hat\\epsilon||^2=\\sum_{i=1}^n \\hat\\epsilon_i^2\\). Which of following statements is true? ( ) A. \\(\\frac{S_e^2}{\\sigma^2}\\sim \\chi^2(n)\\) B. \\(S_e^2\\) is independent of the length of \\(\\hat\\beta\\), i.e., \\(||\\hat\\beta||\\). C. \\(\\hat\\beta\\sim N(\\beta,\\sigma^2 X^\\top X)\\) D. \\(\\sqrt{S_e^2/(n-p)}\\) is an unbiased estimate of \\(\\sigma\\). Part III. (15 points) Let \\(X_1,\\dots,X_n\\) be i.i.d. sample of \\(X\\sim N(\\mu,\\sigma^2)\\), where \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma&gt;0\\). If \\(\\sigma\\) is known, find a \\(1-\\alpha\\) confidence interval (CI) for \\(\\mu\\). If \\(\\sigma\\) is unknown, find a \\(1-\\alpha\\) CI for \\(\\mu\\). Would you use the CI established in Part (b) if you were able to get the value of \\(\\sigma\\)? Why? Part IV. (10 points) For a random sample of size \\(n\\) from a population \\(X\\), consider the following as an estimate of \\(\\theta=\\mathbb{E}[X]\\): \\[\\hat\\theta = \\sum_{i=1}^n c_i X_i,\\] where \\(c_i\\) are fixed numbers and \\(X_1,\\dots,X_n\\) is i.i.d. sample. Find a condition on the \\(c_i\\) such that the estimate is unbiased. Show that the choice of \\(c_i\\) that minimizes the mean squared errors (MSEs) of the estimate subject to the condition in Part (a) is \\(c_i = 1/n\\), where \\(i=1,\\dots,n\\). Part V. (10 points) Suppose that \\(X\\) is a discrete random variable with \\[P(X=1) = (1-\\theta)^2,\\ P(X=2) = 2\\theta(1-\\theta),\\ P(X=3)=\\theta^2,\\] where \\(\\theta\\in(0,1)\\). Now a total of \\(100\\) independent observations of \\(X\\) are made with the following frequencies: Case \\(X=1\\) \\(X=2\\) \\(X=3\\) Frequency 70 10 20 What is the maximum likelihood estimate of \\(\\theta\\)? Part VI. (10 points) Write down the Neyman-Pearson (N-P) Lemma and prove it. Part VII. (10 points) There are 37 blood alcohol determinations made by Analyzer GTE-10, a three-year-old unit that may be in need of recalibration （校准）. All 37 measurements were made using a test sample on which a properly adjusted machine would give a reading of \\(12.6\\%\\). Based on the data, the sample mean \\(\\bar x = 12.7\\%\\) and the sample standard deviation \\(s = 0.6\\%\\). (\\(t_{0.975}(36)=2.028, t_{0.975}(37)=2.026, t_{0.95}(36)=1.688, t_{0.95}(37)=1.687, u_{0.975}=1.960, u_{0.95}=1.645\\)) Would you recommend that the machine should be readjusted （重新调整） at the level of significance \\(\\alpha = 0.05\\)? What is the p-value of your test? (Suppose that the CDFs of the standard normal, t, \\(\\chi^2\\), F distributions are known. You can use them whenever you need.) What assumptions are made when conducting the test? Part VIII. (15 points) Suppose that in the model \\[y_i= \\beta_0+\\beta_1x_i+\\epsilon_i,\\ i=1,\\dots,n,\\] the errors \\(\\epsilon_i\\) have mean zero and are uncorrelated, but \\(\\mathrm{Var}(\\epsilon_i) = \\rho_i^2\\sigma^2\\), where the \\(\\rho_i&gt;0\\) are known constants, so the errors do not have equal vairance. Because the variances are not equal, the theory developed in our class does not apply. Try to transform suitably the model such that the basic assumptions (i.e., the errors have zero mean and equal variance, and are uncorrelated) of the standard statistical model are satisfied. Find the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\) for the transformed model. Find the variances of the estimates of Part (b). "],
["references.html", "参考文献", " 参考文献 "]
]
