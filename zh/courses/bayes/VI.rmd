---
date: "2019-05-05T00:00:00+01:00"
draft: false
linktitle: 变分推断
menu:
  Bayes:
    parent: 主要内容
    weight: 15
title: 变分推断
type: docs
weight: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This note is adapted to the paper entitled "Variation Inference: A Review for Statisticans" by Blei et al. (2017).

## Basic ideas of VI


Let $z=z_{1:m}$ be the latent variables that govern the distribution of the data (observations) $x=x_{1:n}$.
The prior is denoted by $p(z)$. The likelihood is $p(x|z)$. The posterior thus is given by
$$p(z|x)=\frac{p(z)p(x|z)}{p(x)}\propto p(z)p(x|z).$$
The denominator $p(x)$ contains the marginal density of the obsevations, also called the *evidence*.

- ABC algorithms provide a kind of approximations of the posterior in the context of simulation.

- VI provides another kind of approximations of the posterior by minizing the *Kullback-Leibler (KL) divergence* to the exact posterior over a family of approximate densities $\mathcal Q$. That is
$$q^*(z) = \arg \min_{q(z)\in\mathcal Q} KL (q(z)||p(z|x)).$$
The KL divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. The formal definition of KL divergence is $$KL(p_1||p_2)=E_{p_1}[\log (p_1(x)/p_2(x))]=E_{p_1}[\log p_1(x)]-E_{p_1}[\log p_2(x)].$$


- $KL(p_1||p_2)\ge 0$, where the equality holds iff $p_1(z)=p_2(z)$ w.p.1. This can be proved via Jensen inequality. Noting that 
$$KL(p_1||p_2)=-E_{p_1}[\log (p_2(x)/p_1(x))]\ge -\log E_{p_1}[p_2(x)/p_1(x)]=-\log \int \frac{p_2(x)}{p_1(x)} p_1(x) dx =0.$$
The equality holds iff $p_2(x)/p_1(x)$ is constant w.p.1.

- It is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread, i.e, $KL(p_1||p_2)\neq KL(p_2||p_1)$.

- It also does not satisfy the triangle inequality $KL(p_1||p_2)+KL(p_2||p_3)\ge KL(p_1||p_3)$.


Note that the objective is not computable because it requires computing $\log p(x)$, which is typically infeasible. To see why, 
$$KL (q(z)||p(z|x))=E_{q}[\log q(z)]-E_{q}[\log p(z|x)]=E_{q}[\log q(z)]-E_{q}[\log p(z)]-E_{q}[\log p(x|z)]+\log p(x).$$
As a result, we optimize an alternative objective that is equivalent to the KL up to an added constant, 
\begin{equation}
ELBO(q)=E_{q}[\log(p(x,z)/q(z))]=E_{q}[\log p(z)]+E_{q}[\log p(x|z)]-E_{q}[\log q(z)].\label{eq:elbo}
\end{equation}
This function is called the *evidence lower bound (ELBO)*.  It is easy to see that
$$ELBO(q)=-KL (q(z)||p(z))+\log p(x).$$ 
Since the log-evidence is constant, 
\begin{equation}
q^*(z) = \arg \min_{q(z)\in\mathcal Q} KL (q(z)||p(z|x))=\arg\max_{q(z)\in\mathcal Q} ELBO(q).
\label{eq:elboopt}
\end{equation}

- It follows from $KL(p_1||p_2)\ge 0$ that $ELBO(q)\le \log p(x)$ for any $q$. This means the ELBO is a lower-bound of the log-evidence, explaining its name.

- From the second equality in \eqref{eq:elbo}, maximazing the ELBO mirrors the usual balance between likelihood and prior.

## The Mean-Field Variational Family

The complexity of the family determines the complexity of the optimization; it is more difficulty to optimize over a complex family than a simple family. We next focus on the *mean-field variational family*, 
\begin{equation}
q(z) = \prod_{j=1}^m q_j(z_j).\label{eq:mfvf}
\end{equation}
Each latent variable $z_j$ is governed by its own variational factor, the density $q_j$. That is $z_j\stackrel{ind}\sim q_j$.

One may specify the parametric form of the individual variational factors. In principle, each can take on any parametric form appropriate to the corresponding random variable. 

- A continous variable might have a Gaussian factor.

- A categorical variable will typically have a categorical factor.


## Coordinate Ascent Mean-Field VI

This section describe one of the most commonly used algorithms for solving the optimizatin problem \eqref{eq:elboopt} subject to the mean-field variational family \eqref{eq:mfvf}. The coordinate ascent VI (CAVI) iteratively optimizes each factor of the mean-field variation density, while holding the others fixed. It climbs the ELBO to a local optimum.

Let $z_{-j}$ be the vector of $z$ by removing the $j$th component $z_j$, and let $p(z_j|z_{-j},x)$ be the *complete conditional* of $z_j$ given all of the other latent variables in the model and the observations.
Fixing the other variational factors, $q_\ell(z_\ell)$, $\ell\neq j$, the optimal $q_j(z_j)$ is then propotional to the exponentiated expected log of the complete conditional, 
\begin{equation}
q_j^*(z_j) \propto \exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}\propto \exp\{E_{-j}[\log p(x,z)]\},\label{eq:cavi}
\end{equation}
where the expectation is with respective to the currently fixed variational density over $z_{-j}$, i.e, $\prod_{\ell\neq j} q_\ell (z_\ell)$. To see why, when fixing the other variational factors, $q_\ell(z_\ell)$, $\ell\neq j$, it follows from \eqref{eq:elbo} that
\begin{align*}
ELBO(q) &= ELBO(q_j) = E_{q}[\log(p(x,z)/q(z))] \\&= E_{q}[\log(p(z_{j}|z_{-j},x)p(z_{-j},x)/q_{j}(z_j)/q_{-j}(z_{-j}))]\\
& = E_{q}[\log(p(z_{j}|z_{-j},x)/q_j(z_j)] + \text{const}\\
&=E_{q_j}[E_{-j}[\log(p(z_{j}|z_{-j},x)]-\log q_j(z_j)] + \text{const}\\
&=E_{q_j}[\log (\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}/q_j(z_j))]+ \text{const}\\
&= - KL(q_j(z_j)||c\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\})+ \text{const},
\end{align*}
where $c$ is a normalized constant such that $c\exp\{E_{-j}[\log p(z_j|z_{-j},x)]$ is a PDF.
Since $KL\ge 0$, the maximization of ELBO attains at $q_j(z_j)=c\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}$ w.p.1. Therefore, the optimal $q_j(z_j)$ is propotional to $\exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}$.

CAVI goes as follows: Initizlize the variational factors $q_j(z_j)$; Update each factor of the mean-field variation density by \eqref{eq:cavi}, while holding the others fixed, until the ELBO converges. To check the convergence, we may compute the ELBO after a (few) loop of all the factors.

The ELBO is (generally) a nonconvex objective function. CAVI only guarantees to a local optimum, which can be sensitive to iniitialization. Also, the updated variational factor should have a closed form.

## Application I: Bayesian mixture of Gaussians

As a concrete example, we consider a Bayesian mixture of *unit-variance univariate Gaussians*. There are $K$ mixture components, corresponding to $K$ Gaussian distributions with means $\mu=(\mu_1,\dots,\mu_K)$. Given the means, the data is generated via
$$x_i|\mu,\alpha\stackrel{iid}{\sim} \sum_{k=1}^K \alpha_{k} N(\mu_k,1),$$
where $\alpha_{k}>0$ is the probablity drawn from the $k$th Guassian with $\sum_{k=1}^K \alpha_{k} =1$. 

We now add some latent variables to reformulate the model. This is actually a technique of *data augment*. Let the latent variable $c_i$ be an indicator $K$-vector, all zeros expect for a one in the position corresponding to $x_i$'s cluster. There are $K$ possible values for $c_i$. As a result, $x_i|\mu,c_i\sim N(c_i^\top \mu,1)$, $c_i\sim \text{categorical}(\alpha)=:CG(\alpha)$, where $\alpha=(\alpha_{1},\dots,\alpha_{K})$. Assume that the mean parameters are drawn independently from a common prior $p(\mu_k)\sim N(0,\sigma^2)$; the prior variance $\sigma^2$ ia a hyperparameter; and the prior for the latent indicators is $c_i\sim CG(1/K,1/K,\dots,1/K)$. 

The full hierarchical model is 
\begin{align}
\mu_k&\stackrel{iid}{\sim} N(0,\sigma^2), & k=1,\dots,K,\\
c_i&\stackrel{iid}{\sim} \text{categorical}(1/K,1/K,\dots,1/K), & i=1,\dots, n,\\
x_i|\mu,c_i&\stackrel{ind}{\sim} N(c_i^\top \mu,1), &i=1,\dots, n.
\end{align}
The latent variables are $z=(\mu, c)$. The joint density of latent and observed variables is
$$p(\mu,c,x) = p(\mu) \prod_{i=1}^n p(c_i)p(x_i|c_i,\mu).$$
The evidence is
\begin{align}
p(x)= \int p(\mu) \prod_{i=1}^n \sum_{c_i} p(c_i)p(x_i|c_i,\mu) d\mu=\sum_{c_1,\dots,c_n}\prod_{i=1}^n p(c_i) \int p(\mu) \prod_{i=1}^n p(x_i|c_i,\mu) d\mu.\label{eq:gmmevi}
\end{align}
Thanks to conjugacy between the Gaussian prior on the components and the Gaussian likelihood, each individual integral $I(c_1,\dots,c_n):=\int p(\mu) \prod_{i=1}^n p(x_i|c_i,\mu) d\mu$ is computable. However, the total cases of the configuration $(c_1,\dots,c_n)$ is $K^n$. As a result, the complexity of computing \eqref{eq:gmmevi} is $O(K^n)$, which is  infeasible for moderate sample size $n$ and $K$. For example, when $K=3$ and $n=100$, $K^n  = 3^{100}\approx 5.2\times 10^{47}$. In this sense, we can say that the evidence \eqref{eq:gmmevi} is intractable.

In VI, we choose the mean-field variational family as the form
$$q(\mu,c) = \prod_{k=1}^K q(\mu_k;m_k,s_k^2)\prod_{i=1}^nq(c_i;\psi_i),$$
where the variational factor $q(\mu_k;m_k,s_k^2)$ for the mean $\mu_i$ is a Guassian $N(m_k,s_k^2)$, and the variational factor $q(c_i;\psi_i)$ for the indicator is $CG(\psi_i)$.
By \eqref{eq:elbo}, we have
\begin{align}
ELBO(m,s^2,\psi)&=E_{q}[\log p(z)]+E_{q}[\log p(x|z)]-E_{q}[\log q(z)]\notag\\
&=\sum_{k=1}^K E_{\mu_k\sim N(m_k,s_k^2)}[\log p(\mu_k)]\notag\\
&\quad+\sum_{i=1}^n (E_{c_i\sim CG(\psi_i)}[\log p(c_i)]+E_{c_i\sim CG(\psi_i),\mu\sim N(m,\text{diag}(s^2))}[\log p(x_i|c_i,\mu)])\notag\\
&\quad-\sum_{i=1}^n E_{c_i\sim CG(\psi_i)}[\log q(c_i;\psi_i)]-\sum_{k=1}^K E_{\mu_k\sim N(m_k,s_k^2)}[\log q(\mu_k;m_k,s_k^2)]\notag\\
&=\frac K 2-K\log\sigma-n\log K-\frac 12 n\log(2\pi)+\frac 1 2\sum_{i=1}^n x_i^2+\sum_{k=1}^K\left[\log(s_k)-\frac{m_k^2+s_k^2}{2\sigma^2}\right] \notag\\
&\quad-\sum_{i=1}^n\sum_{k=1}^K\psi_{ik}\left[\frac{m_k^2+s_k^2}2-x_im_k+\log(\psi_{ik}) \right]\notag\\
&=\sum_{k=1}^K\left[\log(s_k)-\frac{m_k^2+s_k^2}{2\sigma^2}\right] -\sum_{i=1}^n\sum_{k=1}^K\psi_{ik}\left[\frac{m_k^2+s_k^2}2-x_im_k+\log(\psi_{ik}) \right]+\text{const}.\label{eq:BMGelbo}
\end{align}

Note that all the expectation in the ELBO \eqref{eq:BMGelbo} can be computed in closed form. There are many methods to find a local optimum of \eqref{eq:BMGelbo}.

- **Newton-Raphson algorithm.** It suffices to find the root of $\nabla  ELBO(m,s^2,\psi) = 0$. Let $\lambda = (m,s^2,\psi)\in \mathbb{R}^{2K+n(K-1)}$ be a vector of parameters. The Newton-Raphson method uses the iteration
$$\lambda^{(t+1)}=\lambda^{(t)}-(D^2 ELBO(\lambda^{(t)}))^{-1}  \nabla ELBO(\lambda^{(t)}),$$
where $D^2 ELBO(\lambda^{(t)})$ is the Hessian matrix.

- **Gradient ascent algorithm.** It is a first-order iterative optimization algorithm for finding a local maximum. The iteration is
$$\lambda^{(t+1)}=\lambda^{(t)} + \eta_t \nabla ELBO(\lambda^{(t)}),$$
where $\eta_t>0$ is the learning rate.

- **CAVI.** The iteration is
\begin{align}
\psi_{t+1,ik} &\propto \exp\{E[\mu_k]x_i-E[\mu_k^2]/2\}\propto \exp\{m_{t,k}x_i-(m_{t,k}^2+s_{t,k}^2)/2\},\\
m_{t+1,k}&=\frac{\sum_{i=1}^n \psi_{t+1,ik}x_i}{1/\sigma^2+\sum_{i=1}^n\psi_{t+1,ik}}, \label{eq:miter} \\
s_{t+1,k}^2&=\frac{1}{1/\sigma^2+\sum_{i=1}^n\psi_{t+1,ik}}, k=1,\dots,K, i=1,\dots,n, \label{eq:siter}
\end{align} 
where $\psi_{t,\cdot}, m_{t,\cdot},s^2_{t,\cdot}$ denote the parameters at the step $t$. Note that the algorithm does not need the initial varitional factors for $\psi_i$.

Next, we implement the  CAVI algorithm for $K=5$ and $n=10^3$. After a few steps, we can see that the ELBO converges.

```{r, out.width= '80%',fig.align='center'}
set.seed(1)
## data generation
K = 5 # the number of clusters
n = 1000 # the number of data x_i
mu = matrix(rnorm(K,mean=0.2,sd=2),ncol = 1) # the means of the K clusters
c = sample(1:K,n,replace = T) # the indicator
x = matrix(mu[c]+rnorm(n),ncol = 1)
plot(density(x),xlab = 'x',main='kernel density of the data')

sig = 1 # hyperparameter for the variance of mu_i
## ELBO minus a constant
elbo <- function(m,s,psi){
  re = sum(log(s)-(m^2+s^2)/(2*sig^2))- sum(psi%*%(m^2+s^2)/2)+
    t(x)%*%psi%*%m-sum(log(psi)*psi)
  return(re)
}
## iteration for CAVI
cavi <- function(m,s){
  psi = matrix(0,n,K)
  for(i in 1:n){
    tmp = x[i]*m-(m^2+s^2)/2
    mtmp = max(tmp)
    logsum = mtmp+log(sum(exp(tmp-mtmp)))
    psi[i,] = exp(tmp-logsum)
  }
  de = 1/sig^2+colSums(psi)
  m = t(x)%*%psi/de
  s = sqrt(1/de)
  return(list(m_next=matrix(m,ncol = 1),s_next=matrix(s,ncol = 1),psi_next=psi))
}
## initialization
nstep = 1e4 # maximal steps
tolerance = 1e-6 # tolerance for the relative change
m = matrix(rnorm(K,0,1),K,1) 
s = matrix(5,K,1)
ELBO = matrix(0,nstep,1)
step = 1
relative_change = tolerance+1
while(TRUE){
  para = cavi(m,s)
  m = para$m_next
  s = para$s_next
  psi = para$psi_next
  ELBO[step] = elbo(m,s,psi)
  if(step>1)
    relative_change = (ELBO[step]-ELBO[step-1])/ELBO[step]
  if(step==nstep | abs(relative_change)<tolerance){ # stopping rule
    break
  }
  else{
    step = step+1
  }
}
hatc = apply(psi, 1,which.max)
center = cbind(sort(mu),sort(m))
colnames(center) = c('True Centers','VI Means')
knitr::kable(center)
plot(1:step,ELBO[1:step],type = 'b',xlab = 'Step',ylab='ELBO',pch = 16)
plot(x,col=hatc)
abline(h=m,col=1:5,lty=2,lwd=2)
```


## Exponential Family Conditionals

Are there specific forms for the local variational
approximations in which we can easily compute closed-form conditional ascent updates? Yes, the answer is exponential family conditionals. 


Consider the generic model $p(z,x)$ and suppose each complete conditional is in the exponential family:
\begin{equation}
p(z_j|z_{-j},x) = h(z_j)\exp\{\eta_j(z_{-j},x)^\top t(z_j)-a(\eta_j(z_{-j},x))\},
\end{equation}
where $t(z_j)$ is the sufficient statistic, and $\eta_j(z_{-j},x)$ are the natural parameters.

Consider mean-field VI for this class of models, where $q(z)$ is given by \eqref{eq:mfvf}. The update \eqref{eq:cavi} becomes
\begin{align}
q_j^*(z_j) &\propto \exp\{E_{-j}[\log p(z_j|z_{-j},x)]\}\\
&=\exp\{\log h(z_j)+ E_{-j}[\eta_j(z_{-j},x)^\top t(z_j)]-E_{-j}[a(\eta_j(z_{-j},x))]\}\\
&\propto h(z_j)\exp\{E_{-j}[\eta_j(z_{-j},x)]^\top t(z_j)\}.
\end{align}
This updata reveals the parametric form of the optimal VI factors. Each one is in the same exponential family as its corresponding complete conditional. Let $\nu_j$ denote the variational parameter for the $j$th variational factor. When we update each factor, we set its parameter equal to the expected parameter of the complete conditional, 
$$\nu_j = E_{-j}[\eta_j(z_{-j},x)].$$

There are many popular models fall into this category, including:

- Bayesian mixtures of exponential family models with conjugate priors.
- Hierarchical hidden Markov models.
- Kalman filter models and switching Kalman filters.
- Mixed-membership models of exponential families.
- Factorial mixtures / hidden Markov models of exponential families.
- Bayesian linear regression.
- Any model containing only conjugate pairs and multinomials.

Some popular models do not fall into this category, including:

- Bayesian logistic regression and other nonconjugate Bayesian generalized linear models.
- Correlated topic model, dynamic topic model.
- Discrete choice models.
- Nonlinear matrix factorization models.



## Stochastic Gradient Variational Inference 

CAVI may require interating thought the entire dataset at each iteration. As the dataset size grows, each 
iteration becomes more computationally expensive (see \eqref{eq:miter} and \eqref{eq:siter}). In more realistic models, the gradient of the ELBO \eqref{eq:elbo} is rarely available in closed form. Stochastic gradient methods (Robbins
and Monro, 1951) are useful for optimizing an objective function whose gradient can be unbiasedly estimated.  Stochastic gradient variational inference (SGVI) becomes an alternative to coordinate ascent. SGVI combines gradients and stochastic optimazation.

Now we rewrite the ELBO as a function of variational parameters $\lambda$ (a vector), denoted by $\mathcal L(\lambda)$. Let $\nabla_\lambda\mathcal L(\lambda)$ be the gradient vector of $\mathcal L(\lambda)$ w.r.t. $\lambda$. Gradient ascent algorithm iterates
$$\lambda^{(t+1)}=\lambda^{(t)} + \eta_t \nabla_\lambda\mathcal L(\lambda^{(t)}),\quad t=0,\dots,T.$$
Let $\widehat{\nabla_\lambda\mathcal L(\lambda)}$ be an unibased estimate of $\nabla_\lambda\mathcal L(\lambda)$. SGVI iterates as follow, 
\begin{equation}
\lambda^{(t+1)}=\lambda^{(t)} + \eta_t \widehat{\nabla_\lambda\mathcal L(\lambda^{(t)})},\quad t=1,\dots,T.\label{eq:sgdite}
\end{equation}
Under certain regularity conditions, and the learning rates satisfy the **Robbins-Monro** conditions
$$\sum_{t=0}^\infty \eta_t=\infty,\ \sum_{t=0}^\infty \eta_t^2<\infty,$$
the iterations converge to a local optimum (Robbins and Monro, 1951). Many sequences will satisfy these conditions, for example, $\eta_t=t^{-\kappa}$ for $\kappa\in(0.5,1]$. Adaptive learning rates are currently
popular (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2015; Kingma and Ba, 2015). **Adam** is a promising method, which is pulished in

> Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the International Conference on Learning Representations. (Cited by 43201 on 2020/5/27)

The name Adam
is derived from adaptive moment estimation.

One important thing is to obtain an unbiased estimate of the gradient $\nabla_\lambda\mathcal L(\lambda)$.
The variational density is now written as $q(z;\lambda)$. Then,
$$\nabla_\lambda\mathcal L(\lambda) = \nabla_\lambda E_q[\log p(z,x)]-\nabla_\lambda E_q[\log q(z;\lambda)].$$
Note that in some cases (such as mean-field variational family with Gaussian or categorical factors) of variational densities, $E_q[\log q(z;\lambda)]$ is analytically solvable, and so does its gradient $\nabla_\lambda E_q[\log q(z;\lambda)]$. We thus focus on estimating $\nabla_\lambda E_q[\log p(z,x)]$. Suppose that 
$\nabla_\lambda E_q[\log p(z,x)]$ can be written as an expectation, i.e., 
\begin{equation}
\nabla_\lambda E_q[\log p(z,x)]=E[h(z)].\label{eq:expform}
\end{equation}
Then one can easily find an unbiased estimate of $\nabla_\lambda\mathcal L(\lambda)$, 
\begin{equation}
\widehat{\nabla_\lambda\mathcal L(\lambda^{(t)})}=\frac 1 N \sum_{i=1}^N h(z_i) -\nabla_\lambda E_q[\log q(z;\lambda)],\label{eq:sgd}
\end{equation}
where $z_i$ are Monte Carlo samples or quasi-Monte Carlo samples.

There are two tricks to obtain the expectation  form \eqref{eq:expform}. Allowing the interchange of integration and differentiation, **the score function gradient method**
makes use of 
\begin{align}
\nabla_\lambda E_q[\log p(z,x)]&=\nabla_\lambda\int \log p(z,x) q(z;\lambda)d z\\
&= \int \log p(z,x) \nabla_\lambda q(z;\lambda)d z\\
&= \int \log p(z,x) (\nabla_\lambda \log q(z;\lambda)) q(z;\lambda)d z\\
&=E_q[\log p(z,x) \nabla_\lambda \log q(z;\lambda)],
\end{align}
achieving the  form \eqref{eq:expform} with $h(z)=\log p(z,x) \nabla_\lambda \log q(z;\lambda)$ and $z\sim q(z;\lambda)$. On the other hand, **the reparameterization method** rewrites the expectation $E_q[\log p(z,x)]$ 
as an expectation w.r.t. a density independently of the parameter $\lambda$, say, $E_{p_0}[\log p(h(z;\lambda),x)]$, where $h(z;\lambda)\sim q(z;\lambda)$ and $z\sim p_0(z)$ independent of $\lambda$. As a result,
by allowing the interchange of integration and differentiation, 
\begin{align}
\nabla_\lambda E_q[\log p(z,x)]&=\nabla_\lambda E_{p_0}[\log p(h(z;\lambda),x)]\\
&= \nabla_\lambda \int \log (p(h(z;\lambda),x)) p_0(z) d z\\
&=  \int \nabla_\lambda \log (p(h(z;\lambda),x)) p_0(z) d z\\
&=E_{p_0}[\nabla_\lambda \log (p(h(z;\lambda),x))],
\end{align}
achieving the  form \eqref{eq:expform} with $h(z)=\nabla_\lambda \log (p(h(z;\lambda),x))$ and $z\sim p_0(z)$.

The reparameterization gradient typically exhibits *lower variance* than the score function gradient but
is restricted to models where the variational family can be reparametrized via a differentiable mapping. We refer to the article 

> Xu, M., Quiroz, M., Kohn, R., & Sisson, S. A. (2018). Variance reduction properties of the reparameterization trick. arXiv preprint arXiv:1809.10330.


The convergence of the gradient ascent scheme in \eqref{eq:sgdite} tends
to be slow when gradient estimators \eqref{eq:sgd} have a high variance.
Therefore, various approaches for reducing the variance of
both gradient estimators exist; e.g. control variates,
Rao-Blackwellization, importance sampling as well as quasi-Monte Carlo. For the use of qausi-Monte Carlo in VI, we refer to 

> Buchholz, A., Wenzel, F., & Mandt, S. (2018). Quasi-monte carlo variational inference. arXiv preprint arXiv:1807.01604.




## Bayesian multinomial logistic regression


The famous (Fisher's or Anderson's) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.



Let $y_i\in \{0,\dots,K\}$ be the categorial data, which relates to $x_i = (x_{i1},\dots,x_{ip})^\top$. The multinomial logistic regression is given by

\begin{equation}
P(y_i=k|\beta) = \frac{\exp\{x_i^\top \beta_k\}}{\sum_{j=0}^K \exp\{x_i^\top \beta_j\}},\quad k=0,\dots,K,
\end{equation}

where $\beta_k\in \mathbb{R}^{p\times 1}$ and the parameters are $\beta=(\beta_1,\dots,\beta_K)$, and we set $\beta_0=0$ for indentifying the model.
The prior we used is $\beta_k\stackrel{iid}\sim N(0,\sigma_0^2I_p),k=1,\dots,K$.
We treated the designed matrix $X$ as a constant matrix. 

The variational density we used is Gaussian, i.e., $q(\beta_{ij})\sim N(\mu_{ij},\sigma_{ij}^2)$. Let $\psi_{ij}=\log (\sigma_{ij})$ so that $q(\beta_{ij})\sim N(\mu_{ij},\exp(2\psi_{ij})).$ Now the variational parameters are $\mu_{ij}$ and $\psi_{ij}$. We encapsulate them in a vector $\lambda$. Denote the ELBO by $L(\lambda)$. We thus have
\begin{align}
L(\lambda) &= E_q[\log p(\beta)] + E_q[\log p(y|\theta)] - E_q[\log q(\beta)]\\
&=\sum_{ik}\left(\psi_{ik}-\frac {\mu_{ik}^2+\exp(2\psi_{ik})}{2\sigma^2_0}\right) + \sum_{i=1}^n E_q\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top \beta_k\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top \beta_k\}}\right)\right]+\text{const}\\
&=L_1(\lambda)+L_2(\lambda)+\text{const}.
\end{align}

It is easy to see that
$$\frac{\partial L_1(\lambda)}{\partial \mu_{ik}}=-\frac{\mu_{ik}}{\sigma_0^2},\quad \frac{\partial L_1(\lambda)}{\partial \psi_{ik}}=1-\frac{\exp(2\psi_{ik})}{\sigma_0^2}.$$
This implies $\nabla_\lambda L_1(\lambda)$ has a close form.

The  score function gradient for $L_2(\lambda)$ is given by
$$\nabla_\lambda L_2(\lambda)=\sum_{i=1}^n E_q\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top \beta_k\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top \beta_k\}}\right)\nabla_\lambda \log q(\beta;\lambda)\right],$$
where
$$\frac{\partial  \log q(\beta;\lambda)}{\partial \mu_{ik}}=\frac{\beta_{ik}-\mu_{ik}}{\exp(2\psi_{ik})},\ \frac{\partial  \log q(\beta;\lambda)}{\partial \psi_{ik}}=\frac{(\beta_{ik}-\mu_{ik})^2}{\exp(2\psi_{ik})}-1.$$

We now rewrites the expectation $L_2(\lambda)$ as 
\begin{align}
L_2(\lambda)&=\sum_{i=1}^n E_q\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top \beta_k\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top \beta_k\}}\right)\right]\\
&=\sum_{i=1}^n E\left[\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}\right)\right],\\
\end{align}
where $z_k\stackrel{iid}\sim N(0,I_p)$.
The
reparameterization gradient is given by
\begin{align}
\nabla_\lambda L_2(\lambda)&=  \sum_{i=1}^n  E\left[\nabla_\lambda\log \left(\frac{\sum_{k=0}^K\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}1\{y_i=k\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}\right)\right]\\
&=:\sum_{i=1}^n E[\nabla_\lambda h_{i}(z;\lambda)].
\end{align}


where 
\begin{align}
\frac{\partial  h_{i}(z;\lambda)}{\mu_{jk}}&= x_{ij}1\{y_i=k\} -\frac{x_{ij}\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}},
\end{align}
\begin{align}
\frac{\partial  h_{i}(z;\lambda)}{\psi_{jk}}&=x_{ij}z_{jk}\exp(\psi_{jk})1\{y_i=k\}-\frac{x_{ij}z_{jk}\exp(\psi_{jk})\exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}{\sum_{k=0}^K \exp\{x_i^\top (\mu_{\cdot k}+\text{diag}(\exp(\psi_{\cdot k}))z_k)\}}.
\end{align}
As a result,



```{r,fig.cap='The species are Iris setosa, versicolor, and virginica.',out.width='80%',fig.align='center',echo=F}
knitr::include_graphics('iris.png')
knitr::kable(iris[c(1:3,51:53,101:103),])
```

We next show the results for the iris data with the score function gradient based Adam algorithm. 

\newpage

```{r,fig.align='center',fig.cap='Score function gradient based Adam', eval=FALSE}
## data generation
set.seed(0)
data(iris)
n <- nrow(iris)
p <- ncol(iris)
K <- nlevels(iris$Species)
X <- model.matrix(Species ~ ., data=iris) # design matrix
Y <- model.matrix(~ Species - 1, data=iris)

sigma0 = 10
elbo_hat<- function(mu,psi,N){
  L1 = sum(psi-(mu^2+exp(2*psi))/(2*sigma0^2))
  beta = matrix(0,p,K) # the first column is zero 
  est = matrix(0,N,1)
  for (i in 1:N){
    beta[,-1] = mu + exp(psi) * matrix(rnorm(p*(K-1)),p,K-1)
    tmp = exp(X%*%beta)
    den = rowSums(tmp)
    num = rowSums(tmp*Y)
    est[i] = sum(log(num/den))
  }
  return(mean(est)+L1)
}

score_fun_gradient <- function(mu,psi,N){
  dmu = -mu/sigma0^2
  dpsi = 1-exp(2*psi)/sigma0^2
  beta = matrix(0,p,K) # the first column is zero 
  for (i in 1:N){
    beta[,-1] = mu + exp(psi) * matrix(rnorm(p*(K-1)),p,K-1)
    tmp = exp(X%*%beta)
    den = rowSums(tmp)
    num = rowSums(tmp*Y)
    sumlog = sum(log(num/den))
    dmu = dmu + sumlog*(beta[,-1]-mu)/exp(2*psi)/N
    dpsi = dpsi + sumlog*((beta[,-1]-mu)^2/exp(2*psi)-1)/N
  }
  return(list(dmu=-dmu,dpsi=-dpsi)) # return negative gradient for adapting the Adam
}

#Adam: See the paper 'ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION'

alpha = 0.1
beta1 = 0.9
beta2 = 0.999
eps = 1e-8

#initial parameters
mut = matrix(0,p,K-1)
psit = matrix(0,p,K-1)
mu_mt = matrix(0,p,K-1) #first moment
psi_mt = matrix(0,p,K-1)
mu_vt = matrix(0,p,K-1) #second moment
psi_vt = matrix(0,p,K-1)
T = 500
Ng = 100 #sample size for gradient
Nelbo = 10000 #sample size for estimating elbo
elbo = matrix(0,T,1)
for(t in 1:T){
  gt = score_fun_gradient(mut,psit,Ng)
  mu_mt = beta1*mu_mt + (1-beta1)*gt$dmu
  mu_vt = beta2*mu_vt + (1-beta2)*gt$dmu^2
  hat_mu_mt = mu_mt/(1-beta1^t)
  hat_mu_vt = mu_vt/(1-beta2^t)
  mut = mut - alpha*hat_mu_mt/(sqrt(hat_mu_vt)+eps)
  
  psi_mt = beta1*psi_mt + (1-beta1)*gt$dpsi
  psi_vt = beta2*psi_vt + (1-beta2)*gt$dpsi^2
  hat_psi_mt = psi_mt/(1-beta1^t)
  hat_psi_vt = psi_vt/(1-beta2^t)
  psit = psit - alpha*hat_psi_mt/(sqrt(hat_psi_vt)+eps) #update the state
  
  elbo[t] = elbo_hat(mut,psit,Nelbo)
}
plot(elbo,type='b',xlab = 'Iteration',ylab="ELBO",pch=16)
```

